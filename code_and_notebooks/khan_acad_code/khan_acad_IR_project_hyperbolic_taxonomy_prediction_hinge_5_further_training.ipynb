{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "khan_acad_IR_project_hyperbolic_taxonomy_prediction_hinge_5_further_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR9av2JU3kf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d01c8a6e-3f06-45e6-c5e5-9d9b9eeff01f"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da-zaZJUGMFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404ea120-bbd3-4d70-e972-20ef1a3ba148"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plVnrNiy2lme"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/train_khan_acad.csv\" /content\n",
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/test_khan_acad.csv\" /content\n",
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/val_khan_acad.csv\" /content\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlE6vskY9VmG"
      },
      "source": [
        "# !cp -r \"/content/drive/My Drive/Information_retrieval_project/IR_project_model_hyperbolic_hinge_5_2/\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzKeqoCs3kgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3441dc6e-849c-4fea-8e59-8aef13d3c42e"
      },
      "source": [
        "!pip install transformers==3.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.2.0 in /usr/local/lib/python3.6/dist-packages (3.2.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.1.95)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.8.1rc2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.2.0) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf-OXnTs-ZhS"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZFv4UU8sLTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc81456-fd2a-47a7-f485-1c8e725b967a"
      },
      "source": [
        "!pip install git+https://github.com/geoopt/geoopt.git\n",
        "! pip install git+https://github.com/ferrine/hyrnn.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/geoopt/geoopt.git\n",
            "  Cloning https://github.com/geoopt/geoopt.git to /tmp/pip-req-build-01z1jl_j\n",
            "  Running command git clone -q https://github.com/geoopt/geoopt.git /tmp/pip-req-build-01z1jl_j\n",
            "Requirement already satisfied (use --upgrade to upgrade): geoopt==0.3.1 from git+https://github.com/geoopt/geoopt.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from geoopt==0.3.1) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from geoopt==0.3.1) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->geoopt==0.3.1) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->geoopt==0.3.1) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->geoopt==0.3.1) (0.16.0)\n",
            "Building wheels for collected packages: geoopt\n",
            "  Building wheel for geoopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for geoopt: filename=geoopt-0.3.1-cp36-none-any.whl size=76165 sha256=be62a6f2256697d15033ccdb14e1d31cf55219a7eecccae2c4b630a8afd91765\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-53z9jois/wheels/10/df/30/e0d857f034c142ca5f38af048b62aae3da773b272553e5dd21\n",
            "Successfully built geoopt\n",
            "Collecting git+https://github.com/ferrine/hyrnn.git\n",
            "  Cloning https://github.com/ferrine/hyrnn.git to /tmp/pip-req-build-o0yke76t\n",
            "  Running command git clone -q https://github.com/ferrine/hyrnn.git /tmp/pip-req-build-o0yke76t\n",
            "Requirement already satisfied (use --upgrade to upgrade): hyrnn==0.0.0 from git+https://github.com/ferrine/hyrnn.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from hyrnn==0.0.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyrnn==0.0.0) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->hyrnn==0.0.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->hyrnn==0.0.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->hyrnn==0.0.0) (3.7.4.3)\n",
            "Building wheels for collected packages: hyrnn\n",
            "  Building wheel for hyrnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyrnn: filename=hyrnn-0.0.0-cp36-none-any.whl size=13955 sha256=6cfd01546644d7f7182840fc71b254f21fcb07f4fb39ecaa1a1a97088cd92fb7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jrhcamhr/wheels/24/c3/64/cc0e9d25d466081dc154a2a8843157f54d845b916b4ba66418\n",
            "Successfully built hyrnn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsADhaO93kgD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "6519e2f5-220d-4514-be1d-4483b5ba4671"
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"train_khan_acad.csv\")\n",
        "val_data = pd.read_csv(\"val_khan_acad.csv\")\n",
        "test_data = pd.read_csv(\"test_khan_acad.csv\")\n",
        "\n",
        "train_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the last couple of videos we saw that we c...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-  What we're going to do in this video is gi...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;natural-selection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So once again, we have three equal, or we say...</td>\n",
              "      <td>math&gt;&gt;pre-algebra&gt;&gt;pre-algebra-equations-expre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  Liz's math test included a survey question...</td>\n",
              "      <td>math&gt;&gt;engageny-alg-1&gt;&gt;alg1-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>- The following two equations form a linear s...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-system-of-equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4183</th>\n",
              "      <td>-  Hello everyone. So this is what I might ca...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4184</th>\n",
              "      <td>-  Let's try now to subtract some two-digit n...</td>\n",
              "      <td>math&gt;&gt;early-math&gt;&gt;cc-early-math-add-sub-100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4185</th>\n",
              "      <td>- Let's say that I have a circle. My best att...</td>\n",
              "      <td>math&gt;&gt;engageny-geo&gt;&gt;geo-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4186</th>\n",
              "      <td>- So let's look at the female reproductive cy...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4187</th>\n",
              "      <td>-  We're now in the home stretch. We just hav...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;greens-theorem-a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4188 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts                                          hierarchy\n",
              "0      In the last couple of videos we saw that we c...  math>>multivariable-calculus>>multivariable-de...\n",
              "1      -  What we're going to do in this video is gi...             science>>ap-biology>>natural-selection\n",
              "2      So once again, we have three equal, or we say...  math>>pre-algebra>>pre-algebra-equations-expre...\n",
              "3      -  Liz's math test included a survey question...                       math>>engageny-alg-1>>alg1-2\n",
              "4      - The following two equations form a linear s...        math>>algebra-home>>alg-system-of-equations\n",
              "...                                                 ...                                                ...\n",
              "4183   -  Hello everyone. So this is what I might ca...  math>>multivariable-calculus>>multivariable-de...\n",
              "4184   -  Let's try now to subtract some two-digit n...        math>>early-math>>cc-early-math-add-sub-100\n",
              "4185   - Let's say that I have a circle. My best att...                          math>>engageny-geo>>geo-5\n",
              "4186   - So let's look at the female reproductive cy...  science>>health-and-medicine>>human-anatomy-an...\n",
              "4187   -  We're now in the home stretch. We just hav...  math>>multivariable-calculus>>greens-theorem-a...\n",
              "\n",
              "[4188 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2GT0Ti4-HLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5226ee7-57ac-4ebe-9955-5fca9d36d8f8"
      },
      "source": [
        "set(train_data[\"video_transcripts\"].values).intersection(set(test_data[\"video_transcripts\"].values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQhO6qqt6lge"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD4IAgG1XQGp"
      },
      "source": [
        "import re\n",
        "def clean_sentence(question):\n",
        "  # print(question)\n",
        "  question = re.sub('<[^>]*>', ' ',question)\n",
        "  question = re.sub(' +', ' ', question)\n",
        "  question = re.sub('\\xa0','',question)\n",
        "  question = question.rstrip()\n",
        "  question = re.sub('nan','',question)\n",
        "  question = re.sub(u'\\u2004','',question)\n",
        "  question = re.sub(u'\\u2009','',question)\n",
        "\n",
        "  # question = question.decode(\"utf-8\")\n",
        "  # question = question.replace(u'\\u200\\d*','').encode(\"utf-8\")\n",
        "  question = re.sub('&nbsp','',question)\n",
        "  question = re.sub('&ndash','',question)\n",
        "  question = re.sub('\\r','',question)\n",
        "  question = re.sub('\\t','',question)\n",
        "  question = re.sub('\\n',' ',question)\n",
        "\n",
        "  question = re.sub('MathType@.*','',question)\n",
        "  question = re.sub('&thinsp','',question)\n",
        "  question = re.sub('&times','',question)\n",
        "  question = re.sub('\\u200b','',question)\n",
        "  question = re.sub('&rarr;;;','',question)\n",
        "\n",
        "  return question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNrGNk8f3kgh"
      },
      "source": [
        "# final_data_1 = final_data.loc[0:71003,:]\n",
        "# final_data_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrS5sxE3kgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc312c5-724f-4f05-8c0f-121dc1dcb4d8"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mgc72PQYV1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607c5e5a-00b2-46fb-af91-64be40632b6c"
      },
      "source": [
        "train_data[\"video_transcripts\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " [This video contains no audio narrations.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                3\n",
              " -  Now this time in TAPVR, we're dealing with a pulmonary veins that happens to be the one thing I didn't draw. But let's label what I did draw. We have the right atrium receiving blood from the body going to the right ventricle pumping it to the lungs through the pulmonary arteries. And then the pulmonary veins are the things that give blood back to the atrium on the left side from the lungs. See pulmonary meaning it comes from the lungs and vein means it's going back to the heart instead of artery which is away from the heart. So the lungs giving oxygen and red blood back to the left atrium. Those are our pulmonary veins and we have the left ventricle which pumps it to the body. The thing with TAPVR is instead of it being plugged over here where it's supposed to be, our pulmonary veins are plugged over here into the right ventricle. I'm going to draw two here coming from the left and right lungs. So the right ventricle usually receives blue deoxygenated blood from the body. But now we've red blood from the lungs coming by here too. So, let's see there's deoxygenated blue blood from the veins and red blood. So here our right atrium's receiving this mixture. Again this purplish kind of blood is floating around in our right atrium. And from there the rest of the circuit is the same. It goes to the right ventricle. It goes to the lungs. So again we have purple blood going out to our lungs. But from there the blood picks up enough oxygen to become red again and then comes back through these misplugged pulmonary veins right here. So as you can see, there's nothing going on on the left side. It's as if this chamber is sealed off. There's nothing coming back into the left atrium. and therefore, this is all empty. That means there's nothing going on in the aorta. There's no blood being pumped to the body, As you know, that's not compatible with life, so usually TAPVR also involves ASD. So Atrial Septal, which is the wall between them, Septal Defect. So thankfully, we have a hole between the left and right atria here, a little hole, So that our purple blood can go this way and the left atrium will be filled from the right side with purple blood. And now this purple blood can then go into the left ventricle and out through the aorta into the body. Because without this little hole here, then we just have deoxygenated blood coming from the body, mixing there with the red blood from the lungs into purple blood going back to the lungs and coming back as red blood. And then we have again, we have this closed circuit. It has nothing to do with the body and we can't survive without blood going into our aorta, into the body. So this closed system has to be opened and that's why we have the ASD here, to allow the mixed blood into the left side so we can survive. Usually, this also involves right ventricular hypertrophy, because look, it's doing twice the work it's used to. There's just an overwhelming amount of volume of blood going to the right side. So this increased workload will give us right ventricular hypertrophy as well. And this, again, needs surgical correction to replug the pulmonary veins back where they belong. So, just remember these misplaced pulmonary veins when you think of TAPVR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
              " If we were to rewind the clock back about 70 million years, you would see dinosaurs roaming the Earth. And this is a very nice picture here of a dinosaur enjoying a sunset at the beach. But unfortunately for the dinosaurs, about 65 million years ago, we believe that a huge meteorite struck the earth and essentially wiped out the dinosaurs. And it probably wiped out a bunch of other species with it. Because you can imagine, the shock wave itself would just exterminate tons of species. Then you would have the tsunami of unimaginable size that would just envelop the continents for some period of time. And then you would have all of the soot that would go into the air and maybe make it impossible for most of the plant species to live, because it would be blocking out all of the sunlight. And so in an environment like that, we could imagine that an animal like this would be well suited to survive. It's sitting there underground. Maybe it can hibernate in some way, so it doesn't need food for long periods of time. Maybe it has its own food stash under there someplace. And so we believe that our ancient, ancient, ancient, ancient, ancient ancestors, after this mass extinction event, might have been something like this-- kind of a mole-looking, rodent animal that was protected from all of this craziness that was happening on the surface, because they like to hang out underground and have all their food nearby them. And maybe they could hibernate in some way. So you could imagine, once everything settled down-- and now we're talking, who knows, hundreds of years, thousands of years, even millions of years-- some of this guy's descendants start to poke their head out of the ground. And they're like, you know what? There's food in trees. And there's no one else in the trees. And trees are a good place to maybe get away from some of the other predators that have managed to survive this mass extinction event. And some of its descendants, I should say, that were good at climbing trees decide, hey, let's try this tree thing out. And so you started to have some selection for the descendants of this rodent that could climb trees well. They were able to find food where their ancestors couldn't . They could find protection in the trees where their ancestors couldn't. And so you could imagine that some subset of this guy's descendants evolved into something that might have looked like this guy. And all the pictures I'm showing you, these are of modern animals, except for, of course, the dinosaur. I'm sure this was kind of Photoshopped in some way. This is a modern bush baby. But I show this picture because it could have been what some of these primitive primates looked like. Because a bush baby, it kind of a climbs trees. It kind of looks like it's starting to get a hand here to start climbing the trees. But it also has rodent-like qualities. But this is, of course, a modern version of it. So this bush baby's ancient, ancient, ancient ancestor might have been that primitive primate or that species of primitive primate that was a descendant of rodents that starts to say, hey, let's see if we can climb these trees and find some food. And then some of its descendants might have had just the right adaptations, found their own little niche in the right ecosystems. And they would have evolved into monkeys. Once again, this is a modern monkey, but you could imagine some type of primitive monkey. And then some of those primitive monkeys' descendants, they turn into these modern monkeys eventually. But some of them, they grow larger in size. They spend more time outside of trees. They lose their tail. They don't need it as much for balance. Maybe it's actually a bad thing to have, because someone else could grab it when you're in a fight or something like that. And they evolve into apes, and in particular, the great apes. So one of the great apes. The great apes involve gorillas and chimpanzees and the ancestor. Or really, the great apes also include humanity. So let me just review back on this timeline, just so that we don't get confused. I'll review what we just talked about. So before this mass extinction event, 65 million years ago, you had all these types of species here. Maybe this right up here. Actually, if I'm talking about species, maybe this was Tyrannosaurus Rex, because the dinosaurs involve whole bunch of-- so this might have been T Rex. And there's a bunch of species that we could list over here. But after that mass extinction event, that was an endpoint for a ton of species, except for maybe this primitive rodent mole-like thing. Maybe a lot of them died in this event. But just enough of them survived because they were underground or just in the right place or they were in a mountain someplace. Who knows where they were? And some of them were able to evolve into primitive primates. And once again, these are pictures of primitive primates. And when I say primitive, these are modern versions of them. So primitive doesn't necessarily mean worse, because obviously, these guys, even in today's world, they have a niche for themselves. They're able to find food and reproduce in ways that don't get in the way of other people and the way people don't get in the way of them. When I talk about primitive primate, I'm just talking about kind of an ancestral primate, maybe something that's not there today. Although maybe some of those descendents look very much like it. But anyway, some of those primates evolve into primitive monkeys. Some of those primitive monkeys' descendants become modern monkeys. So I'll call it M monkeys, for Modern monkeys. And some of them evolve into primitive apes. And apes, their distinctive characteristic is that they're like monkeys, but they don't have tails, and they're larger than most monkeys. And so these primitive apes, some of their descendants are modern gorillas. At some point, they break off. Some of these descendants are an ancestor of both modern chimpanzees and of human beings. And we think, just looking at the DNA evidence, we think that this departure right here-- and the fossil evidence-- was about seven million years ago. That's our best guess for when we, as human beings, had a common ancestor with the chimpanzees. Now, you have that common ancestor. Some of that common ancestor's descendants became modern chimpanzees. And some of them-- maybe they explored the right ecosystem, where it was more advantageous to do so-- started to walk on two legs. And the most famous fossil of this is the australopithicine fossil of Lucy that was discovered 3.2 million years-- it was discovered more recently. It's 3.2 million years old. So the whole genus-- and genus is kind of one level of categorization above species. The whole genus of australopithecine, these were four to two million years ago. And we never know. You could always find a fossil that's older than this, maybe newer than this. I read one account that says, maybe one million years ago. But give or take, the Lucy fossil, which is the most well established australopithecine fossil, is about three million years old. And this is a reconstruction I have over here of Lucy. So this is probably what Lucy looked like. And once again, there were many Lucys. It wasn't just there was one Lucy. And we're all descended from Lucys. And it's actually not even clear that we're even descended directly from australopithecine. We might be a cousin species or a cousin genus, I should say. Genus is the category right above species. So if you fast forward a little bit more-- you go to about 2.3 to 1.4 million years ago-- we see fossils that they're standing upright. The brain size is bigger. Because if you look at the australopithecine fossils, they are standing upright. But they're cranial capacity isn't that different than chimpanzees. You fast forward to 2.3 million to 1.4 million years ago, we start to see fossils where they're standing upright still. And the cranial capacity has grown. And you're starting to see primitive stone tools around the bone fossils. And so we believe that these are one of the first. And this is really just how we categorize it. But these are some of the first fossils that we categorize as belonging to the same genus as ours. And the genus is Homo. And Homo just means man. So it's the group right above species of man. And we call them similar to man, because it looks like they're starting to make primitive stone tools. And they stand upright like us. And they have larger cranial capacities than the australopithecine fossils or modern chimpanzees. And once again, we don't know if Homo habilis, which literally means-- so the Homo part means man. Habilis means handy, because he liked to, I guess, make tools or whatever else. We don't know if Homo habilis is a descendant of Lucy's species of Australopithecus or maybe a cousin species. Maybe they're both descendents from some common ancestor. We're not quite sure. Then you fast forward a little bit more. We're talking now about 1.8 to 1.38 million years ago. And we start seeing fossils where the cranial capacity larger than Homo habilis, getting closer in size to what our notion is of kind of a modern person's cranial capacity, at least relative to body size. And this is Homo erectus. And once again, we don't know if Homo erectus is the descendant of Homo habilis. Maybe they have a common ancestor. Who knows? And it looks from the fossil evidence that there was, especially when you look at this range here, some overlap where you had both Homo erectus and Homo habilis living on the same planet at the same time. Now, you fast forward even more. And we think about 600,000 to 300,000-- once again, all of these are constantly being modified, as we get better at finding new fossils or interpreting the fossils we have or we look at DNA evidence or whatever. About 600,000 to 300,000 years ago, the Neanderthals appear. And Neanderthals are in the same genus as humans. 1So it's really Homo neanderthalensis. 1I always have trouble saying this. 1So this is still part of Homo. 1And a common misconception is that the Neanderthals are 1somehow a more primitive version of humans, 1that they're somehow cavemen, and we're modern men. 1That's not the case. 1The belief is that Neanderthals are either 1cousin species-- we have a common ancestor-- 1or that they're actually a sub-species of human beings. 1And there's some belief that they 1might have interbred with Homo sapiens. 1And maybe some or a good number of us have a Neanderthal genes. 1And it's nothing to be ashamed of. 1It's just something, unfortunately, 1that Neanderthals just get a bad name, because of, I guess, 1our popular culture. 1So this is a drawing of a Neanderthal brain. 1They actually had a fairly large cranial capacity. 1Although scientists, they kind of make one reason or another 1why we think that they might have 1been more primitive than Homo sapiens. 1But who knows? 1We don't know. 1We're constantly learning things every day. 1But of course, the whole point of this 1is to talk about how humans showed up on this planet. 1And the first really human fossils 1we find about 200,000 years ago. 1And remember, we're in the genus Homo. 1And now, we've finally found something that looks just 1like us, anatomically, at least. 1We can't study its behavior and all the rest. 1And now we get to Homo sapiens. 1The Homo part, once again, means man. 1And the sapiens means thinking. 1So we can debate whether it's an appropriate title 1for our species, but it's \"thinking man.\" 1So once again, the Neanderthals, they 1were either a cousin species for a lot of this time, 1especially once Homo sapiens showed up. 1And maybe Homo sapiens showed up before this. 1We just haven't found the fossils yet. 1They were maybe both inhabiting the same planet. 1Maybe there was some interbreeding. 1But the Neanderthals disappeared about 30,000 years ago. 130,000 years ago, these guys disappeared. 1Maybe some of them kind of got mixed 1in with the Homo sapiens, started 1to interbreed with them. 1Or they might have just been killed off, 1because they were fighting over the same ecosystems. 1And I've made a little sample here of Homo sapiens. 1Well, I'm assuming most of you watching this video are one. 1But just in case, here's my little sample. 1We can debate how representative a sample of our species 1this really is.    1\n",
              " -  This is a painting of Nicole Oresme. I looked up the phonetic spelling before making this video. I'm assuming I'm still mispronouncing it. But I apologize to all the French speakers out there ahead of time. He was a famous French philosopher mathematician, who lived in medieval France, he lived in the 1300s. And he is famous for his proof that the harmonic series actually diverges. And just as a little bit of review, this is a harmonic series. One plus 1/2, plus 1/3, plus 1/4, plus 1/5. And it's always been in my brain, the first time that I saw the harmonic series, it wasn't obvious to me whether it converged or diverged. It looks like, well, gee, all these terms are positive, but they're going towards zero, so I could imagine that this thing could converge. But he proved it otherwise. He proved one of the most famous and most elegant proofs in mathematics that it does indeed diverge. And the way that he did this, is he replaced every term in the harmonic series with a term that is less than or equal to that term. And then by proving that his new series diverges and it's less than or equal to this series, or each of the terms are less than or equal to each of the corresponding terms up here, then he says, therefore, by the comparison test, this must diverge. So, how did he construct that? One way to think about it is he replaced each of the terms in the harmonic series with the largest power of 1/2 that is less than or equal to that term. So, what's the largest power of 1/2 that is less than or equal to one? Well, one is a power of 1/2, so that is, 1/2 to the zero power is one. So, one is the largest power of 1/2 that is less than or equal to one. I'll just write the one there. And now, what's the largest power of 1/2 that is less than or equal to 1/2? That's just going to be 1/2, that's just 1/2 to the first power. Now, what's the largest power of 1/2 that is less than or equal to 1/3? 1/2 is larger than 1/3, it's not less than 1/3. We want it to be less than 1/3, so the largest power of 1/2 that is less than or equal to 1/3 is 1/4. I replace 1/3 with 1/4 and, of course, replace 1/4 with 1/4. And then we get to 1/5. What's the largest power of 1/2 that is less than or equal to 1/5? Once again, 1/4 is greater than 1/5. The largest power of 1/2 that is less than or equal to 1/5 is 1/8. So, you replace that with 1/8. Of course, we replace for the same reason that with 1/8. You would replace that one with 1/8. And, of course, 1/8. The largest power of 1/2 that is less than or equal to 1/8 is 1/8. And then what would you replace 1/9 with? You would replace 1/9 with 1/16 by the exact same argument. And you would keep going all the way until you get to 1/16, so you would extensionally have eight 1/16s in a row. What's interesting here? Let's first verify that we can use the comparison test here. In this first series each of the terms are non-negative. In the second series each of the terms are non-negative. And we also see that each of the corresponding terms in a harmonic series is greater than or equal to the corresponding term in this series. We constructed it this way. These are equal, this one is equal, this is greater than this, 1/3 is greater than 1/4. 1/4 is equal to 1/4, 1/5 is greater than 1/8, 1/6 is greater than 1/8, 1/7 is greater than 1/8. 1/8 is equal to 1/8. One way to think about it is, each of the corresponding terms in this new constructed series are smaller. Let me just call it S. In this infinite sum, and, of course, we keep going on and on and on. Maybe i should do that in magenta. We see that each of the corresponding terms here are smaller than the corresponding terms up here. And they are all positive. If we can prove that S, this sum right over here, diverges, then, by the comparison test, the larger series, the harmonic series here, the one where the corresponding terms are larger, that must also diverge. And how do we do that? Let's just actually take these sums. This is going to be, let me write it. S is going to be equal to one plus 1/2. 1/4 plus 1/4, what's that? That's 2/4, or 1/2. Did you see what's going on here? This is exciting. What's 1/8 plus 1/8 plus 1/8 plus 1/8? That's 4/8, or 1/2. What's 1/16 plus 1/16, and we are going to go all the way until we are going to have eight of these. That's going to be 8/16, or 1/2. And then you are going to have sixteen 1/32nds, or 1/2. And so, we are essentially just going to be adding 1/2. We start with one, we just keep adding plus 1/2, plus 1/2, plus 1/2, plus 1/2. This is clearly going to be equal to, or this is unbounded. You could say, this is equal to infinity. Or, another way to think about this, is S clearly diverges. And since each of its terms are smaller than the corresponding terms in the harmonic series, we can then say the harmonic series diverges. There is no way that this thing over here can converge. If each of its corresponding terms are smaller, you could even think of the sum as being smaller. But this sum goes to infinity, so this one must also go to infinity. Hopefully, you found that as interesting as I did.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
              " Respiratory distress is one of those things where if you had it, nobody has to tell you that you're short of breath. But I think it's important to recognize when somebody else is in respiratory distress. First, let's just draw our generic patient. Let's call him Bob and let's give him some hair. I feel like giving hair to my stick figures masks the fact that I can't draw (laughs). Okay, so Bob here has respiratory distress. Let's draw the rest of his body. Here we're gonna talk about acute respiratory distress. In other words, it happened pretty recently in terms of days not like Bob has had lung disease for years and years. That will look like a different picture. If somebody has respiratory distress, we're going to talk about the things you can see to tell you and things you can hear. Those are the only two senses that are really important. I don't think you can touch or taste respiratory distress. In terms of things you can see. First of all, you never really see a person lying down saying, \"I'm short of breath.\" The first thing that somebody would do if they're short of breath is to sit up. That's why in the hospital if somebody has lung disease you rarely see them lying down all the way in bed. Now, of course, there are many different kinds of lung disease, but the fact that they like to sit up has a few common reasons. First there's our muscle called the diaphragm. In normal breathing this curved muscle moves down, which expands the chest cavity and that's how air goes in through your mouth and your nose. Just by simple rule of gravity we have to move something down and it's easier to do that when sitting up. Just so you don't have to fight the push of gravity down on your chest where you're trying to move your diaphragm toward your feet. Another reason, the reason you are short of breath is because a fluid, if we think about how fluid behaves then when it's flat, again, gravity makes it distribute all over the place and your whole lungs are bathed in the fluid. If you sit up the fluid kind of pulls to the bottom. This frees up the top of the lungs here to breathe a little better, so usually people like to sit up when they can't breathe. Next, you might notice that the rate of breathing might go up. In an adult, our normal rate of breathing is eight to 16 per minute and this is gonna go up just because when you can't move breath as well people tend to compensate by breathing more. Along with that, the panic of breathing faster and everything, happens to trigger the sympathetic nervous system, which is our fight or flight response. This is what happens to your body when it's in an emergency and it's trying to either run away or fight. One thing that happens here, which is kind of not related to our topic is that sympathetic nervous system allows your pupils to get bigger. This allows more light in and additionally your blood flows to your arms and your legs and away from your intestines so you don't have to digest for the few minutes that you're gonna be fighting. In our lungs, though, sympathetic nervous system can cause bronchial dilation, which literally just means these airways in our lungs as they branch off, they get bigger. And just the bigger diameter allows air to flow better. I didn't mean to draw his trachea, his windpipe, so deviated here. Imagine that this is straight down and in the middle of the neck. But in fact, actually if you have respiratory distress because of something like a pneumal thorax, when one lung has collapsed, then this trachea would deviate to one side or the other. But we can't see that by just looking at the person without an x-ray or some other form of imaging. Today we're just talking about naked eye seeing someone. So rate goes up and then the next thing there's a group of signs we call it increased work of breathing. Which is at the same time really specific, work of breathing, and also really vague, what is that? Work of breathing usually is pretty quiet. The diaphragm moves down, the chest expands as you're doing right now it doesn't take too much conscious effort. If someone is short of breath, other muscles get recruited. Like muscles in the neck, muscles in the shoulders. You might see them kind of tensing up their neck and shoulders trying to force that chest cavity bigger. Additionally, we have these what's called retractions, which just means there's muscles between our ribs. Our ribs going all the way up and the muscles between them can work so hard to compensate for respiratory distress that you can see the markings. If you take off our patient's shirt, you might be able to see the traces of their ribs both because the muscles are working hard and the decreased pressure in the lungs because you can't breathe well sucks that tissue in. These tracks are called retractions and that's part of work of breathing. In an infant, you might see something called nasal flaring, where their nostrils get bigger. If you try it right now you can sort of consciously increase the size of your nostrils and that just, again, let's more air in. That's more seen in little babies. Another thing you might see is called cyanosis. Which it just means that it's blue, purplish discoloration. Now, we think of blue blood as being lacking oxygen and it's not really blue, but it's just a little darker. This in our body can show up kind of bluish. This actually tends to happen in mucous membranes because there's less skin to cover that color. So in the lips and in the eyes and sometimes in the extremities just because they're the furthest away from our heart. So the hands and the feet sometimes might get blue. Now we're talking about acute distress here, but if Bob goes on to have this for years and years, a sign you can see in chronic respiratory distress is called clubbing. Usually we have our hand, 1,2,3 that's 4 fingers. So you have your hand and the fingers kind of taper off at the fingertips, that's the normal shape. But in clubbing, the tips of the fingers get big and kind of sausage like instead of tapering. It's called clubbing and this is chronic. The theory is that because these tips lack oxygen year in, year out, they kind of go through this hypertrophuge, which means more tissues grow to try to gather more oxygen. That's why clubbing happens. This again, is chronic. So now you can see it today in Bob since he just developed this. Alright, now moving on to things you can hear. There are a lot of lung sounds that are associated with having lung disease, but in terms of respiratory distress one thing that comes to mind is stridor. Stridor is usually above the chest cavity so the restriction is not so much down there but up here in the shoulder and neck level. I don't know if I can make a convincing noise for stridor, but it just sounds like (labored inhale). When this person is taking a breath in there's that extra noise. It's like if you put a straw in your mouth and you try to inhale, this is an inhalation noise. Next, you might hear some grunting. This is just because there are so many muscles between the abs, the shoulders, the neck trying to force air in and out and some of it goes across our vocal cords. And it just makes a grunt. This is like in the movies in the martial arts when they're punching someone and just using so much muscle force that this grunt escapes. In general I think of grunting as kind of an exhaling noise. For example, in emphysema when air doesn't get out some patients use their abs to try to force it out. And the grunting happens when they exhale. Usually an adult should be able to say, \"I can't breathe.\" They might not say respiratory distress, but they will definitely say, \"I can't catch my breath.\" But in the event that they can't tell you that or if it's a baby or a child, it's important to keep these signs in mind. The things you can see and you can hear when somebody's in respiratory distress.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ..\n",
              " What is sensory adaptation versus sensory amplification? So let's go into adaptation first. So sensory adaptation is change over time and the responsiveness of the sensory receptor to a constant stimulus. And what this basically is, is downregulation of a sensory receptor somewhere on your body. So for example, if we were to take our hand and place it on a table. So the hand is placed on the table. As soon as the hand touches the table, there are a whole bunch of pressure receptors throughout your fingers, in your palm. And they all experience a change in pressure. And these pressure receptors all simultaneously send a signal to the brain. After a few seconds of your hand being placed on the table, the pressure receptors are no longer firing. And in fact, you can even forget that your hand is touching the table. So this occurs because of adaptation. Another way we can think of this is if we draw a pressure receptor here. So this pressure receptor is in our hand. This is the cell body, the axon over here, and the axon terminal. As soon as the hand rests on the table, there is pressure from the weight of your hand touching the table, there's pressure. And this causes the cell to fire in action potential. And this action potential reaches the brain. Over a period of time, however, as soon as your hand is just resting on the table, there's no longer any change in pressure. So this cell is no longer sending a signal to the brain. And in fact, if you started to press your hand down on the table, then all of a sudden there would be again a change in pressure. But then if you hold your hand pressed on the table, then there's no longer any change in pressure. And basically this is in a nutshell what adaptation is. Adaptation is different cells in your body responding to a change in a stimulus. If the stimulus is no longer changing, then there's no longer any information that's being sent to the brain. In contrast, amplification is an upregulation. So upregulation of some sort of stimulus in the environment. So for example, if we take a ray of light-- and in previous videos, we talked about vision and how a ray of light is converted into an electrical impulse that is sent to your brain. So the ray of light hits a photoreceptor in your eye. And it actually triggers a cascade of events. So for example, we can say that if it will hit one molecule, and that molecule can activate two molecules. And then those two molecules can each activate two and so on. So eventually, what happens is one ray of light can actually cause a cell to fire. And when this cell fires an action potential, it can actually be-- it might be connected to maybe two cells. And these two cells then also fire an action potential to two more. And so on and so forth. And by the time the signal that this cell started reaches the brain, it's been amplified. And so this is basically amplification in a nutshell. And adaptation is important, because if the cell is overexcited-- If any cell is excited too much, it can actually be harmful to the cell. And it can actually die. So it's really important to have this adaptation. So for example, if this was a pain receptor instead of a pressure receptor, and if there is too much of a pain signal-- so for example, one molecule that can actually cause pain receptors to be activated is capsaicin. And we spoke about this in another video. So if there's too much capsaicin, for example, it can actually cause the cell to die. And so that's why it's important to downregulate a cell. It's important to adapt to any type of stimulus in the environment, in order for the cell both not to die, and then also for your brain to not be overwhelmed with information.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
              " y is directly proportional to x. If y equals 30 when x is equal to 6, find the value of x when y is 45. So let's just take this each statement at a time. y is directly proportional to x. That's literally just saying that y is equal to some constant times x. This statement can literally be translated to y is equal to some constant times x. y is directly proportional to x. Now, they tell us, if y is 30 when x is 6-- and we have this constant of proportionality-- this second statement right over here allows us to solve for this constant. When x is 6, they tell us y is 30 so we can figure out what this constant is. We can divide both sides by 6 and we get this left-hand side is 5-- 30 divided by 6 is 5. 5 is equal to k or k is equal to 5. So the second sentence tells us, this gives us the information that y is not just k times x, it tells us that y is equal to 5 times x. y is 30 when x is 6. And then finally, they say, find the value of x when y is 45. So when y is 45 is equal to-- so we're just putting in 45 for y-- 45 is equal to 5x. Divide both sides by 5 to solve for x. We get 45 over 5 is 9, and 5x divided by 5 is just x. So x is equal to 9 when y is 45.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1\n",
              " -  What we're going to do in this video is think about the market for chocolate and we're going to think about supply and demand curves, but we're going to get an intuition for them in a slightly different way. In particular for the demand curve we will think about the idea of marginal benefit. Now marginal benefit, when we're talking about margin it's really thinking about, well, what happens on the increment? What happens for each little extra that you do? So this is saying, what is the benefit that I get if I get a little bit more of, in this case, chocolate? Well, from the market's point of view, imagine if there was no chocolate, but there's people in the market who crave chocolate, who dream of chocolate. If all of a sudden they were able to get their hands on some chocolate, they would get a huge benefit for that incremental amount of chocolate. Maybe for these folks, their benefit, which we could quantify as, in terms of dollars, maybe their benefit is 50 dollars per pound. One way to think about it, they'd be willing to pay 50 dollars because they get that much benefit, or if they paid less than 50 dollars, let's say they paid 10 dollars for it, well then they're getting 40 dollars of extra benefit a kind of surplus of benefit from being able to get it at a price lower than their marginal benefit. But then let's say more chocolate becomes available. People still like it but some of that really deep need, that deep addiction for chocolate has been satiated, and so the marginal benefit tends for, in most markets, the marginal benefit tends to go down as quantity increases. One way to think about it, that first initial amount of quantity if you, so we have some small amount of quantity right over here, I'll say delta quantity. That first quantity, if you multiply it times the marginal benefit, well that gives you an area roughly of a rectangle like this, it's not quite rectangular at the top, it's more of a trapezoid if this is downward sloping, but you could approximate it as a rectangle. But either way, the area right over here, the area under the marginal benefit curve, you could think about this as, well, that's just the benefit that the market is getting from consuming this chocolate in this case. And so let's just continue on this trend. If there's more and more chocolate the market will get benefit from it, but people aren't as excited about it anymore. They're saying, oh, well, the chocolate's around, yeah, it'd be nice to have a little bit more, but I don't need so much more. And at some point people might be all chocolated out, and they have maybe even zero marginal benefit from that incremental amount of chocolate, chocolate has filled up the town, there's nowhere to actually put it. Now that won't always be the case, you might go someplace like that, but either way you think about it we would view this as our marginal benefit curve. And notice, this is exactly the same as a demand curve in the market for chocolate. We have plotted price versus quantity in the market for chocolate, but we've thought about it in terms of marginal benefit. Now on the supply side there's a related idea, we're going to think about marginal cost, marginal cost. So let's say at first there's no chocolate being produced in this market. And a savvy entrepreneur says, hey, I know some folks who are addicted to chocolate, they would get a lot of benefit from it, so I'm going to try to produce some chocolate. And they look around and they find out, hey, there's actually a derelict chocolate factory in town that no one is using and it's surrounded by these wild cocoa bushes that are perfect for chocolate. And there are some people in town who are actually unemployed, but they are amazing at producing chocolate. And so the first units of chocolate, the marginal cost to produce it is actually quite low. But once you utilize those folks, you utilize that derelict factory, you utilize those free cocoa bushes or whatever, cocoa trees, well then you've got to plant new ones, you've got to train new employees, you've got to build a new factory. And so to produce that next increment, well that's going to cost a little bit more and then a little bit more and then a little bit more, which is the general trend in most markets. Initially that first amount you produce in as cheap a way, using the low hanging fruit, as possible, but then you've got to go up the tree, find higher and higher fruit, maybe I'm mixing metaphors. But your cost, your marginal cost per unit goes higher and higher and higher. Now, what have we constructed here? Well you might say, hey, Sal, that's a marginal cost curve. But once again, this also could be viewed as the supply curve for this particular market. Now what is happening at these low quantities right over here? Well, the cost of production is, let's say they produce this delta Q amount, the cost of production would be the area right over here under the marginal cost curve, that would be the cost of production. But they're able to sell it, the benefit to the market I should say, would be the total area under this red curve, would be the benefit to the market, the total area under this curve. So if you have the total benefit to the market, you take out the cost, then what you have in between these curves, you could view this as a surplus, you could view this as a surplus of benefit right over here. So let me write this down, this is surplus, and you won't hear this term but I like to use it, because it makes it intuitive on what we're talking about, this is surplus benefit. So as long as there's surplus benefit the suppliers are going to say, hey, wow, I can produce this cheaply, people have a, people get a lot of benefit for it, they'd be definitely willing to pay 10 dollars a pound wherever I am. If people are getting this much benefit, they're definitely going to be willing to pay 10 dollars for it. So I'm going to produce some, or actually I'm going to produce some and I could even charge, I could charge anywhere in between these areas. Maybe I could charge right over here and I get some of the benefit and then the consumers get some of the benefit. But then another maybe entrepreneur realizes, hey, well, there's more benefit to be had in this market, so they keep producing, they keep producing as long as there is benefit here, as long as the marginal benefit is higher than the marginal cost, all the way until we get to that point right over here. Now what happens, what happens right over here? Well we talked about just supply and demand, we talk about that's an efficient price and efficient quantity, but let's just think about, we said up until this point it makes sense to produce more and more and more. Even this increment, if we're already at this quantity, it makes sense to produce even a little bit more because you're going to have this cost, you're going to have this cost to incur, but then the market could have all of this, the market could have all of this benefit, which is larger than the cost. And so you say, well, as long as I sell it for something in between we can split that surplus benefit, so to speak. But once these two lines intersect and we have the situation where our marginal benefit, marginal benefit, is equal to our marginal cost, well at that point there is no surplus benefit now to be had, there's no really incentive to produce more than this. Beyond this point, your incremental cost of production, your marginal cost is higher than your marginal benefit. So, if you actually wanted to give it to someone for their benefit, you would be taking a loss. Or even if you just think about the market itself, the society would be incurring more incremental cost per unit than they would be getting of benefit, so why even do it? And so this point right over here where these two lines, these two curves intersect and we've talked about this with just supply and demand, but when we think about it in terms of marginal benefit and marginal cost, we think about this quantity right over here, let's just call it Q subzero, this quantity is considered allocatively efficient, allocatively efficient. Which is a very fancy word, allocatively efficient. Why is that the case? Well, any other quantity would not be efficient. For example, let's say for some reason we were at this quantity right over here, let's say Q, Q one. Well what happens at this quantity right over here? Well, at this quantity, at this quantity right over here, the marginal benefit is higher than the marginal cost. Marginal benefit is greater than the marginal cost. And so we're leaving a bunch of stuff on the table, the market is leaving a bunch of surplus benefit, you could say total surplus, on the table. And so this benefit that the market could have had, but it does not get, this is called a deadweight loss, deadweight loss, and we talk about it in other videos. Remember, in the allocatively efficient quantity we have this huge total surplus, which is the area under the marginal benefit curve and above the marginal cost curve up until the point of intersection. But if you do a quantity less than that allocatively efficient quantity, your marginal benefit is higher than your marginal cost, and you are leaving, you are leaving all of this total surplus on the table, regardless of how you would have actually allocated it or distributed it between the consumers or the producers. And what if you produced a quantity larger than the allocatively efficient quantity? Once again, very fancy word, so let's say that's Q two, what happens over here? Well, here you're able to take advantage of all of this surplus right over here, this total surplus right over here, but now you're creating negative surplus. 1So, in this part, now all of this area shows 1a net negative benefit, or a net I guess I should say 1a net cost that our market is incurring. 1And so this here it was a deadweight loss because 1we were leaving stuff on the table that we could have had. 1Here we're producing at a cost that our market, 1not just our suppliers are producing at a cost, 1the benefit our market is getting 1is less for each incremental unit, 1is less than, or is far less than the cost 1and so we are incurring a net negative total surplus. 1And so this, too, would be considered, 1this, too, would be considered a dead, let me write that 1in a color you can see, a deadweight loss. 1Deadweight loss we often assume it was, hey, 1we're leaving some total surplus on the table, 1but we also have deadweight loss in the case 1where we are producing unnecessarily 1because the benefit is less than the actual cost. 1And so whether our marginal benefit is greater 1than our marginal cost, or in this case 1our marginal cost is greater than our marginal benefit, 1we are going to produce deadweight loss in either situation. 1And a properly functioning market should be 1producing the quantity that is allocatively efficient. 1In an ideal world and of course all of our models assume 1a lot of assumptions to make things a little bit cleaner, 1so we can do lines to describe market behavior.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
              " - The speed of sound is approximately 768 miles per hour. When an object travels faster than the speed of sound, it creates a sonic boom. And there's actually a whole video on how sonic booms are created, on Khan Academy. I encourage you to watch it, it is actually fascinating. But anyway, back to this question. Write an inequality to represent, s, the speeds at which a moving object creates a sonic boom. So we'll start an inequality, we'll do what they told us to. S, let's see. S has to be, they say when an object travels faster than the speed of sound. So our speed has to be greater than the speed of sound. Our speed has to be greater than the speed of sound, which is 768 miles per hour, so s has to be set greater than 768. Now another way we could have written that is that 768 needs to be less than s. We could have written 768 needs to be less than s. That's another way we could write it. In either case, our speed is going to be greater than 768 miles per hour. We check our answer. Let's do a few more of these. To cook a steak to medium-rare, it needs to have an internal temperature of 135 degrees Fahrenheit. A lower internal temperature will undercook the steak. Write an inequality to represent all of the possible temperatures, t, of an undercooked steak. Well, to be an undercooked steak your temperature is going to be less than 135 degrees. So your temperature, t, is going to be less than. You could either just use the less than on your keyboard, or you could click over here on less than. Your temperature is going to be less than 135. We're assuming your temperature is in Fahrenheit degrees. So your temperature needs to be less than Fahrenheit, less then 135 degrees, if you are going to have an undercooked steak. So, you got that one right. Let's do a couple more. Or one, let's do one more. Simone is going snowboarding tomorrow if the temperature is equal to or colder than five degrees celsius. Write an inequality to represent the temperatures at which Simone will go snowboarding. So you might want to say \"Oh, well \"maybe you know that t is less than five degrees.\" Remember, they're telling us that the temperature is equal to or colder, so she's even willing to go snowboarding if it's less than or equal to five degrees. So instead of just saying less than, let's say less than or, whoops. Let's say less than or equal to five degrees. So this inequality represented here, this shows us all of the temperatures at which she is willing to go snowboarding. We check our answer and we got it right.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1\n",
              " -  What we're going to do in this video is talk about the notion of equilibrium in a macroeconomics context. So let's review a little bit of what we've already studied about aggregate demand and aggregate supply. So this vertical axis here, that is the price level for the economy that we are trying to study, and this horizontal axis right over here, this would be the real GDP for that economy, the real GDP, and now I could draw an aggregate demand curve. In previous videos we've talked at length why economists like to model it as a downward-sloping curve, but once again, take all of these things with a grain of salt. There's a lot of assumptions baked in, and then I could also draw aggregate supply, aggregate supply. This would be short-run aggregate supply, upward-sloping curve, so I'll just call that short-run aggregate supply, and I'll call that short-run aggregate supply one, 'cause we might look at other potential aggregate supply curves, I could also look at other potential aggregate demand curves, but let's just do that for now. So given these curves, what would be the price level and the level of output for this economy? Pause this video and think about it. Well, some of you all might just very naturally say, well, it would be the output and the price level that corresponds to this point of intersection, and if you said that, you would be correct. So this would be our short-run equilibrium output, let me label that. So that right over there is our short-run equilibrium, equilibrium, equilibrium output corresponds to where the short-run aggregate supply intersects to the aggregate demand curve, and then this right over here would be our equilibrium price level. Let's call that PL1. Now why do we feel good that this would be the short-run equilibrium output and this would be the price level? Well, let's imagine what would happen if we were at a lower price level, let's say right over here. At that price level, we see that aggregate demand is outstripping aggregate supply. The output that the aggregate demand wants is much higher than the output of the aggregate the short-run aggregate supply, and so that would be a shortage situation. There's not enough output for all of that demand, and what would likely happen in that situation? Well, the folks producing that output would probably say, hey, I'm gonna charge a little bit more for my output, and as they're charging more they'll say, hey, maybe I'll also produce a little bit more output, and they'll move up the curve towards that equilibrium point, and then on the demand side, people would say, hey, I'm not getting the output I need. I'm willing to pay more for it, but as the cost of that output also goes up, the demand, the output demanded would go down, and we'd end up back at that equilibrium point, and we could do the same thought exercise if, for some reason we were at a price level above PL1. In this situation, aggregate supply, short-run aggregate supply is a good bit higher than aggregate demand, and so you have more output than what is being demanded, and so what's likely to happen? Well, the suppliers, the people producing the output, will say, well, I'm gonna charge a little bit less for my output, and produce less, and then similarly, those demanding will say, hey, there's this glut of output. I'm gonna pay less for that output, but as they're able to pay less for it, they'll say, hey, I want more and more of it, and we get back to the equilibrium point. Now, what we've talked about so far is in the short run, but some of you might be saying, well, what about the long run? And in previous videos we have talked about long-run aggregate supply, and so let's say that this curve right over here represents the long-run aggregate supply curve, and where it intersects the horizontal axis, this Yf, you could view this as the output of this economy at full employment, and it's really the sustainable output of this economy at full employment, and so what's going on in the graph right over here, our equilibrium output is well, our short-run equilibrium output is below our full employment output, and so we have a gap. So there's a negative to go from our full employment output to our equilibrium output, and so if we wanted to think about this in the context of the business cycle, where would we be on it? So let's draw the business cycle. So now I'll make the vertical axis the level of real output. So this would be real GDP right over here, and then on the horizontal axis, this will be the passage of time. That is time, and so what typically So at any given point in time, there will be a Yf. There will be some sustainable potential output for that economy, and when I say sustainable it means, you know, people are sleeping properly, resting properly, you're not unsustainably depleting resources, and so, for example, at this point in time, that might be the Yf, and that may be a few years later. Maybe the population has grown, there's more infrastructure, technology's improved. So now they can sustainably produce more, and then a few years after that, maybe they could produce even more. Population grows, they've thought about better ways to arrange the resources in their economy, technology improves, and so you could imagine a world where that full employment output could just grow in a very nice way like this, where it could just grow nicely like this, but we know that's not the way real economies work. They experience the business cycle, and the business cycle looks more like this. So it will look more like this, where you have your peaks and these troughs, a boom-and-bust cycle, sometimes people will talk about it, and the parts of this curve where you have increasing real GDP, like there, or there, or there, we would call those expansions. So that is economic expansion, and the parts where GDP is receding. So for example, right over here, right over here, we would call those recessions, recessions, but let's go back to our aggregate demand and aggregate supply world right over here. This equilibrium point, Y1, what point could that correspond to on this graph right over here of the business cycle? And let me label that. This is the business the business cycle. Pause this video. Think about what point it could correspond to. Well, we're at a point where a short-run equilibrium output is below our full employment output, our potential output, our sustainable full potential output. So this would correspond to some point where our real GDP is sitting below this blue curve, or this blue line the way I've drawn it, and so Y1 could, for example, be this point, or it could be that point, or it could be that point, and so if it was this one right over here, then that right over there would be Y1 and this right over here would be the Yf, would be the Yf for this point in time. Now, as we go forward in time, this Yf, we see this economic growth, it could move to the right, as population grows, as we have better technology, et cetera, et cetera, et cetera, but you're probably thinking, well, what about other points? Are there other possible scenarios? And my answer to you would be absolutely. So you could imagine a world where the equilibrium output is to the right of our potential output, and I will construct that by making a different short-run aggregate supply curve, although I could also do that by shifting the aggregate demand curve, and we'll do that in future videos, but imagine a situation like this, imagine. So I'll call this short-run aggregate supply two, and now this is our equilibrium, equilibrium output, Y2, and it corresponds to price level, price level two right over here, and notice, here there's a gap, but it's a positive gap. Our actual output is above our sustainable output, and so for example, this could correspond to maybe this point. If it does correspond to this point, so if this was Y2, then this would be our current Yf. I didn't actually shift it, but hopefully you're getting the general idea, and you're saying, well, how can you produce beyond full employment? How can you produce beyond your sustainable potential? Well, this is an economy that is producing output in an unsustainable way. Unemployment is unusually low. You have, maybe, depleting resources. People are working too hard. They're getting stressed out. They're not sleeping properly. However you wanna think about it, it's not considered sustainable, and then you have a third scenario where your short-run equilibrium output actually equals your full employment output, and so that could be this scenario right over here. So this would be our short-run aggregate supply three, and notice over here, our equilibrium output, Y3, is equal to our full employment output, and when this happens, this is considered a long-run equilibrium. So let me write that down. That is considered a long-run equilibrium, equilibrium, and points that correspond to long-run equilibria on this business cycle right over here would be this point right over there, and that point, and that point, and that point. So I'll leave you there. In future videos, 1we will actually think about how aggregate demand 1and short-run aggregate supply will shift, 1and connect it even further 1to the cycles in the business cycle.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1\n",
              "Name: video_transcripts, Length: 4186, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3lYOb2K3kgy"
      },
      "source": [
        "\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# LE = LabelEncoder()\n",
        "# final_data['label'] = LE.fit_transform(final_data['board_syllabus'])\n",
        "# final_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp64MkNB3kg1"
      },
      "source": [
        "# def get_labels(prediction):\n",
        "#     predicted_label =  LE.inverse_transform([prediction])\n",
        "#     return predicted_label[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPgTmJPS3kg4"
      },
      "source": [
        "# get_labels(330)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrQ1BBx0vWUQ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# train_data = pd.concat([train_data,val_data])\n",
        "# train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijn1nIpByb3e"
      },
      "source": [
        "train_features = train_data[\"video_transcripts\"]\n",
        "test_features = test_data[\"video_transcripts\"]\n",
        "train_labels = train_data[\"hierarchy\"]\n",
        "test_labels = test_data[\"hierarchy\"]\n",
        "val_features = val_data[\"video_transcripts\"]\n",
        "val_labels = val_data[\"hierarchy\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prM_km_83khD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5708b286-8963-44b7-9c8e-491e17520d12"
      },
      "source": [
        "train_labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "science>>health-and-medicine>>circulatory-system-diseases           99\n",
              "science>>health-and-medicine>>human-anatomy-and-physiology          65\n",
              "science>>health-and-medicine>>respiratory-system-diseases           55\n",
              "science>>health-and-medicine>>circulatory-system                    54\n",
              "science>>health-and-medicine>>infectious-diseases                   52\n",
              "                                                                    ..\n",
              "math>>ap-statistics>>quantitative-data-ap                            1\n",
              "math>>5th-engage-ny>>engage-5th-module-1                             1\n",
              "science>>chemistry>>states-of-matter-and-intermolecular-forces       1\n",
              "math>>in-in-class-7-math-india-icse>>in-in-7-pair-of-angles-icse     1\n",
              "math>>precalculus>>x9e81a4f98389efdf:vectors                         1\n",
              "Name: hierarchy, Length: 569, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfhPstXJ03oz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f544bd6-f7ec-4b6c-a2bb-90b78051f5e7"
      },
      "source": [
        "test_labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "science>>health-and-medicine>>human-anatomy-and-physiology          24\n",
              "science>>health-and-medicine>>circulatory-system-diseases           22\n",
              "science>>health-and-medicine>>circulatory-system                    17\n",
              "science>>electrical-engineering>>robots                             11\n",
              "science>>biology>>crash-course-bio-ecology                          11\n",
              "                                                                    ..\n",
              "math>>calculus-all-old>>derivative-applications-calc                 1\n",
              "math>>ap-calculus-bc>>bc-advanced-functions-new                      1\n",
              "math>>differential-equations>>first-order-differential-equations     1\n",
              "math>>algebra2>>x2ec2f6f830c9fb89:poly-factor                        1\n",
              "math>>algebra-home>>alg-radical-eq-func                              1\n",
              "Name: hierarchy, Length: 416, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkyM7gqv3khI"
      },
      "source": [
        "question_answer = train_features.values\n",
        "categories = train_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFkS_H_83khL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c69c8c-6d56-4ab9-8d5e-dc8b7754ebd9"
      },
      "source": [
        "question_answer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\" In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\",\n",
              "       \" -  What we're going to do in this video is give ourselves a little bit of a tour of eukaryotic cells. And the first place to start is just to remind ourselves what it means for a cell to be eukaryotic. It means that inside the cell, there are membrane-bound organelles. Now, what does that mean? Well, you could view it as sub-compartments within the cell. Membrane-bound organelles. And in this video in particular, we're going to highlight some of these membrane-bound organelles that make the cells eukaryotic. So let's just start with some of the ingredients that we know is true of all cells. So you'll have your cellular membrane here. I drew it big, so that we have a lot of space to draw things in. So this is our cellular membrane. I'll do some nice shading so you appreciate that it'll actually be three-dimensional. We see so many slices of cells that sometimes we forget that they are more spherical, or that they have three-dimensional shape to them. They're not all spherical. They can have different shapes. Now all cells, and there are some exceptions that we've talked about in previous videos. I should say, most cells will have some genetic information in them in the form of DNA. So that is our DNA, right over there. Now, one of the key characteristics of a eukaryotic cell is that the genetic information is going to be inside a membrane-bound organelle. And that membrane-bound organelle, or the membrane that surrounds the DNA here, that is the nuclear membrane. So let me draw the nuclear membrane right over here, and I'll put some shading in to appreciate that that also is going to be in three dimensions, around the DNA. So that is the first membrane-bound organelle that we're gonna discuss, the nucleus. Now the nucleus, it turns out, is connected to another membrane-bound organelle. And we're gonna study this in future videos, but right here I'm drawing holes or pores in the nuclear membrane. And those pores connect to something, it's a very fancy word called the endoplasmic reticulum. And the endoplasmic reticulum is essentially these layers of these membranes. So I'm gonna do my best job at trying to draw an endoplasmic reticulum. Imagine extending from these pores, going into a space that has really these layered membranes that have a lot of surface area. And I'm not gonna go all the way around this nucleus, but in many cells it will go around, all the way around the nucleus. And this right over here, and this is just a rough diagram. That is our endoplasmic, endoplasmic... Not blasmic, endoplasmic... Endoplasmic reticulum, which I've mentioned in previous videos would be an excellent name for a band. And what goes on in the endoplasmic reticulum is when you are in the process of taking that genetic information from DNA, and as we talk about in other videos it gets transcribed into mRNA. So that mRNA is now containing that information. That mRNA will make its way out of that nuclear membrane through one of these pores, and then make its way to a ribosome that is attached to the membrane of the endoplasmic reticulum. And so that's a ribosome there. I'm gonna do a bunch of ribosomes. And so as we've talked about in previous videos, ribosomes are really where you take that genetic information from that mRNA, and then you translate it into a protein. So the ribosomes are the protein synthesis, so let me label that. So this right over here is a ribosome. And some ribosomes might be attached to the endoplasmic reticulum. Some of them might just be floating out here in the cytoplasm, so that would be a free ribosome. Free ribosome. And even from the point of view of the endoplasmic reticulum, the parts of the endoplasmic reticulum where you have ribosomes attached, this is known as rough endoplasmic reticulum. It's the ribosomes that are making them rough. It looks that way in a microscope. So I'll just say rough ER, for endoplasmic reticulum for short. And then you also have parts of the endoplasmic reticulum where you do not have ribosomes attached. And because that looks smooth through our microscope, it has been called, you can imagine, smooth endoplasmic reticulum. There are things known as golgi bodies. Once again, another fascinating name. You gotta love these names in biology. That look kind of like an endoplasmic reticulum, but detached from the nuclear membrane. So let's say it's something like that. That's my best drawing there. That's a golgi body. And these are really good at packaging molecules, even proteins that might've just been produced, and packaging them so that they can be used outside of the cell, for example. And we'll go into detail in other videos, where a protein might go to the golgi body, get a little envelope around it, get some little processing going on, and then make its way outside of a cell. Now another, and this is maybe one of the most famous membrane-bound organelles outside of the nucleus, is what's known as the powerhouse of the cell, and that is the mitochondria. So I'll draw this mitochondria in magenta, because that's a nice powerful color. So mitochondria. And I love mitochondria because it's fascinating how they even came to be. Mitochondria actually have their own DNA, and all of your mitochondrial DNA comes from your mother. So that's actually very interesting for tracing maternal lineage. But mitochondria, this is where your, I'm gonna say let's see what we could see inside of this. This is where you ATP is produced. This is your mitochondria. It's really the powerhouse of the cell. What's interesting about mitochondria is evolutionary biologists believe that the ancestors of mitochondria, because mitochondria have their own DNA, they might've been independent organisms, independent cells. And at some point in our evolutionary past, they started living in symbiosis inside of what would be the ancestors of our cells. And over time, they became so codependent that they started to replicate together. And mitochondria, in fact, became part of these eukaryotic cells. Now if this eukaryotic cell was a plant cell or maybe an algae cell, you would have something called chloroplasts there. We don't have them because we don't have photosynthesis, but this is a chloroplast. And if you could see inside, you could see the little thylakoid stacks right over here. You could see the thylakoids if you could see inside. And so this right over here is a chloroplast. Chloroplast. And this would be plants and algae. Animals do not have these. And these are where you have your photosynthesis take place. Photosynthesis. Now there's also some membrane-bound organelles that are maybe less famous than the mitochondria or the chloroplast, or for sure the nucleus, and that might be something like a vacuole. And in plants, vacuoles tend to be very big. I could draw it, this is three-dimensional so I'll draw it on top of something that I've drawn before. So if a vacuole right over here, this is a... And in a plant it could be a fairly significant compartment inside. And in fact, it can even give structure to the plant itself because it is so big. And it contains water and enzymes. It's viewed as kind of a storage compartment. But it can also contain enzymes that help digest things, that help break things down so that they can be used in some way. So that is a vacuole. And they don't just exist in plants. They can also exist in animal cells. But in plant cells, they can be very, very, very visible. Now, something that is somewhat related to some of the function that a vacuole plays, that are most associated with animal cells but now there's evidence that they also exist in plant cells, is the idea of a lysosome. So a lysosome right over here, that also is a compartment. And it's going to contain a whole series of enzymes in it that is useful for lysing, you could say, that is useful for breaking down either waste products as the cell lives, or even foreign substances that might not be helpful for the cells. So it's gonna contain a bunch of enzymes, and it helps break down things. Now, I'll leave you there. These aren't all of the structures in eukaryotic cells, but these are enough of the structures so that you can appreciate that there are a lot of membrane-bound organelles in eukaryotic cells. And to be clear, even if I were to show all of the membrane-bound structures, that's not all the complexity of the cell. The big thing to appreciate is that cells are incredibly complex. There's all sorts of structures in here that help transport things and move things around. If you could shrink yourself down and look inside of a cell, it would look more complex than the most complex cities. There's all sorts of activities, things being moved around, shuttled around. The cell itself is replicating and copying things. And so this is just the beginning. We're just starting to scratch the surface of the complexity of the most basic unit of life.\",\n",
              "       \" So once again, we have three equal, or we say three identical objects. They all have the same mass, but we don't know what the mass is of each of them. But what we do know is that if you total up their mass, it's the same exact mass as these nine objects right over here. And each of these nine objects have a mass of 1 kilograms. So in total, you have 9 kilograms on this side. And over here, you have three objects. They all have the same mass. And we don't know what it is. We're just calling that mass x. And what I want to do here is try to tackle this a little bit more symbolically. In the last video, we said, hey, why don't we just multiply 1/3 of this and multiply 1/3 of this? And then, essentially, we're going to keep things balanced, because we're taking 1/3 of the same mass. This total is the same as this total. That's why the scale is balanced. Now, let's think about how we can represent this symbolically. So the first thing I want you to think about is, can we set up an equation that expresses that we have these three things of mass x, and that in total, their mass is equal to the total mass over here? Can we express that as an equation? And I'll give you a few seconds to do it. Well, let's think about it. Over here, we have three things with mass x. So their total mass, we could write as-- we could write their total mass as x plus x plus x. And over here, we have nine things with mass of 1 kilogram. I guess we could write 1 plus 1 plus 1. That's 3. Plus 1 plus 1 plus 1 plus 1. How many is that? 1, 2, 3, 4, 5, 6, 7, 8, 9. And actually, this is a mathematical representation. If we set it up as an equation, it's an algebraic representation. It's not the simplest possible way we can do it, but it is a reasonable way to do it. If we want, we can say, well, if I have an x plus another x plus another x, I have three x's. So I could rewrite this as 3x. And 3x will be equal to? Well, if I sum up all of these 1's right over here-- 1 plus 1 plus 1. We're doing that. We have 9 of them, so we get 3x is equal to 9. And let me make sure I did that. 1, 2, 3, 4, 5, 6, 7, 8, 9. So that's how we would set it up. And so the next question is, what would we do? What can we do mathematically? Actually, to either one of these equations, but we'll focus on this one right now. What can we do mathematically in order to essentially solve for the x? In order to figure out what that mystery mass actually is? And I'll give you another second or two to think about it. Well, when we did it the last time with just the scales we said, OK, we've got three of these x's here. We want to have just one x here. So we can say, whatever this x is, if the scale stays balanced, it's going to be the same as whatever we have there. There might be a temptation to subtract two of the x's maybe from this side, but that won't help us. And we can even see it mathematically over here. If we subtract two x's from both sides, on the left-hand side you're going to have 3x minus 2x. And on the right-hand side, you're going to have 9 minus 2x. And you're just going to be left with 3 of something minus 2 something is just 1 of something. So you will just have an x there if you get rid of two of them. But on the right-hand side, you're going to get 9 minus 2 x's. So the x's still didn't help you out. You still have a mystery mass on the right-hand side. So that doesn't help. So instead, what we say is-- and we did this the last time. We said, well, what if we took 1/3 of these things? If we take 1/3 of these things and take 1/3 of these things, we should still get the same mass on both sides because the original things had the same mass. And the equivalent of doing that mathematically is to say, why don't we multiply both sides by 1/3? Or another way to say it is we could divide both sides by 3. Multiplying by 1/3 is the same thing as dividing by 3. So we're going to multiply both sides by 1/3. When you multiply both sides by 1/3-- visually over here, if you had three x's, you multiply it by 1/3, you're only going to have one x left. If you have nine of these one-kilogram boxes, you multiply it by 1/3, you're only going to have three left. And over here, you can even visually-- if you divide by 3, which is the same thing as multiplying by 1/3, you divide by 3. So you divide by 3. You have an x is equal to a 1 plus 1 plus 1. An x is equal to 3. Or you see here, an x is equal to 3. Over here you do the math. 1/3 times 3 is 1. You're left with 1x. So you're left with x is equal to 9 times 1/3. Or you could even view it as 9 divided by 3, which is equal to 3.\",\n",
              "       ...,\n",
              "       ' - Let\\'s say that I have a circle. My best attempt to draw a reasonably perfect circle. So, there you go, not too bad, it\\'s a little bit of a hairy circle but you get the idea. So, this is a circle, this is the center of the circle, and let\\'s say that I have an arc along this circle. So, I\\'ll do the arc in green. So, I have an arc that is part of the circle, and it subtends an angle, so that\\'s my arc. Right over there, and it subtends an angle, and the angle that it subtends, so what I mean subtends, you take each of the endpoints of the arc, go to the center of the circle, go to the center of the circle just like this, and so it subtends angle theta, right over here, so it subtends angle theta, and let\\'s say that we know that angle theta is equal to two radians. So my question to you is what fraction of the entire circumference is this green arc? What fraction of the entire circumference is this green arc? And like always, pause the video, and give it a go. (laughs) All right, so let\\'s think through it a little bit. So, you might say well how do I know that, I don\\'t know what the radius of this thing is, I don\\'t, how do I think through this? And we just have to remind ourselves what radians mean, what radians mean. If an arc subtends the angle of two radians, that means that the arc itself is two \"radiuseseses\" long. (laughs) So, this right over here, let me make this a little clearer, so this, if the radius is r, if this radius is, I already used that color, if this radius... I have trouble switching colors (laughs) all right. If this radius is length r, then the length, if this angle is two radians, then the arc that subtends it is going to be two radiuses long, so this length right over here, is two radiuses. Now, what fraction of the entire circumference is that? Well, the entire circumference, we know, we know this from basic geometry, the entire circumference is two pi times the radius, or you can say it\\'s two pi radii, two pi \"radiuseses\", (laughs) two pi radii is the correct way to say it. So, what fraction is it? It\\'s two radii, it\\'s two radii, over two pi radii, over two pi radii, twos cancel out, rs cancel out, and so it is one \"pith\", (laughs) I guess you could say, it is one over pi of the total circumference.',\n",
              "       \" - So let's look at the female reproductive cycle. The female reproductive cycle refers to the maturation of eggs within the ovaries. The ovaries initially created these eggs during gestation. In other words, when a baby girl is in her mother's womb, the baby girl's entire egg supply will be created but will remain in an inactive state. This process of egg creation is called oogenesis. Then, once she grows up a bit and reaches puberty, her reproductive cycle will start, and one egg in that egg supply in her ovaries will mature or become activated each month, and that allows it to be fertilized by sperm. By the way, another word for egg is oocyte. After an egg matures, it's pushed out of the ovary in a process called ovulation. The other major function of the ovaries is to secrete the female sex hormones, estrogen, progesterone, and one called inhibin, and we'll talk about their functions a little bit later on. So let's first discuss how the eggs are made in the ovary in the first place. So early in uteral development, precursor germ cells, which are called oogonia, and those are homologous to spermatagonia in males, these oogonia undergo a ton of mitotic divisions to make more of themselves. And then, at about the 7th month of development, these divisions stop, and all of the ones that have been produced, which is actually about two to four million, are all she'll have for the rest of her life, and that turns out to be about one to two million per ovary. So while she's still in fetal development, all of these oogonia that have been produced, they all develop into the next stage, which is a primary oocyte. And just remember that the two oo's refers to egg, and the cyte, C-Y-T-E, refers to cell. So this just means egg cell, in case you were wondering. And let me also just mention, on a chromosomal level these oogonia, the germ cells, they're 2n, which means they have two copies of each chromosome, and the primary oocytes are the same. They're also 2n. And then these primary oocytes, they begin meiosis 1, and meiosis is what our germ cells use to reduce our chromosome copy number, and by that, I just mean the number of copies of DNA that we have. So they start this process of meiosis 1, but they don't actually finish it. They just kind of get about halfway through, and then they stop. So they're stuck as these big cells. So they're still primary oocytes, but they're said to be in meiotic arrest. So when the female who's been developing in her mom's womb, when she's born, her primary oocytes are in meiotic arrest. So the question is, do they stay like this? And the answer is, some do and some don't. Let's zoom in on this reproductive system to try to explain. So this is just a closeup of the major parts of the female reproductive system, and I've cut away parts of the uterus and the uterine tubes and the ovaries so you can see sort of the inside and the outsides of both structures. And this is our key organ here. This is the ovary. So the question was, do these primary oocytes that are stuck in meiotic arrest, do they stay like that? So the answer is that the ones that are sort of destined to be ovulated, that is, to be pushed out of the ovary right about here and then to be picked up by the fimbriae and then travel along the uterine tube here, those ones get past meiotic arrest. But most of them sort of die off while they're still stuck in that meiotic arrest phase as a primary oocyte. So I've mentioned the ones that sort of get out of the meiotic arrest phase and move on to develop into secondary oocytes that are able to then fuse with sperm, but when exactly does that happen? Well, it starts at puberty. So they actually stay in this phase as primary oocytes up here, in meiotic arrest for like 12 to 13 years, give or take, and only then do they start moving forward with development by finishing off that first part of meiosis that they started and splitting into two secondary oocytes. And actually, that's not exactly true, even though that's what we'd expect. What actually happens is one primary occyte it attempts to split into two secondary oocytes, but that's not exactly what happens. What does happen is that one of the developing daughter cells develops beautifully into a normal secondary oocyte from the primary oocyte, but it turns out that when they do complete the first part of meiosis, something really interesting happens. One of these cells receives basically all the cytoplasm. So the chromosome copy number is halved, but basically all the cytoplasm is kept in one cell. So this little guy over here that didn't get much cytoplasm, it still has a full complement of chromosomes, but it still ends up being pretty small and not really very functional. So it kind of withers away and dies, and it's called a polar body. So you end up with this really large secondary oocyte, and this is what ends up getting ovulated. And so now you might be thinking, well, meiosis is two steps, right? When does the second step happen? And that's a good question. So again, ovulation happens roughly here with the secondary oocyte coming out, and this secondary oocyte sort of just hangs out in the uterine tubes, and a sperm comes along and fertilizes the egg. So let's look at that down here. So you have your uterine tube here, and you have your egg. That's a secondary oocyte now. And then a sperm is coming along, and the sperm fuses with the egg after fertilizing it. And so the sperm fertilizes the egg and fuses with it. And so, let's just zoom in on what's happening there. So here you have your big secondary oocyte, and then you have your sperm that sort of, let's say that the nucleus of the sperm is right here. It's inside the egg already. This is the nucleus of the sperm. And here's the nucleus of the secondary oocyte. Well this is when meiosis 2 happens, so the second half of meiosis. So as this sperm nucleus is traveling toward the egg nucleus to create a joint nucleus, meiosis 2 occurs, and the oocyte reduces its chromosome copy number by creating another polar body, so a second polar body that kind of divides off the cell. So the oocyte cuts its chromosome copy number in half again, and so this little bit of DNA here that's just an extra copy of the DNA the egg already has, it divides off the cell in the form of another polar body that doesn't really have that much cytoplasm, just like the first one. So again, it leaves its nutrient-rich cytoplasm behind for the sperm and the egg. And by the way, the egg has changed its name now from a secondary oocyte to an ovum, but it won't be an ovum for long. Once the sperm nucleus fuses with the egg nucleus, then it becomes a zygote. So let me just clarify that if the egg doesn't get fertilized by a sperm that comes along, then it doesn't complete that second meiotic division that it did right here, and it just gets discharged from the body in menstruation as a secondary oocyte and not as an ovum, because the name ovum is reserved for the oocyte only once it's been fertilized. So those are the basic concepts behind what goes on with egg development.\",\n",
              "       \" -  We're now in the home stretch. We just have to evaluate the curl of f and then this dot product and then evaluate this double integral. So let's work on the curl of F. So the curl of f is going to be equal to, and I just remember it as the determit, so we have our i, j, k components, and it's really you could imagine it's the del operator crossed with the actual vector. So the del operator, I'll write this in a different color just to ease the monotony, so this is partial with respect to x, partial with respect to y, partial with respect to z, and then our vector field, I copied and pasted it right over here. It is just equal to negative y squared, is our i component, x is our j component, and Z squared is our k component. And so this is going to be equal to, this is going to be equal to i, is going to be equal to i times the partial of Z squared with respect to y. Well, there's the Z squared is just a constant with respect to y so the partial of Z squared with respect to y is just going to be zero, so this is going to be zero. Minus the partial of x with respect to z. Well, once again this is just a constant when you think in terms of z, so that's just going to be zero. So that's nice simplification, and then we're gonna have minus j, we need our little checkerboard patterns, we put a negative in front of the j, minus j and so we'll have the partial of x, the partial of z squared with respect to x, that's zero again, and then minus the partial of negative y squared with respect to z, well that's zero again, and then finally we have our k component, k, so plus, plus k, and k, we're gonna have the partial of x with respect to x, well that actually gives us a value that's just gonna be one minus the partial of negative y squared with respect to y. So the partial of negative y squared with respect to y is negative two y and we're subtracting that, so it's going to be plus, plus, two y. So curl of f simplifies to just, all of this is just zero up here, is just one plus two y times k or k times one plus two y. And so if we go back to this right up here, if we go back up to that, we're going to get let me re-write the integral so zero to one and that's our r, our r parameter is gonna go from zero to one, theta is gonna go from zero to two pi. And now curl of f has simplified to, and I won't skip any steps although it's tempting, it's one plus two y, and actually instead of writing two y, let me write it in terms of the parameters. We saw it up here, y was r sine theta, if I remember correctly, right, y was r sine theta. So let me write y that way. Two times r sine theta k. And we're gonna dot this, we're gonna take the dot product of that with this right over here, with r times j plus r times k, d theto d r. And so we take the dot product, this thing only has a k component, the j component is zero, so when you take the dot product with this j component you're gonna get zero. And neither of them you actually even have an i component. And so the inside is just going to simplify to this piece right over here is going to simplify to, we're just gonna have to think about the k components, cause everything else is zero, so it's gonna be r times this and we're done! So it's gonna be r plus two r squared sine theta, d theta d r, d theta d r and, once again, theta goes from zero to two pi and r goes from zero to one. And now this is just a straight-up double integral. We just have to evaluate this thing. And so, first we take the antiderivative with respect to theta, so the antiderivative with respect to theta is going to give us, so this is going to be giving, so we're going to focus on theta first, so the antiderivative of r with respect to theta is just r theta, you can just do r as a constant, and then the antiderivative of this, antiderivative of sine of theta is negative cosine of theta. So this is gonna be negative two r squared cosine of theta. And we're gonna evaluate it from zero to two pi. And then we have the outside integral, which I will, I'll re-color in yellow, re-color in yellow, so we'll still have to integrate with respect to r and r's gonna go from zero to one. But inside right over here, if we evaluate all of this business right over here at two pi, we get two pi r, two pi r, that's that right over there, minus... Cosine of two pi is just one. So it's minus two r squared and then from that, we're going to subtract from that, we're gonna subtract this evaluated zero. Well r times zero is just zero, and then cosine of zero is one. So it's just minus two r squared, or negative two r squared, negative two r squared. And at this negative and this negative, you get a positive, and but then you have a negative two r squared and then a plus two r squared it's just going to cancel out, that and that cancel out, and so this whole thing has simplified quite nicely to a simple definite integral, zero to one of two pi, two pi r dr, and the antiderivative of this is just going to be pi r squared, so we're just gonna evaluate pi r squared from zero to one, when you evaluate it at one, you get pi; when you evaluate it at zero, you just get zero, so you get pi minus zero, which is equal to, and now we deserve a drumroll 'cause we've been doing a lot of work over many videos, this is equal to pi. So just to remind ourselves what we've done over the last few videos, we had this line integral that we were trying to figure out, and instead of directly evaluating the line integral, which we could do and I encourage you to do so, and if I have time, I might do it in the next video, instead of directly evaluating that line integral, we used Stokes theorem to say, oh we could actually instead say that that's the same thing as a surface integral over a piecewise-smooth boundary over piecewise-smooth surface that this path is the boundary of, and so we evaluated this surface intergal and eventually, with a good bit of, little bit of calculation, we got to evaluating it to be equal to pi.\"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ian7gSDE3khR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c019163-8d69-4bf4-c00d-06166df13d7f"
      },
      "source": [
        "categories"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['math>>multivariable-calculus>>multivariable-derivatives',\n",
              "       'science>>ap-biology>>natural-selection',\n",
              "       'math>>pre-algebra>>pre-algebra-equations-expressions', ...,\n",
              "       'math>>engageny-geo>>geo-5',\n",
              "       'science>>health-and-medicine>>human-anatomy-and-physiology',\n",
              "       'math>>multivariable-calculus>>greens-theorem-and-stokes-theorem'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fepGiggpqOQx"
      },
      "source": [
        "# val_features = test_features.values\n",
        "# val_labels = test_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gNnga-onGn9"
      },
      "source": [
        "# this method can be used to project from euclidean space to hyperbolic space\n",
        "def exponential_map(vector):\n",
        "        norm_v = np.linalg.norm(vector, axis=1)\n",
        "        coef = np.tanh(norm_v) / norm_v\n",
        "        second_term = vector * coef[:, None]\n",
        "        return second_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNdG6eFaEMB"
      },
      "source": [
        "# this method can be used to project from euclidean space to hyperbolic space\n",
        "def tensor_exponential_map(vector):\n",
        "      vector_norm = vector.norm(dim=-1, p=2, keepdim=True).clamp_min(1e-15)\n",
        "      gamma_1 = torch.nn.functional.tanh(vector_norm) * (vector / vector_norm)\n",
        "      return gamma_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN1zRXMOXwLK"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "from bokeh.io import output_file, output_notebook, show\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.util.hex import hexbin\n",
        "from bokeh.models import HoverTool\n",
        "from bokeh import colors\n",
        "\n",
        "from gzip import open as gopen\n",
        "\n",
        "import gensim.models.poincare as poincare\n",
        "poincare_model = poincare.PoincareModel.load(\"taxonomy_khan_acad_embedding_20.pkl\")\n",
        "def get_poincare_embeddings_data(taxonomy):\n",
        "  cleaned_taxonomy = []\n",
        "  for value in taxonomy:\n",
        "      value = value.split(\">>\")\n",
        "      cleaned_taxonomy.append( list(tok.lower() for tok in value) )\n",
        "  return cleaned_taxonomy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPAl0TNuX6mx"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "\n",
        "poincare_emb_data = get_poincare_embeddings_data(categories)\n",
        "poincare_val = get_poincare_embeddings_data(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdBbsrO9zp2p"
      },
      "source": [
        "# poincare_embedding =  [[ poincare_model.kv.get_vector(str(x)) for x in taxonomy ] for taxonomy in poincare_emb_data ]\n",
        "# np.linalg.norm(poincare_embedding[1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAWBKw-U3azu"
      },
      "source": [
        "# for index,embedding in enumerate(poincare_embedding):\n",
        "#   poincare_embedding[index] = np.stack(embedding,axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTlCYX9Q34JG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p834oM1Pzzu8"
      },
      "source": [
        "# np.array(poincare_embedding).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq2G2XIrYjy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec40659-a8c9-49e6-ff30-573d4501a9ec"
      },
      "source": [
        "poincare_embedding =  [exponential_map(np.expand_dims( np.hstack(  [ poincare_model.kv.get_vector(str(x)) for x in taxonomy ] ),axis=0)) for taxonomy in poincare_emb_data ]\n",
        "np.linalg.norm(poincare_embedding[1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.899765215781272"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWL3XpuJq5iW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925f2282-efb4-43e9-93cd-9da40b25442f"
      },
      "source": [
        "poincare_embedding_val = [exponential_map(np.expand_dims( np.hstack(  [ poincare_model.kv.get_vector(str(x)) for x in taxonomy ] ),axis=0)) for taxonomy in poincare_val ]\n",
        "np.linalg.norm(poincare_embedding_val[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8857759138716806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ5VB0_frWRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7891b92-fb90-4614-fc0a-5fc98dbd5def"
      },
      "source": [
        "poincare_val[100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['math', 'cc-seventh-grade-math', 'cc-7th-ratio-proportion']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cli5GQTE6yl5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085dc101-1a19-42ab-a0b3-66666b3e1826"
      },
      "source": [
        "poincare_embedding_val[100].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II_vjGgRLqeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b28f22-96e8-43eb-d6f8-9df2338c5306"
      },
      "source": [
        "poincare_embedding[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbl7uFDmEdHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04731976-3703-4d6b-c16c-6cfd1da81776"
      },
      "source": [
        "max_val_train = 0\n",
        "max_emb =None\n",
        "for embedding in poincare_embedding:\n",
        "  val = embedding.shape[1]\n",
        "  if val >max_val_train:\n",
        "    max_val_train=val\n",
        "    max_emb =embedding\n",
        "max_val_train\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nrk2qTerql2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc4149e-a9d0-421e-d698-f0e8c26ab11a"
      },
      "source": [
        "max_val_val = 0\n",
        "max_emb =None\n",
        "for embedding in poincare_embedding_val:\n",
        "  val = embedding.shape[1]\n",
        "  if val >max_val_val:\n",
        "    max_val_val=val\n",
        "    max_emb =embedding\n",
        "max_val_val\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0RXfn5xEf5t"
      },
      "source": [
        "def get_concat_embedding(poincare_embedding,max_val):\n",
        "  concatenated_embedding = []\n",
        "  for embedding in poincare_embedding:\n",
        "    if embedding.shape[1] < max_val:\n",
        "      new_embedding = np.append(embedding, np.expand_dims(np.zeros(max_val-embedding.shape[1]),axis=0),axis=1)\n",
        "    else:\n",
        "      new_embedding = embedding\n",
        "    concatenated_embedding.append(np.squeeze(new_embedding,axis=0))\n",
        "  return concatenated_embedding\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3CBrpnmpYU-"
      },
      "source": [
        "concat_embedding_train = get_concat_embedding(poincare_embedding,max_val_train)\n",
        "concat_embedding_val = get_concat_embedding(poincare_embedding_val,max_val_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svuJZqfpLu_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00583620-bbb8-4184-9fe9-9020c37a4ae4"
      },
      "source": [
        "concat_embedding_train[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCQswt5pMBRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64360b66-a283-45c5-db26-d3cc4ae98bf0"
      },
      "source": [
        "poincare_embeddings_train = np.stack(concat_embedding_train, axis=0)\n",
        "poincare_embeddings_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4188, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qy10KlXpxp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1497351-7232-4f38-d526-43de3ea45e53"
      },
      "source": [
        "poincare_embeddings_val = np.stack(concat_embedding_val, axis=0).squeeze()\n",
        "poincare_embeddings_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(924, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ZeuHc63khU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bc672e-0ebe-4288-e607-59b82032abed"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in question_answer:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', question_answer[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:   In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\n",
            "Token IDs: tensor([  101,  1999,  1996,  2197,  3232,  1997,  6876,  2057,  2387,  2008,\n",
            "         2057,  2064,  6235,  1037, 10543,  2011,  1037,  2597,  9207,  1011,\n",
            "        11126,  3853,  1012,  1998,  1999,  2200,  2236,  3408,  1010,  2009,\n",
            "         2052,  2022,  1996,  1060,  2597,  2004,  1037,  3853,  1997,  2051,\n",
            "         2335,  1996,  3131,  9207,  1999,  1996,  9876,  3257,  1012,  4606,\n",
            "         1996,  1061,  2597,  2004,  1037,  3853,  1997,  2051,  2335,  1996,\n",
            "         3131,  5125,  1999,  1996,  7471,  3257,  1012,  1998,  2023,  2097,\n",
            "         7687,  6235,  2023,  1011,  1011,  2295,  1010,  2065,  2017,  2064,\n",
            "         5674,  1037, 10811,  1998,  2292,  1005,  1055,  2360,  1996, 16381,\n",
            "         1056,  5836,  2051,  1012,  2009,  1005,  2222,  6235,  2073,  1996,\n",
            "        10811,  2003,  2012,  2151,  2445,  2051,  1012,  1998,  2065,  2057,\n",
            "         2359,  1037,  3327,  7774,  2057,  2064,  2360,  1010,  2092,  1010,\n",
            "         2023,  2069, 12033,  2005,  2070,  7774,  1011,  1011,  2057,  1005,\n",
            "         2128,  7149,  1010,  2009,  1005,  1055,  1054,  1997,  1056,  1012,\n",
            "         1998,  2009,  1005,  1055,  2069, 12711,  2090,  1056,  2108,  3618,\n",
            "         2084,  1037,  1998,  2625,  2084,  1038,  1012,  1998,  2017,  2113,\n",
            "         1010,  2008,  2052,  6235,  2070,  7774,  1999,  2048,  9646,  1012,\n",
            "         2074,  2033,  2074,  4009,  2009,  2182,  1012,  2023,  2003,  2035,\n",
            "         1037,  3319,  1997,  2428,  1010,  1996,  2197,  2048,  6876,  1012,\n",
            "         2061,  2023,  7774,  1010,  2009,  2453,  2298,  2242,  2066,  2008,\n",
            "         2073,  2023,  2003,  2073,  1056,  2003,  5020,  2000,  1037,  1012,\n",
            "         2008,  1005,  1055,  2073,  1056,  2003,  5020,  2000,  1038,  1012,\n",
            "         1998,  2061,  1054,  1997,  1037,  2097,  2022,  2023,  9207,  2157,\n",
            "         2182,  2008,  4515,  2012,  2008,  2391,  1012,  1998,  2059,  2004,\n",
            "         1056,  2030,  2065,  2017,  2064,  5674,  1996, 16381,  2108,  2051,\n",
            "         1010,  2009,  2987,  1005,  1056,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjkhiN3pAfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b628d3-9f54-49e7-85ab-aebe7e81177b"
      },
      "source": [
        "input_ids_val = []\n",
        "attention_masks_val = []\n",
        "\n",
        "for sent in val_features:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids_val.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks_val.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
        "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', question_answer[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:   In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\n",
            "Token IDs: tensor([  101,  1999,  1996,  2197,  3232,  1997,  6876,  2057,  2387,  2008,\n",
            "         2057,  2064,  6235,  1037, 10543,  2011,  1037,  2597,  9207,  1011,\n",
            "        11126,  3853,  1012,  1998,  1999,  2200,  2236,  3408,  1010,  2009,\n",
            "         2052,  2022,  1996,  1060,  2597,  2004,  1037,  3853,  1997,  2051,\n",
            "         2335,  1996,  3131,  9207,  1999,  1996,  9876,  3257,  1012,  4606,\n",
            "         1996,  1061,  2597,  2004,  1037,  3853,  1997,  2051,  2335,  1996,\n",
            "         3131,  5125,  1999,  1996,  7471,  3257,  1012,  1998,  2023,  2097,\n",
            "         7687,  6235,  2023,  1011,  1011,  2295,  1010,  2065,  2017,  2064,\n",
            "         5674,  1037, 10811,  1998,  2292,  1005,  1055,  2360,  1996, 16381,\n",
            "         1056,  5836,  2051,  1012,  2009,  1005,  2222,  6235,  2073,  1996,\n",
            "        10811,  2003,  2012,  2151,  2445,  2051,  1012,  1998,  2065,  2057,\n",
            "         2359,  1037,  3327,  7774,  2057,  2064,  2360,  1010,  2092,  1010,\n",
            "         2023,  2069, 12033,  2005,  2070,  7774,  1011,  1011,  2057,  1005,\n",
            "         2128,  7149,  1010,  2009,  1005,  1055,  1054,  1997,  1056,  1012,\n",
            "         1998,  2009,  1005,  1055,  2069, 12711,  2090,  1056,  2108,  3618,\n",
            "         2084,  1037,  1998,  2625,  2084,  1038,  1012,  1998,  2017,  2113,\n",
            "         1010,  2008,  2052,  6235,  2070,  7774,  1999,  2048,  9646,  1012,\n",
            "         2074,  2033,  2074,  4009,  2009,  2182,  1012,  2023,  2003,  2035,\n",
            "         1037,  3319,  1997,  2428,  1010,  1996,  2197,  2048,  6876,  1012,\n",
            "         2061,  2023,  7774,  1010,  2009,  2453,  2298,  2242,  2066,  2008,\n",
            "         2073,  2023,  2003,  2073,  1056,  2003,  5020,  2000,  1037,  1012,\n",
            "         2008,  1005,  1055,  2073,  1056,  2003,  5020,  2000,  1038,  1012,\n",
            "         1998,  2061,  1054,  1997,  1037,  2097,  2022,  2023,  9207,  2157,\n",
            "         2182,  2008,  4515,  2012,  2008,  2391,  1012,  1998,  2059,  2004,\n",
            "         1056,  2030,  2065,  2017,  2064,  5674,  1996, 16381,  2108,  2051,\n",
            "         1010,  2009,  2987,  1005,  1056,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDW74Ny3khj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ad38c5-4457-4c60-ef83-bbd16d65e87b"
      },
      "source": [
        "num_classes = len(list(set(categories)))\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmaLk5Ab3khl"
      },
      "source": [
        "\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "train_poincare_tensor = torch.tensor(poincare_embeddings_train,dtype=torch.float)\n",
        "val_poincare_tensor = torch.tensor(poincare_embeddings_val,dtype=torch.float)\n",
        "\n",
        "val_dataset = TensorDataset(input_ids_val,attention_masks_val,val_poincare_tensor)\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "train_dataset = TensorDataset(input_ids, attention_masks, train_poincare_tensor)\n",
        "\n",
        "# Create a 80-20train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "# train_size = int(0.90 * len(dataset))\n",
        "# val_size = len(dataset) - train_size\n",
        "\n",
        "# # Divide the dataset by randomly selecting samples.\n",
        "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# print('{:>5,} training samples'.format(train_size))\n",
        "# print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_lTinod3kho"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vduf9fOMviK"
      },
      "source": [
        "import numpy as np\n",
        "from torch.autograd import Function\n",
        "class Distance(Function):\n",
        "    @staticmethod\n",
        "    def grad(x, v, sqnormx, sqnormv, sqdist, eps):\n",
        "        alpha = (1 - sqnormx)\n",
        "        beta = (1 - sqnormv)\n",
        "        z = 1 + 2 * sqdist / (alpha * beta)\n",
        "        a = ((sqnormv - 2 * torch.sum(x * v, dim=-1) + 1) / torch.pow(alpha, 2))\\\n",
        "            .unsqueeze(-1).expand_as(x)\n",
        "        a = a * x - v / alpha.unsqueeze(-1).expand_as(v)\n",
        "        z = torch.sqrt(torch.pow(z, 2) - 1)\n",
        "        z = torch.clamp(z * beta, min=eps).unsqueeze(-1)\n",
        "        return 4 * a / z.expand_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, u, v, eps):\n",
        "        squnorm = torch.clamp(torch.sum(u * u, dim=-1), 0, 1 - eps)\n",
        "        sqvnorm = torch.clamp(torch.sum(v * v, dim=-1), 0, 1 - eps)\n",
        "        sqdist = torch.sum(torch.pow(u - v, 2), dim=-1)\n",
        "        ctx.eps = eps\n",
        "        ctx.save_for_backward(u, v, squnorm, sqvnorm, sqdist)\n",
        "        x = sqdist / ((1 - squnorm) * (1 - sqvnorm)) * 2 + 1\n",
        "        # arcosh\n",
        "        z = torch.sqrt(torch.pow(x, 2) - 1)\n",
        "        return torch.log(x + z)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        u, v, squnorm, sqvnorm, sqdist = ctx.saved_tensors\n",
        "        g = g.unsqueeze(-1)\n",
        "        gu = Distance.grad(u, v, squnorm, sqvnorm, sqdist, ctx.eps)\n",
        "        gv = Distance.grad(v, u, sqvnorm, squnorm, sqdist, ctx.eps)\n",
        "        return g.expand_as(gu) * gu, g.expand_as(gv) * gv, None\n",
        "def distanceTo(vector1,vector2):\n",
        "        return Distance.apply(vector1,vector2,1e-5)\n",
        "        # vector1 = vector1.detach().cpu().numpy()\n",
        "        # vector2 = vector2.detach().cpu().numpy()\n",
        "        # euclidean_dists = np.linalg.norm(vector1 - vector2)  \n",
        "        # gamma = 1 + 2 * ((euclidean_dists ** 2) / ((1-(np.linalg.norm(vector1))) * (1-np.linalg.norm(vector2))))  # (1 + neg_size, batch_size)\n",
        "        # poincare_dists = np.arccosh(gamma) \n",
        "        # return torch.tensor(poincare_dists,dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN6jdKp0uhO3"
      },
      "source": [
        "\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
        "from random import randint\n",
        "import geoopt.manifolds.stereographic.math as pm\n",
        "import geoopt.optim.rsgd as rsgd_\n",
        "import geoopt.optim.radam as radam_\n",
        "# from hyrnn.nets import MobiusLinear\n",
        "from geoopt.tensor import ManifoldParameter\n",
        "from geoopt.manifolds.stereographic import PoincareBall\n",
        "from tqdm import tqdm\n",
        "import geoopt\n",
        "import time\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
        "from random import randint\n",
        "import geoopt.optim.rsgd as rsgd_\n",
        "import geoopt.optim.radam as radam_\n",
        "# from hyrnn.nets import MobiusLinear\n",
        "from geoopt.tensor import ManifoldParameter\n",
        "import time\n",
        "import argparse\n",
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "\n",
        "def create_ball(ball=None, c=None):\n",
        "    \"\"\"\n",
        "    Helper to create a PoincareBall.\n",
        "    Sometimes you may want to share a manifold across layers, e.g. you are using scaled PoincareBall.\n",
        "    In this case you will require same curvature parameters for different layers or end up with nans.\n",
        "    Parameters\n",
        "    ----------\n",
        "    ball : geoopt.PoincareBall\n",
        "    c : float\n",
        "    Returns\n",
        "    -------\n",
        "    geoopt.PoincareBall\n",
        "    \"\"\"\n",
        "    if ball is None:\n",
        "        assert c is not None, \"curvature of the ball should be explicitly specified\"\n",
        "        ball = geoopt.PoincareBall(c)\n",
        "    # else trust input\n",
        "    return ball\n",
        "\n",
        "\n",
        "class MobiusLinear(torch.nn.Linear):\n",
        "    def __init__(self, *args, nonlin=None, ball=None, c=1.0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # for manifolds that have parameters like Poincare Ball\n",
        "        # we have to attach them to the closure Module.\n",
        "        # It is hard to implement device allocation for manifolds in other case.\n",
        "        self.ball = create_ball(ball, c)\n",
        "        if self.bias is not None:\n",
        "            self.bias = geoopt.ManifoldParameter(self.bias, manifold=self.ball)\n",
        "        self.nonlin = nonlin\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return mobius_linear(\n",
        "            input,\n",
        "            weight=self.weight,\n",
        "            bias=self.bias,\n",
        "            nonlin=self.nonlin,\n",
        "            ball=self.ball,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.eye_(self.weight)\n",
        "        self.weight.add_(torch.rand_like(self.weight).mul_(1e-3))\n",
        "        if self.bias is not None:\n",
        "            self.bias.zero_()\n",
        "\n",
        "\n",
        "# package.nn.functional.py\n",
        "def mobius_linear(input, weight, bias=None, nonlin=None, *, ball: geoopt.PoincareBall):\n",
        "    output = ball.mobius_matvec(weight, input)\n",
        "    if bias is not None:\n",
        "        output = ball.mobius_add(output, bias)\n",
        "    if nonlin is not None:\n",
        "        output = ball.logmap0(output)\n",
        "        output = nonlin(output)\n",
        "        output = ball.expmap0(output)\n",
        "    return output\n",
        "# Neural Classifierwork\n",
        "class MulticlassClassifier(nn.Module):\n",
        "    def __init__(self,bert_model_path):\n",
        "        super(MulticlassClassifier,self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_path,output_hidden_states=False,output_attentions=False)\n",
        "        for param in self.bert.parameters():\n",
        "          param.requires_grad=True\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = MobiusLinear(768, 384)\n",
        "        self.fc2 = MobiusLinear(384, 60)\n",
        "\n",
        "    def forward(self,tokens,masks):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks)\n",
        "        hyerbolic_transform = tensor_exponential_map(pooled_output)\n",
        "        x = self.fc1(hyerbolic_transform)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MyHingeLoss(torch.nn.Module):\n",
        "    def __init__(self, margin):\n",
        "        super(MyHingeLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = 0\n",
        "        for i in range(len(output)):\n",
        "            text_emb = output[i]\n",
        "            t_label = target[i]\n",
        "            j = randint(0, len(output)-1)\n",
        "            while j == i:\n",
        "                j = randint(0, len(output)-1)\n",
        "            t_j = target[j]\n",
        "            loss += torch.relu( self.margin + \\\n",
        "                            distanceTo(t_label, text_emb) - distanceTo(t_j, text_emb) )\n",
        "        return loss / len(output)\n",
        "\n",
        "class MyHingeLoss_cos(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(MyHingeLoss_cos, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = 0\n",
        "        for i in range(len(output)):\n",
        "            text_emb = output[i]\n",
        "            t_label = target[i]\n",
        "            j = randint(0, len(output)-1)\n",
        "            while j == i:\n",
        "                j = randint(0, len(output)-1)\n",
        "            t_j = target[j]\n",
        "            loss += torch.relu( self.margin - cos(t_label, text_emb) + cos(t_j, text_emb) )\n",
        "        return loss / len(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHYyjxMDIx2U"
      },
      "source": [
        "from transformers import BertModel, AdamW, BertConfig\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2tmAMlw3khr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7ac783-f495-4592-c938-9f0b91b9964e"
      },
      "source": [
        "from transformers import BertModel, AdamW, BertConfig\n",
        "\n",
        "# Loads BertModel, the pretrained BERT model with a single \n",
        "model = MulticlassClassifier('bert-base-uncased')\n",
        "# model.load_state_dict(torch.load(\"IR_project_model_hyperbolic_hinge_5_1/model_weights\"))\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MulticlassClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): MobiusLinear(\n",
              "    in_features=768, out_features=384, bias=True\n",
              "    (ball): PoincareBall manifold\n",
              "  )\n",
              "  (fc2): MobiusLinear(\n",
              "    in_features=384, out_features=60, bias=True\n",
              "    (ball): PoincareBall manifold\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4paz_8iTZ9o"
      },
      "source": [
        "mobius_params = []\n",
        "bert_params = []\n",
        "\n",
        "def mobius_params():\n",
        "  for param in model.named_parameters():\n",
        "    if 'fc' in param[0]:\n",
        "      yield param[1]\n",
        "def bert_params():\n",
        "  for param in model.named_parameters():\n",
        "    if 'bert' in param[0]:\n",
        "      yield param[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbpDN77r8c4T"
      },
      "source": [
        "for param in model.bert.parameters():\n",
        "  param.requires_grad=True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awQ2Y9Jb3kht"
      },
      "source": [
        "optimizer_1 = torch.optim.AdamW(bert_params(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "optimizer_2 = radam_.RiemannianAdam(mobius_params(), lr=0.01, stabilize=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ys-M4-e3khv"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrYqErOD3khx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fa8cfb-49c2-44fe-d79d-10331086ce06"
      },
      "source": [
        "len(train_dataloader) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWVSE9LM3kh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1b5c48-afba-4aa5-be5e-8a7734d90926"
      },
      "source": [
        "1935 * 32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61920"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcvxVVi63kh3"
      },
      "source": [
        "# scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "#                                             num_warmup_steps = 0, # Default value in run_glue.py\n",
        "#                                             num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUw3zm6g3kh5"
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Function to calculate the accuracy of our predictions vs labels\n",
        "# def flat_accuracy(preds, labels):\n",
        "#     pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "#     labels_flat = labels.flatten()\n",
        "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta6zfUTa3kh7"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFq9gd5kQSHb"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsInxVoqbsFW"
      },
      "source": [
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di68jXK5WaYr"
      },
      "source": [
        "criterion = MyHingeLoss(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LhAy2hZ3kh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63521580-1624-4caf-c5a8-f873a014f7c5"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import f1_score\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "early_stopping = EarlyStopping(patience=4, verbose=True)\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "\n",
        "        model.zero_grad() \n",
        "        optimizer_1.zero_grad()       \n",
        "        optimizer_2.zero_grad()       \n",
        "\n",
        "        logits = model(b_input_ids, \n",
        "                             b_input_mask)\n",
        "        \n",
        "        loss = criterion(logits,b_labels)\n",
        "\n",
        "  \n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer_1.step()\n",
        "        optimizer_2.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        # scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_f1 = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "\n",
        "          logits = model(b_input_ids, \n",
        "                              b_input_mask)\n",
        "          \n",
        "        loss = criterion(logits,b_labels)\n",
        "\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy().round()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        # total_eval_f1 += f1_score(label_ids,logits, average='macro')\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    # avg_val_accuracy = total_eval_f1 / len(validation_dataloader)\n",
        "    # print(\"  f1score: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    early_stopping(avg_val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break  \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    output_dir = 'IR_project_khan_acad_model_hyperbolic_hinge_5/'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n",
        "\n",
        "    !rm -rf \"/content/drive/My Drive/Information_retrieval_project/khan_acad/IR_project_khan_acad_model_hyperbolic_hinge_5\"\n",
        "    !mv IR_project_khan_acad_model_hyperbolic_hinge_5 \"/content/drive/My Drive/Information_retrieval_project/khan_acad/\"\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 30 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    131.    Elapsed: 0:01:01.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:04.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:09.\n",
            "\n",
            "  Average training loss: 2.48\n",
            "  Training epcoh took: 0:03:27\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (inf --> 2.932065).  Saving model ...\n",
            "  Validation Loss: 2.93\n",
            "  Validation took: 0:00:18\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 2 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:07.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:14.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:22.\n",
            "\n",
            "  Average training loss: 2.45\n",
            "  Training epcoh took: 0:03:40\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (2.932065 --> 2.914573).  Saving model ...\n",
            "  Validation Loss: 2.91\n",
            "  Validation took: 0:00:18\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 3 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:15.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:23.\n",
            "\n",
            "  Average training loss: 2.38\n",
            "  Training epcoh took: 0:03:42\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 1 out of 4\n",
            "  Validation Loss: 2.95\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 4 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:24.\n",
            "\n",
            "  Average training loss: 2.35\n",
            "  Training epcoh took: 0:03:43\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (2.914573 --> 2.833537).  Saving model ...\n",
            "  Validation Loss: 2.83\n",
            "  Validation took: 0:00:18\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 5 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:23.\n",
            "\n",
            "  Average training loss: 2.22\n",
            "  Training epcoh took: 0:03:41\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 1 out of 4\n",
            "  Validation Loss: 2.85\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 6 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:17.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:17.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:25.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:25.\n",
            "\n",
            "  Average training loss: 2.20\n",
            "  Training epcoh took: 0:03:43\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Average training loss: 2.20\n",
            "  Training epcoh took: 0:03:43\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 2 out of 4\n",
            "  Validation Loss: 2.87\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "EarlyStopping counter: 2 out of 4\n",
            "  Validation Loss: 2.87\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 7 / 30 ========\n",
            "Training...\n",
            "\n",
            "======== Epoch 7 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:24.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:24.\n",
            "\n",
            "  Average training loss: 2.11\n",
            "  Training epcoh took: 0:03:42\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Average training loss: 2.11\n",
            "  Training epcoh took: 0:03:42\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 3 out of 4\n",
            "  Validation Loss: 2.89\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "EarlyStopping counter: 3 out of 4\n",
            "  Validation Loss: 2.89\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 8 / 30 ========\n",
            "Training...\n",
            "\n",
            "======== Epoch 8 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:24.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:24.\n",
            "\n",
            "  Average training loss: 2.10\n",
            "  Training epcoh took: 0:03:42\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Average training loss: 2.10\n",
            "  Training epcoh took: 0:03:42\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (2.833537 --> 2.759493).  Saving model ...\n",
            "Validation loss decreased (2.833537 --> 2.759493).  Saving model ...\n",
            "  Validation Loss: 2.76\n",
            "  Validation took: 0:00:18\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "  Validation Loss: 2.76\n",
            "  Validation took: 0:00:18\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 9 / 30 ========\n",
            "Training...\n",
            "\n",
            "======== Epoch 9 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:24.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:24.\n",
            "\n",
            "  Average training loss: 2.02\n",
            "  Training epcoh took: 0:03:43\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Average training loss: 2.02\n",
            "  Training epcoh took: 0:03:43\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 1 out of 4\n",
            "  Validation Loss: 2.83\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "EarlyStopping counter: 1 out of 4\n",
            "  Validation Loss: 2.83\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 10 / 30 ========\n",
            "Training...\n",
            "\n",
            "======== Epoch 10 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:15.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:15.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:23.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:23.\n",
            "\n",
            "  Average training loss: 1.95\n",
            "  Training epcoh took: 0:03:41\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Average training loss: 1.95\n",
            "  Training epcoh took: 0:03:41\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 2 out of 4\n",
            "  Validation Loss: 2.92\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "EarlyStopping counter: 2 out of 4\n",
            "  Validation Loss: 2.92\n",
            "  Validation took: 0:00:16\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 11 / 30 ========\n",
            "Training...\n",
            "\n",
            "======== Epoch 11 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:15.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:15.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:23.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:23.\n",
            "\n",
            "  Average training loss: 1.86\n",
            "  Training epcoh took: 0:03:42\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Average training loss: 1.86\n",
            "  Training epcoh took: 0:03:42\n",
            "\n",
            "Running Validation...\n",
            "EarlyStopping counter: 3 out of 4\n",
            "  Validation Loss: 2.79\n",
            "  Validation took: 0:00:17\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "EarlyStopping counter: 3 out of 4\n",
            "  Validation Loss: 2.79\n",
            "  Validation took: 0:00:17\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 12 / 30 ========\n",
            "Training...\n",
            "\n",
            "======== Epoch 12 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch    80  of    131.    Elapsed: 0:02:16.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:23.\n",
            "  Batch   120  of    131.    Elapsed: 0:03:23.\n",
            "\n",
            "  Average training loss: 1.78\n",
            "  Training epcoh took: 0:03:41\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "  Average training loss: 1.78\n",
            "  Training epcoh took: 0:03:41\n",
            "\n",
            "Running Validation...\n",
            "Validation loss decreased (2.759493 --> 2.731927).  Saving model ...\n",
            "Validation loss decreased (2.759493 --> 2.731927).  Saving model ...\n",
            "  Validation Loss: 2.73\n",
            "  Validation took: 0:00:18\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "  Validation Loss: 2.73\n",
            "  Validation took: 0:00:18\n",
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
            "\n",
            "======== Epoch 13 / 30 ========\n",
            "Training...\n",
            "\n",
            "======== Epoch 13 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n",
            "  Batch    40  of    131.    Elapsed: 0:01:08.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-0d8c201e43be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         logits = model(b_input_ids, \n\u001b[0;32m---> 64\u001b[0;31m                              b_input_mask)\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-4fe7f9905086>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, masks)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mhyerbolic_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_exponential_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyerbolic_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m         )\n\u001b[1;32m    839\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    490\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m                 )\n\u001b[1;32m    494\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         )\n\u001b[1;32m    351\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-0d8c201e43be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         logits = model(b_input_ids, \n\u001b[0;32m---> 64\u001b[0;31m                              b_input_mask)\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-4fe7f9905086>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, masks)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mhyerbolic_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_exponential_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyerbolic_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m         )\n\u001b[1;32m    839\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    490\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m                 )\n\u001b[1;32m    494\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         )\n\u001b[1;32m    414\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         )\n\u001b[1;32m    351\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RACcsko3kh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "outputId": "ce98d1b8-70bf-4be9-9c9b-021eb51be7ef"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.24</td>\n",
              "      <td>3.94</td>\n",
              "      <td>0:03:47</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.82</td>\n",
              "      <td>3.73</td>\n",
              "      <td>0:03:47</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.66</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0:03:47</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.60</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0:03:47</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.43</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0:03:46</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3.43</td>\n",
              "      <td>3.43</td>\n",
              "      <td>0:03:47</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.37</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0:03:46</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3.37</td>\n",
              "      <td>3.31</td>\n",
              "      <td>0:03:45</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3.28</td>\n",
              "      <td>3.28</td>\n",
              "      <td>0:03:45</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3.23</td>\n",
              "      <td>3.29</td>\n",
              "      <td>0:03:45</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3.14</td>\n",
              "      <td>3.12</td>\n",
              "      <td>0:03:45</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3.10</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0:03:45</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3.06</td>\n",
              "      <td>3.17</td>\n",
              "      <td>0:03:44</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3.00</td>\n",
              "      <td>3.05</td>\n",
              "      <td>0:03:45</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2.95</td>\n",
              "      <td>3.09</td>\n",
              "      <td>0:03:44</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2.86</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0:03:44</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2.79</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0:03:44</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2.69</td>\n",
              "      <td>2.94</td>\n",
              "      <td>0:03:43</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2.68</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0:03:43</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2.64</td>\n",
              "      <td>3.05</td>\n",
              "      <td>0:03:44</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2.57</td>\n",
              "      <td>2.95</td>\n",
              "      <td>0:03:43</td>\n",
              "      <td>0:00:16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss Training Time Validation Time\n",
              "epoch                                                          \n",
              "1               4.24         3.94       0:03:47         0:00:18\n",
              "2               3.82         3.73       0:03:47         0:00:18\n",
              "3               3.66         3.70       0:03:47         0:00:18\n",
              "4               3.60         3.47       0:03:47         0:00:18\n",
              "5               3.43         3.49       0:03:46         0:00:16\n",
              "6               3.43         3.43       0:03:47         0:00:18\n",
              "7               3.37         3.47       0:03:46         0:00:16\n",
              "8               3.37         3.31       0:03:45         0:00:18\n",
              "9               3.28         3.28       0:03:45         0:00:18\n",
              "10              3.23         3.29       0:03:45         0:00:16\n",
              "11              3.14         3.12       0:03:45         0:00:18\n",
              "12              3.10         3.11       0:03:45         0:00:18\n",
              "13              3.06         3.17       0:03:44         0:00:16\n",
              "14              3.00         3.05       0:03:45         0:00:18\n",
              "15              2.95         3.09       0:03:44         0:00:16\n",
              "16              2.86         3.06       0:03:44         0:00:16\n",
              "17              2.79         3.02       0:03:44         0:00:18\n",
              "18              2.69         2.94       0:03:43         0:00:18\n",
              "19              2.68         3.04       0:03:43         0:00:16\n",
              "20              2.64         3.05       0:03:44         0:00:16\n",
              "21              2.57         2.95       0:03:43         0:00:16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5TicdiP3kiC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "f904ae20-c0d8-4663-830a-79a2954cb97f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbiTDpVv3kiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94cd6c04-f472-4bc9-840a-dc05ae783ba2"
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "output_dir = 'IR_project_khan_acad_model_hyperbolic_hinge_5/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "\n",
        "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "# model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to IR_project_khan_acad_model_hyperbolic_hinge_5/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IR_project_khan_acad_model_hyperbolic_hinge_5/vocab.txt',\n",
              " 'IR_project_khan_acad_model_hyperbolic_hinge_5/special_tokens_map.json',\n",
              " 'IR_project_khan_acad_model_hyperbolic_hinge_5/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U2UQ29a3kiI"
      },
      "source": [
        "# !pip install joblib\n",
        "# import joblib\n",
        "# joblib.dump(LE, \"label_encoder\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F5PxZm9vAOI"
      },
      "source": [
        "import json\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlb0IpVJO1XQ"
      },
      "source": [
        "# with open(os.path.join(output_dir, 'model_config.json'), 'w') as f:\n",
        "#     json.dump(model.config, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpGY8vSDI6u4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73d74be-ecfe-41ec-bacf-57b5d94d6944"
      },
      "source": [
        "!zip -r IR_project_khan_acad_model_hyperbolic_hinge_5.zip IR_project_khan_acad_model_hyperbolic_hinge_5/\n",
        "# files.download('IR_project_khan_acad_model_hyperbolic_hinge_5.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: IR_project_khan_acad_model_hyperbolic_hinge_5/ (stored 0%)\n",
            "updating: IR_project_khan_acad_model_hyperbolic_hinge_5/tokenizer_config.json (stored 0%)\n",
            "updating: IR_project_khan_acad_model_hyperbolic_hinge_5/special_tokens_map.json (deflated 40%)\n",
            "updating: IR_project_khan_acad_model_hyperbolic_hinge_5/model_weights (deflated 7%)\n",
            "updating: IR_project_khan_acad_model_hyperbolic_hinge_5/vocab.txt (deflated 53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlNMgdFAYtXB"
      },
      "source": [
        "!cp IR_project_khan_acad_model_hyperbolic_hinge_5.zip \"/content/drive/My Drive/Information_retrieval_project/khan_acad/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvFDCDIxKDOf"
      },
      "source": [
        "# !zip -r label_encoder_categorized_reduced.zip label_encoder\n",
        "# files.download('label_encoder_categorized_reduced.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4178_yLFMWmx"
      },
      "source": [
        "test_features = test_features.values\n",
        "labels = test_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZpmBJuIC2nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da603d1e-2d6d-4118-8f79-5d246942466d"
      },
      "source": [
        "test_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' -  What I hope to do in this video is get familiar with the notion of an interval, and also think about ways that we can show an interval, or interval notation. Right over here I have a number line. Let\\'s say I wanted to talk about the interval on the number line that goes from negative three to two. So I care about this-- Let me use a different color. Let\\'s say I care about this interval right over here. I care about all the numbers from negative three to two. So in order to be more precise, I have to be clear. Am I including negative three and two, or am I not including negative three and two, or maybe I\\'m just including one of them. So if I\\'m including negative three and two, then I would fill them in. So this right over here, I\\'m filling negative three and two in, which means that negative three and two are part of this interval. And when you include the endpoints, this is called a closed interval. Closed interval. And I just showed you how I can depict it on a number line, by actually filling in the endpoints and there\\'s multiple ways to talk about this interval mathematically. I could say that this is all of the... Let\\'s say this number line is showing different values for x. I could say these are all of the x\\'s that are between negative three and two. And notice, I have negative three is less than or equal to x so that\\'s telling us that x could be equal to, that x could be equal to negative three. And then we have x is less than or equal to positive two, so that means that x could be equal to positive two, so it is a closed interval. Another way that we could depict this closed interval is we could say, okay, we\\'re talking about the interval between, and we can use brackets because it\\'s a closed interval, negative three and two, and once again I\\'m using brackets here, these brackets tell us that we include, this bracket on the left says that we include negative three, and this bracket on the right says that we include positive two in our interval. Sometimes you might see things written a little bit more math-y. You might see x is a member of the real numbers such that... And I could put these curly brackets around like this. These curly brackets say that we\\'re talking about a set of values, and we\\'re saying that the set of all x\\'s that are a member of the real number, so this is just fancy math notation, it\\'s a member of the real numbers. I\\'m using the Greek letter epsilon right over here. It\\'s a member of the real numbers such that. This vertical line here means \"such that,\" negative three is less x is less than-- negative three is less than or equal to x, is less than or equal to two. I could also write it this way. I could write x is a member of the real numbers such that x is a member, such that x is a member of this closed set, I\\'m including the endpoints here. So these are all different ways of denoting or depicting the same interval. Let\\'s do some more examples here. So let\\'s-- Let me draw a number line again. So, a number line. And now let me do-- Let me just do an open interval. An open interval just so that we clearly can see the difference. Let\\'s say that I want to talk about the values between negative one and four. Let me use a different color. So the values between negative one and four, but I don\\'t want to include negative one and four. So this is going to be an open interval. So I\\'m not going to include four, and I\\'m not going to include negative one. Notice I have open circles here. Over here had closed circles, the closed circles told me that I included negative three and two. Now I have open circles here, so that says that I\\'m not, it\\'s all the values in between negative one and four. Negative .999999 is going to be included, but negative one is not going to be included. And 3.9999999 is going to be included, but four is not going to be included. So how would we-- What would be the notation for this? Well, here we could say x is going to be a member of the real numbers such that negative one-- I\\'m not going to say less than or equal to because x can\\'t be equal to negative one, so negative one is strictly less than x, is strictly less than four. Notice not less than or equal, because I can\\'t be equal to four, four is not included. So that\\'s one way to say it. Another way I could write it like this. x is a member of the real numbers such that x is a member of... Now the interval is from negative one to four but I\\'m not gonna use these brackets. These brackets say, \"Hey, let me include the endpoint,\" but I\\'m not going to include them, so I\\'m going to put the parentheses right over here. Parentheses. So this tells us that we\\'re dealing with an open interval. This right over here, let me make it clear, this is an open interval. Now you\\'re probably wondering, okay, in this case both endpoints were included, it\\'s a closed interval. In this case both endpoints were excluded, it\\'s an open interval. Can you have things that have one endpoint included and one point excluded, and the answer is absolutely. Let\\'s see an example of that. I\\'ll get another number line here. Another number line. And let\\'s say that we want to-- Actually, let me do it the other way around. Let me write it first, and then I\\'ll graph it. So let\\'s say we\\'re thinking about all of the x\\'s that are a member of the real numbers such that let\\'s say negative four is not included, is less than x, is less than or equal to negative one. So now negative one is included. So we\\'re not going to include negative four. Negative four is strictly less than, not less than or equal to, so x can\\'t be equal to negative four, open circle there. But x could be equal to negative one. It has to be less than or equal to negative one. It could be equal to negative one so I\\'m going to fill that in right over there. And it\\'s everything in between. If I want to write it with this notation I could write x is a member of the real numbers such that x is a member of the interval, so it\\'s going to go between negative four and negative one, but we\\'re not including negative four. We have an open circle here so I\\'m gonna put a parentheses on that side, but we are including negative one. We are including negative one. So we put a bracket on that side. That right over there would be the notation. Now there\\'s other things that you could do with interval notation. You could say, well hey, everything except for some values. Let me give another example. Let\\'s get another example here. Let\\'s say that we wanna talk about all the real numbers except for one. We want to include all of the real numbers. All of the real numbers except for one. Except for one, so we\\'re gonna exclude one right over here, open circle, but it can be any other real number. So how would we denote this? Well, we could write x is a member of the real numbers such that x does not equal one. So here I\\'m saying x can be a member of the real numbers but x cannot be equal to one. It can be anything else, but it cannot be equal to one. And there\\'s other ways of denoting this exact same interval. You could say x is a member of the real numbers such that x is less than one, or x is greater than one. So you could write it just like that. Or you could do something interesting. This is the one that I would use, this is the shortest and it makes it very clear. You say hey, everything except for one. But you could even do something fancy, like you could say x is a member of the real numbers such that x is a member of the set going from negative infinity to one, not including one, or x is a member of the set going from-- or a member of the interval going from one, not including one, all the way to positive, all the way to positive infinity. And when we\\'re talking about negative infinity or positive infinity, you always put a parentheses. And the view there is you could never include everything all the way up to infinity. It needs to be at least open at that endpoint because infinity just keeps going on and on. So you always want to put a parentheses if you\\'re talking about infinity or negative infinity. It\\'s not really an endpoint, it keeps going on and on forever. So you use the notation for open interval, at least at that end, and notice we\\'re not including, we\\'re not including one either, so if x is a member of this interval or that interval, it essentially could be anything other than one. But this would have been the simplest notation to describe that.',\n",
              "       \" In the last video we were able to set up this definite integral using the shell or the hollow cylinder method in order to figure out the volume of this solid of revolution. And so now let's just evaluate this thing. And really the main thing we have to do here is just to multiply what we have here out. So multiply this expression out. So this is going to be equal to-- I'll take the 2 pi out of the integral. 2 pi times the integral from 0 to 1. Let's see, 2 times the square root of x is 2-- I'll write it as 2 square roots of x. But I'll write it as 2x to the 1/2. It'll make it a little bit easier to take the antiderivative conceptually, or at least in our brain. So two times the square root of x is 2x to the 1/2. 2 times negative x squared is negative 2 x squared. And then we have negative x times the square root of x. Well, that's x to the first times x to the 1/2. That's going to be negative x to the 3/2 power. And then we have negative x times negative x squared that's going to be positive x to the third power. And all of that dx. And so now we're ready to take the antiderivative. So this is going to be equal to 2 pi times the antiderivative of all of this business evaluated at 1 and at 0. So the antiderivative of 2 times x to the 1/2 is going to be 2-- it's going to be-- let's see. We're going to take x to the 3/2 times 2/3. So it's going to be 4/3 x to the 3/2. And then for this term right over here it's going to be negative 2/3 x to the third. And you could take the derivative here to verify that you actually do get this. And then right over here, let's see, if we incremented this, you get x to the 5/2. And so we're going to want to multiply by 2/5. So minus-- let me do this in another color. Let's see, so this one right over here, it's going to be minus 2/5 x to the 5/2 power. Yep, that works out. And then finally you're going to have x to the fourth over 4 plus-- let me do that in a different color-- plus x to the fourth over 4. That's this term right over here. And now we just have to evaluate at 1 and 0. And 0, luckily, all of these terms end up being a 0. So that's nice and cancels out. And so we are just left with-- we're just-- [INAUDIBLE] cancel out. It just evaluates to 0. So this is just 2 pi times when you evaluate all this business at 1. So that's going to be 4/3 minus 2/3 minus 2/5 plus 1/4. And the least common multiple right over here looks like 60, so we're going to want to put all this over a denominator of 60. So it's going to be 2 pi times all of this business over a denominator of 60. And 4/3 is same thing as 80/60. Negative 2/3 is the same thing as negative 40/60. Negative 2/5 is the same thing as negative 24/60. And then 1/4 is the same thing as 15/60. So this is equal to-- and actually this will cancel over here, and you'll just get a 30 in your denominator. So in your denominator, you get a 30. And up here 80 minus 40 is 40. 40 minus 24 gets us to 16. 16 plus 15 is 31. So we get 31 times pi over 30 for the volume of the figure right over there.\",\n",
              "       \" -  In previous videos we talk about GDP as the market value of final goods and services produced in a country in a given time period, let's say in a given year and we gave the example of producing jeans where maybe the farmer helps produce the cotton and then the thread maker takes that cotton and makes thread and then the fabric maker takes the thread and makes fabric and then the jean maker takes the fabric and produces jeans and then the market value of those jeans was $50 and so, assuming all of this happened in one year, in the time period that we're measuring GDP for, then we would just count the $50, if we're looking at the final market value or the market value of final goods and services. You would say the GDP for at least for this component of the GDP from these jeans is $50 but I do wanna clarify that there are multiple ways that you can measure GDP and you could even think about it from a value added approach but the key idea is no matter how you measure it you should get to the same value, so let's think about the various actors here and what their value add was. So, first, let's think about the framer right over here. So, this is the farmer, my not so elegantly drawn rectangle around what he's doing. So, the farmer's value add is what? Well, before you just had some dirt and things and so, maybe you could say that the market value was zero and then he's able to produce something or she's able to produce something that now has a market value of $10, so their value add is $10. Now, from there, the cotton goes to the thread maker. The threader maker they take that $10 cotton, so this is thread maker, they take the $10 cotton and are able to produce $20 worth of thread? What is their value add? Value add here? Well, they took something worth $10 and they were able to do something to it to make it worth $20, so their value add is now another $10 and then, this is the thread maker, and then from there it goes to the fabric maker and I think you see where this is going, the fabric maker is this part of our process, fabric maker and their value add is what? Pause this video and think about it. Well, they take something worth $20 and they're to turn it into something that has a market value of $30, so their value add is also $10 and then last but not least, you have the jean company, so the jean manufacturer, I'll call them the jean producer, the jean producer, they take something that has a market value of $30 and they're able to sell it for $50, so their value add here, if you take something for 30 and you make it worth 50, then you've added $20 of value and so, the value added approach to GDP will just sum up these value adds, so this is going to be this $10 from the farmer plus the value add of the thread maker, plus $10 from the thread maker plus $10 from the fabric maker plus $20 from the jean maker and what will that all add up to? Well, that's all going to add up to 10 plus 10 is 20 plus 10 is 30 plus 20 is $50 and lucky for us that is added up to the same amount as we had before where we just looked at the market value of the final goods and services. Now, one benefit of the value added approach is that real supply chains are quite complex and things might be going from one country to another, they might as we've talked about in another video, the year might end right over here and so, when something is made in China and there's value add in China but then it's shipped to the US and some value add is placed on it and then it's shipped back to China or Mexico, you have to be careful to only count the value add in the country for which you are measuring the GDP. So, that's one useful way or one useful reason, or one way in which the value added approach might be useful. The key idea though is that you're getting to the same value. You should get to the same value as the market value of the final goods and services produced in a given time period.\",\n",
              "       ...,\n",
              "       \" Let's see if we can get a little bit more practice and intuition of what cross products are all about. So in the last example, we took a cross b. Let's see what happens when we take b cross a. So let me erase some of this. I don't want to erase all of it because it might be useful to give us some intuition to compare. I'm going to keep that. Actually, I can erase this, I think. So the things I have drawn here, this was a cross b. Let me cordon it off so you don't get confused. So that was me using the right hand rule when I tried to do a cross b, and then we saw that the magnitude of this was 25, and n, the direction, pointed downwards. Or when I drew it here, it would point into the page. So let's see what happens with b cross a, so I'm just switching the order. b cross a. Well, the magnitude is going to be the same thing, right? Because I'm still going to take the magnitude of b times the magnitude of a times the sine of the angle between them, which was pi over 6 radians and then times some unit vector n. But this is going to be the same. When I multiply scalar quantities, it doesn't matter what order I multiply them in, right? So this is still going to be 25, whatever my units might have been, times some vector n. And we still know that that vector n has to be perpendicular to both a and b, and now we have to figure out, well, is it, in being perpendicular, it can either kind of point into the page here or it could pop out of the page, or point out of the page. So which one is it? And then we take our right hand out, and we try it again. So what we do is we take our right hand. I'm actually using my right hand right now, although you can't see it, just to make sure I draw the right thing. So in this example, if I take my right hand, I take the index finger in the direction of b. I take my middle finger in the direction of a, so my middle figure is going to look something like that, right? And then I have two leftover fingers there. Then the thumb goes in the direction of the cross product, right? Because your thumb has a right angle right there. That's the right angle of your thumb. So in this example, that's the direction of a, this is the direction of b, and we're doing b cross a. That's why b gets your index finger. The index finger gets the first term, your middle finger gets the second term, and the thumb gets the direction of the cross product. So in this example, the direction of the cross product is upwards. Or when we're drawing it in two dimensions right here, the cross product would actually pop out of the page for b cross a. So I'll draw it over. It would be the circle with the dot. Or if I were to draw it analogous to this, so this right here, that was a cross b. And then b cross a is the exact same magnitude, but it goes in the other direction. That's b cross a. It just flips in the opposite direction. And that's why you have to use your right hand, because you might know that, oh, something's going to pop in or out of the page, et cetera, et cetera, but you need to know your right hand to know whether it goes in or out of the page. Anyway, let's see if we can get a little bit more intuition of what this is all about because this is all about intuition. And frankly, I'll tell you, the cross product comes into use in a lot of concepts that frankly we don't have a lot of real-life intuition, with electrons flying through a magnetic field or magnetic fields through a coil. A lot of things in our everyday life experience, maybe if we were metal filings living in a magnetic field-- well, we do live in a magnetic field. In a strong magnetic field, maybe we would get an intuition, but it's hard to have as deep of an intuition as we do for, say, falling objects, or friction, or forces, or fluid dynamics even, because we've all played with water. But anyway, let's get a little bit more intuition. And let's think about why is there that sine of theta? Why not just multiply the magnitudes times each other and use the right hand rule and figure out a direction? What is that sine of theta all about? I think I need to clear this up a little bit just so this could be useful. So why is that sine of theta there? Let me redraw some vectors. I'll draw them a little fatter. So let's say that's a, that's a, this is b. b doesn't always have to be longer than a. So this is a and this is b. Now, we can think of it a little bit. We could say, well, this is the same thing as a sine theta times b, or we could say this is b sine theta times a. I hope I'm not confusing-- all I'm saying is you could interpret this as-- because these are just magnitudes, right? So it doesn't matter what order you multiply them in. You could say this is a sine theta times the magnitude of b, all of that in the direction of the normal vector, or you could put the sine theta the other way. But let's think about what this would mean. a sine theta, if this is theta. What is a sine theta? Sine is opposite over hypotenuse, right? So opposite over hypotenuse. So this would be the magnitude of a. Let me draw something. Let me draw a line here and make it a real line. Let me draw a line there, so I have a right angle. So what's a sine theta? This is the opposite side. So a sine theta is a, and sine of theta is opposite over hypotenuse. The hypotenuse is the magnitude of a, right? So sine of theta is equal to this side, which I call o for opposite, over the magnitude of a. So it's opposite over the magnitude of a. So this term a sine theta is actually just the magnitude of this line right here. Another way you could-- let me redraw it. It doesn't matter where the vectors start from. All you care about is this magnitude and direction, so you could shift vectors around. So this vector right here, and you could call it this opposite vector, that's the same thing as this vector. That's the same thing as this. I just shifted it away. And so another way to think about it is, it is the component of vector a, right? We're used to taking a vector and splitting it up into x- and y-components, but now we're taking a vector a, and we're splitting it up into-- you can think of it as a component that's parallel to vector b and a component that is perpendicular to vector b. So a sine theta is the magnitude of the component of vector a that is perpendicular to b. So when you're taking the cross product of two numbers, you're saying, well, I don't care about the entire magnitude of vector a in this example, I care about the magnitude of vector a that is perpendicular to vector b, and those are the two numbers that I want to multiply and then give it that direction as specified by the right hand rule. And I'll show you some applications. This is especially important-- well, we'll use it in torque and we'll also use it in magnetic fields, but it's important in both of those applications to figure out the components of the vector that are perpendicular to either a force or a radius in question. So that's why this cross product has the sine theta because we're taking-- so in this, if you view it as magnitude of a sine theta times b, this is kind of saying this is the magnitude of the component of a perpendicular to b, or you could interpret it the other way. You could interpret it as a times b sine theta, right? Put a parentheses here. And then you could view it the other way. You could say, well, b sine theta is the component of b that is perpendicular to a. Let me draw that, just to hit the point home. So that's my a, that's my b. This is a, this is b. So b has some component of it that is perpendicular to a, and that is going to look something like-- well, I've run out of space. Let me draw it here. If that's a, that's b, the component of b that is perpendicular to a is going to look like this. It's going to be perpendicular to a, and it's going to go that far, right? And then you could go back to SOH CAH TOA and you could prove to yourself that the magnitude of this vector is b sine theta. So that is where the sine theta comes from. 1It makes sure that we're not just multiplying the vectors. 1It makes sure we're multiplying the components of 1the vectors that are perpendicular to each other to 1get a third vector that is perpendicular to both of them. 1And then the people who invented the cross product 1said, well, it's still ambiguous because it doesn't 1tell us-- there's always two vectors that are perpendicular 1to these two. 1One goes in, one goes out. 1They're in opposite directions. 1And that's where the right hand rule comes in. 1They'll say, OK, well, we're just going to say a convention 1that you use your right hand, point it like a gun, make all 1your fingers perpendicular, and then you know what 1direction that vector points in. 1Anyway, hopefully, you're not confused. 1Now I want you to watch the next video. 1This is actually going to be some physics on electricity, 1magnetism and torque, and that's essentially the 1applications of the cross product, and it'll give you a 1little bit more intuition of how to use it. 1See you soon.\",\n",
              "       \" Find the probability of rolling even numbers three times using a six-sided die numbered from 1 to 6. So let's just figure out the probability of rolling it each of the times. So the probability of rolling even numbers. So even roll on six-sided die. So let's think about that probability. Well, how many total outcomes are there? How many possible rolls could we get? Well, you get one, two, three, four, five, six. And how many of them satisfy these conditions, that it's an even number? Well, it could be a 2, it could be a 4, or it could be a 6. So the probability is the events that match what you need, your condition for right here, so three of the possible events are an even roll. And it's out of a total of six possible events. So there is a-- 3 over 6 is the same thing as 1/2 probability of rolling even on each roll. Now they're going to roll-- they want to roll even three times. And these are all going to be independent events. Every time you roll, it's not going to affect what happens in the next roll, despite what some gamblers might think. It has no impact on what happens on the next roll. So the probability of rolling even three times is equal to the probability of an even roll one time, or even roll on six-sided die-- this thing over here is equal to that thing times that thing again. All right, that's our first roll-- we copy and we paste it-- times that thing and then times that thing again. Right? That's our first roll, which is that. That's our second roll. That's our third roll. They're independent events. So this is going to be equal to 1/2-- that's the same 1/2 right there-- times 1/2 times 1/2, which is equal to 1 over 8. There's a 1 in 8 possibility that you roll even numbers on all three rolls. On this roll, this roll, and that roll.\",\n",
              "       ' -  In the last video we began to see some pretty good evidence that DNA was the molecular basis for inheritance and we saw that from the work of Avery, McCarthy and McLead where they tried to identify whether it was DNA or proteins that acted as a transformation principle in Griffith\\'s experiments and I encourage you to watch that video if all of this sounds unfamiliar. But even their work in 1944 was not viewed as conclusive evidence. It was viewed as strong evidence, but not conclusive evidence because, remember how they did it, they took the heat killed smooth strain, the smooth strain you might remember from Griffith\\'s experiment was the virulent one. The heat killed it. When you heat kill it in injected amounts, it didn\\'t do anything to the mouse, but if you took the heat killed smooth strain and put it with the rough strain, it somehow transformed the rough strain in to the smooth strain, in to the virulent strain and so they took the heat killed smooth strain and they took out its different components and they eventually were able to isolate one that was able to transform the rough strain in to the smooth strain by itself and then they applied all sorts of chemical tests to it and said, \"Hey, there\\'s pretty good evidence \"that this is DNA,\" but it wasn\\'t conclusive because, well, maybe they didn\\'t purify it properly or maybe there was still a little bit of protein. Maybe it was mostly DNA, but maybe it was a little bit of protein that was still left there that actually did the transformation. So the scientific community, they weren\\'t just saying, \"Hey, that looks pretty good, \"Let\\'s move on, let\\'s just assume.\" They wanted to continue to test it and especially test it in different ways. And, the conclusive evidence didn\\'t come until a few years later, until 1952 when Alfred Hershey and Martha Chase decided to study T2 bacterio phage. Let me write this down. T2 bacterio phage, this is phage that infects bacteria. Bacterio phage. When you hear the word phage, we\\'re referring to viruses. Now they knew that T2 bacterio phage was composed of proteins and DNA and they didn\\'t, well, we now know, that it\\'s a protein shell and there\\'s DNA inside, but they, from their point of view, they said, \"Okay, it\\'s made up and if we try to \"look at the stuff that this virus is made of, \"it\\'s protein and DNA.\" So protein plus DNA, and they knew that this virus, when it infects bacteria, it injects something into that bacteria. So it injects something and that something is what hijacks that bacteria\\'s genetic information to start producing more of the T2 bacterio phage. So they could identify the something that gets injected. If they could figure out if that something was either a protein or a virus, then they would have conclusively proven so sorry, if they could show that something was not protein or virus, if it was protein or DNA, if they could show that it was either protein or DNA, then they could show conclusively that it\\'s either the protein or the DNA that forms the molecular basis and so they\\'re actually quite sceptical of Avery, McCarthy and MacLead\\'s experiments. They actually, Hershey and Chase, actually thought that they were gonna show that it was the protein, and remember, this whole time people were like, \"Protein, we know it\\'s these complex molecules \"that have these different shapes \"and all these different amino acids. \"It seems like that\\'s much more likely to encode \"the complexity of genetic information than DNA.\" They didn\\'t have an appreciation for the structure of DNA at this time. So they devised an experiment to figure out what is that something that the T2 bacterio phage is infecting. Is that something protein? Is it protein or DNA? So this is the question. So what they do is they take two batches or they developed two batches of T2 bacterio phage. One batch of the T2 bacterio phage they do it in the presence of radioactive phosphorus, phosphorus 32. In the other batch, I should say they grow that T2 bacterio phage in the process of another radioactive isotope, but this time it is of sulfur. This is sulfur 35. So why are they doing that? Well phosphorus is found in DNA. So in this first batch, the radioactive marker, you could say, is going to incorporate itself into the DNA. In the second batch, sulfur is found in the protein and not in the DNA and so this would actually tag the protein parts. And if you\\'re wondering, well, how do you develop these radioactive batches, well, you let the viruses hijack cells in a medium that has either the radioactive sulfur or the radioactive phosphorus and as they reproduce, they are going to incorporate that radioactive material into either the protein or the DNA of the new viruses that get produced. So anyway, they were able to produce some of the T2 bacterio phage in the presence of the radioactive phosphorus and they knew that way that the DNA would get that radioactive material in it and then with the radioactive sulfur they said the protein would have that radioactive sulfur. And then for each of those batches, they then infected bacterio phage with them and they said, okay, they\\'re going to inject something in to the bacterio phage, and to figure out what that something is that was injected in to the bacterio phage they take the products in either of the two scenarios, they first blend \\'em up so that all of the stuff that\\'s left outside gets taken off of the surface of the bacteria cells and then they stick it in to a centrifuge and the centrifuge is, you can imagine, it\\'s kind of just a big spinning machine. If you were to take a test tube and take it sideways, one way to think about it, put it sideways like this. Put maybe a stopper in it so nothing leaks and then you spin it around really, really, really, really fast, what you\\'re going to find, you can actually generate significant g forces and so the heavier stuff is going to gravitate to the bottom of the test tube or to the right when it is on its side, and the lighter stuff is going to gravitate to the left. And, it turns out that the bacteria, the actual bacterial cells, those are heavier so the bacterial cells are going to go towards the bottom of the test tube and they\\'re gonna form a material that we call the pellet and then all of the other stuff, all of the fluid and the leftover phage parts those are going to go up to the top of the test tube and we call that the supernatent. I always have trouble pronouncing that. Supernatent. And so they said, \"Look, if we look at the pellet,\" which they knew had the bacterial cells int here or you could even say the remts of the bacterial cells, \"if the pellet here \"contains phosphorus, that means that the DNA, \"our radioactive DNA or our tagged DNA \"made it in to the bacteria, \"but if it contains sulfur, that means that the protein \"made it in to the bacteria.\" And, what they found is they found that the radioactive phosphorus was in the pellet which allowed them to conclude that, hey, it\\'s the DNA from the virus that made it inside of the bacteria and not the protein and then they said, \"Well, it must be! \"Wow, the Avery, McCarthy and Maclead were correct. \"It\\'s actually the DNA that is this transformation principle \"that can go in and hijack the genetic, \"the machinery of the bacteria \"to produce more of the actual virus, \"so this is a really, really, really big deal.\" Once again, we started with Mendel saying, \"Hey, we have these inheritable factors \"and they seem to segregate and sort in certain ways. \"They seem to be discrete.\" Bover and Sutton said, \"Hey, \"chromosomes seem to kind of, \"their behavior during meiosis \"when cells split \"seem to kind of match up to that.\" Morgan starts to provide some evidence. We have Griffith\\'s experiments with the mice and the bacteria and saying, \"Hey, look, \"there\\'s some transformation principle.\" Avery, McCarthy and MacLeod say, \"Hey, \"looks like when we try to really purify \"this transformation principle, \"it seems like DNA is what really matters.\" And then Hershey and Chase validate that even more conclusively.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRZ54gFokNh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192b0716-3b3d-43f6-8e5a-0b6c0da9f30a"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['math>>math1>>x89d82521517266d4:functions',\n",
              "       'math>>old-ap-calculus-ab>>ab-applications-definite-integrals',\n",
              "       'economics-finance-domain>>ap-macroeconomics>>economic-iondicators-and-the-business-cycle',\n",
              "       ...,\n",
              "       'science>>in-in-class11th-physics>>in-in-system-of-particles-and-rotational-motion',\n",
              "       'math>>precalculus>>x9e81a4f98389efdf:prob-comb',\n",
              "       'science>>biology>>dna-as-the-genetic-material'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ohj1x7frQJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bcb984-02ca-4078-cb96-09a26dbed495"
      },
      "source": [
        "len(list(set(labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "416"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjinn0gXkNuP"
      },
      "source": [
        "\n",
        "# course_taxonomy\n",
        "test_labels = list(set(labels))\n",
        "poincare_emb_data = get_poincare_embeddings_data(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPU8Pm5b8Zwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb14b4b0-ae7e-46b2-eb42-3739e31c24f9"
      },
      "source": [
        "poincare_emb_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['math', 'integral-calculus', 'ic-adv-funcs'],\n",
              " ['math', 'math3', 'x5549cc1686316ba5:poly-div'],\n",
              " ['economics-finance-domain',\n",
              "  'ap-macroeconomics',\n",
              "  'economic-iondicators-and-the-business-cycle'],\n",
              " ['math', 'cc-fourth-grade-math', 'imp-measurement-and-data-2'],\n",
              " ['math',\n",
              "  'in-in-class-6-math-india-icse',\n",
              "  'in-in-6-ratio-and-proportion-icse'],\n",
              " ['math', 'old-integral-calculus', 'integration-techniques'],\n",
              " ['economics-finance-domain', 'macroeconomics', 'forex-trade-topic'],\n",
              " ['math', 'engageny-alg-1', 'alg1-3'],\n",
              " ['math',\n",
              "  'in-in-class-2nd-math-cbse',\n",
              "  'x41ed04e12bec59cd:cc-2nd-add-subtract-1000'],\n",
              " ['science', 'organic-chemistry', 'alkenes-alkynes'],\n",
              " ['math', 'math-for-fun-and-glory', 'aime'],\n",
              " ['math', 'arithmetic', 'arith-review-multiply-divide'],\n",
              " ['math', 'cc-eighth-grade-math', 'geometric-transformations'],\n",
              " ['math', 'math1', 'x89d82521517266d4:systems'],\n",
              " ['science',\n",
              "  'in-in-class11th-physics',\n",
              "  'in-in-system-of-particles-and-rotational-motion'],\n",
              " ['math', 'ap-calculus-ab', 'ab-diff-analytical-applications-new'],\n",
              " ['math', 'algebra-home', 'alg-vectors'],\n",
              " ['math',\n",
              "  'in-in-class-3rd-math-cbse',\n",
              "  'x80b2f4aa70819288:imp-addition-and-subtraction'],\n",
              " ['math', 'arithmetic', 'arith-decimals'],\n",
              " ['science', 'cosmology-and-astronomy', 'universe-scale-topic'],\n",
              " ['science', 'physics', 'work-and-energy'],\n",
              " ['math', 'calculus-all-old', 'taking-derivatives-calc'],\n",
              " ['math', 'engageny-alg-1', 'alg1-2'],\n",
              " ['math',\n",
              "  'in-in-class-6-math-india-icse',\n",
              "  'in-in-6-addition-and-subtraction-of-negative-numbers-icse'],\n",
              " ['math', 'ap-calculus-ab', 'ab-limits-new'],\n",
              " ['math', 'ap-calculus-bc', 'bc-limits-new'],\n",
              " ['science', 'ap-physics-1', 'ap-one-dimensional-motion'],\n",
              " ['math', 'ap-statistics', 'probability-ap'],\n",
              " ['science', 'ap-physics-2', 'ap-geometric-optics'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-matrices'],\n",
              " ['math', '4th-engage-ny', 'engage-4th-module-5'],\n",
              " ['science', 'electrical-engineering', 'reverse-engin'],\n",
              " ['math', '7th-engage-ny', 'engage-7th-module-4'],\n",
              " ['math', 'engageny-geo', 'geo-5'],\n",
              " ['science', 'physics', 'circuits-topic'],\n",
              " ['science', 'ap-chemistry', 'states-of-matter-and-intermolecular-forces-ap'],\n",
              " ['math', 'cc-sixth-grade-math', 'cc-6th-equations-and-inequalities'],\n",
              " ['science', 'health-and-medicine', 'hematologic-system-diseases-2'],\n",
              " ['math', 'old-ap-calculus-bc', 'bc-series'],\n",
              " ['math', '7th-engage-ny', 'engage-7th-module-1'],\n",
              " ['math', 'calculus-1', 'cs1-applications-of-integrals'],\n",
              " ['math', 'old-integral-calculus', 'riemann-sums-ic'],\n",
              " ['math', 'engageny-precalc', 'precalc-3'],\n",
              " ['science', 'chemistry', 'chemical-bonds'],\n",
              " ['science', 'ap-chemistry', 'redox-reactions-and-electrochemistry-ap'],\n",
              " ['economics-finance-domain', 'ap-microeconomics', 'imperfect-competition'],\n",
              " ['math', 'precalculus', 'x9e81a4f98389efdf:conics'],\n",
              " ['math', 'algebra-home', 'alg-rational-expr-eq-func'],\n",
              " ['math', 'linear-algebra', 'alternate-bases'],\n",
              " ['math', 'calculus-1', 'cs1-limits-and-continuity'],\n",
              " ['science', 'physics', 'magnetic-forces-and-magnetic-fields'],\n",
              " ['science', 'electrical-engineering', 'ee-signals'],\n",
              " ['math', 'ap-statistics', 'estimating-confidence-ap'],\n",
              " ['math', 'calculus-all-old', 'limits-and-continuity-calc'],\n",
              " ['math', 'ap-statistics', 'density-curves-normal-distribution-ap'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:exponential-growth-decay'],\n",
              " ['math', 'ap-statistics', 'quantitative-data-ap'],\n",
              " ['math', 'algebra-home', 'alg-polynomials'],\n",
              " ['math', 'in-in-grade-11-ncert', 'in-in-class11-straight-lines'],\n",
              " ['math', 'algebra-basics', 'alg-basics-graphing-lines-and-slope'],\n",
              " ['math', 'integral-calculus', 'ic-series'],\n",
              " ['economics-finance-domain',\n",
              "  'ap-macroeconomics',\n",
              "  'national-income-and-price-determinations'],\n",
              " ['math',\n",
              "  'calculus-1',\n",
              "  'cs1-derivatives-chain-rule-and-other-advanced-topics'],\n",
              " ['math', 'algebra-home', 'alg-exp-and-log'],\n",
              " ['science', 'chemistry', 'states-of-matter-and-intermolecular-forces'],\n",
              " ['math', 'cc-seventh-grade-math', 'cc-7th-ratio-proportion'],\n",
              " ['math', '5th-engage-ny', 'engage-5th-module-2'],\n",
              " ['math',\n",
              "  '7th-grade-illustrative-math',\n",
              "  'unit-2-introducing-proportional-relationships'],\n",
              " ['math', 'geometry-home', 'triangle-properties'],\n",
              " ['science', 'biology', 'energy-and-enzymes'],\n",
              " ['math', 'old-ap-calculus-ab', 'ab-applications-definite-integrals'],\n",
              " ['science', 'health-and-medicine', 'advanced-hematologic-system'],\n",
              " ['math', 'algebra-basics', 'basic-alg-foundations'],\n",
              " ['math', 'in-in-class-6-math-india-icse', 'in-in-6-perimeter-and-area-icse'],\n",
              " ['economics-finance-domain', 'microeconomics', 'firm-economic-profit'],\n",
              " ['math', 'cc-sixth-grade-math', 'cc-6th-expressions-and-variables'],\n",
              " ['math', 'pre-algebra', 'pre-algebra-arith-prop'],\n",
              " ['math', 'algebra2', 'x2ec2f6f830c9fb89:modeling'],\n",
              " ['economics-finance-domain',\n",
              "  'ap-macroeconomics',\n",
              "  'ap-open-economy-international-trade-and-finance'],\n",
              " ['math', 'ap-calculus-bc', 'bc-integration-new'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-probability-of-events'],\n",
              " ['science',\n",
              "  'in-in-class11th-physics',\n",
              "  'in-in-class11th-physics-motion-in-a-straight-line'],\n",
              " ['math', '7th-grade-illustrative-math', 'unit-8-probability-and-sampling'],\n",
              " ['math', 'algebra-basics', 'alg-basics-systems-of-equations'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:foundation-algebra'],\n",
              " ['science', 'electrical-engineering', 'robots'],\n",
              " ['science', 'physics', 'forces-newtons-laws'],\n",
              " ['science', 'ap-biology', 'worked-examples-ap-biology'],\n",
              " ['math', 'math2', 'xe2ae2386aa2e13d6:quad-2'],\n",
              " ['math', 'ap-calculus-ab', 'ab-diff-contextual-applications-new'],\n",
              " ['science', 'physics', 'light-waves'],\n",
              " ['science',\n",
              "  'in-in-class10th-physics',\n",
              "  'in-in-magnetic-effects-of-electric-current'],\n",
              " ['math', 'in-in-grade-11-ncert', 'in-in-class11-derivatives'],\n",
              " ['science', 'in-in-class11th-physics', 'in-in-11th-physics-waves'],\n",
              " ['math',\n",
              "  'calculus-2',\n",
              "  'cs2-parametric-equations-polar-coordinates-and-vector-valued-functions'],\n",
              " ['math', 'arithmetic-home', 'arith-place-value'],\n",
              " ['math', 'algebra-home', 'alg-radical-eq-func'],\n",
              " ['math', 'algebra-basics', 'alg-basics-expressions-with-exponents'],\n",
              " ['science', 'ap-biology', 'cell-structure-and-function'],\n",
              " ['math', 'cc-fourth-grade-math', 'division'],\n",
              " ['math',\n",
              "  'in-in-class-7-math-india-icse',\n",
              "  'in-in-7-ratio-proportion-and-unitary-method-icse'],\n",
              " ['science', 'organic-chemistry', 'bond-line-structures-alkanes-cycloalkanes'],\n",
              " ['math', 'math1', 'x89d82521517266d4:analytic-geo'],\n",
              " ['math', 'precalculus', 'x9e81a4f98389efdf:vectors'],\n",
              " ['math', 'ap-statistics', 'summarizing-quantitative-data-ap'],\n",
              " ['science', 'biology', 'membranes-and-transport'],\n",
              " ['math', 'old-ap-calculus-ab', 'ab-existence-theorems'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-relations-functions'],\n",
              " ['science', 'biology', 'photosynthesis-in-plants'],\n",
              " ['math', 'math-for-fun-and-glory', 'math-warmup'],\n",
              " ['math',\n",
              "  'in-in-class-7-math-india-icse',\n",
              "  'in-in-7-exponents-and-powers-icse'],\n",
              " ['science', 'chemistry', 'atomic-structure-and-properties'],\n",
              " ['math', 'algebra-home', 'alg-series-and-induction'],\n",
              " ['science', 'biology', 'macromolecules'],\n",
              " ['math', 'old-ap-calculus-bc', 'bc-applications-derivatives'],\n",
              " ['math', 'statistics-probability', 'significance-tests-one-sample'],\n",
              " ['science', 'ap-chemistry', 'chemical-equilibrium-ap'],\n",
              " ['math', 'old-ap-calculus-ab', 'ab-accumulation-riemann-sums'],\n",
              " ['science', 'health-and-medicine', 'human-anatomy-and-physiology'],\n",
              " ['math', 'algebra-home', 'alg-quadratics'],\n",
              " ['math', '7th-grade-foundations-engageny', '7th-m5-engage-ny-foundations'],\n",
              " ['science', 'high-school-biology', 'hs-biology-foundations'],\n",
              " ['math', '8th-engage-ny', 'engage-8th-module-7'],\n",
              " ['math',\n",
              "  'in-in-class-7-math-india-icse',\n",
              "  'in-in-7-simple-linear-equations-in-one-variable-icse'],\n",
              " ['science', 'health-and-medicine', 'endocrine-system-diseases'],\n",
              " ['math', 'old-differential-calculus', 'limits-from-equations-dc'],\n",
              " ['math', 'arithmetic', 'arith-review-negative-numbers'],\n",
              " ['science', 'biology', 'dna-as-the-genetic-material'],\n",
              " ['science', 'chemistry', 'periodic-table'],\n",
              " ['math', 'differential-equations', 'laplace-transform'],\n",
              " ['math', 'engageny-alg-1', 'alg1-module-4'],\n",
              " ['science', 'health-and-medicine', 'circulatory-system'],\n",
              " ['math', 'in-in-class-4th-math-cbse', 'x37a2a840963ae149:imp-decimals'],\n",
              " ['science',\n",
              "  'in-in-class11th-physics',\n",
              "  'in-in-11th-physics-units-and-measurement'],\n",
              " ['math', '4th-grade-foundations-engageny', '4th-m5-engage-ny-foundations'],\n",
              " ['science', 'electrical-engineering', 'ee-circuit-analysis-topic'],\n",
              " ['math', 'engageny-geo', 'geo-3'],\n",
              " ['math', 'arithmetic', 'arith-review-add-subtract'],\n",
              " ['math', 'calculus-all-old', 'derivative-applications-calc'],\n",
              " ['science', 'physics', 'geometric-optics'],\n",
              " ['math', 'differential-equations', 'first-order-differential-equations'],\n",
              " ['economics-finance-domain', 'ap-macroeconomics', 'ap-financial-sector'],\n",
              " ['math', 'cc-fifth-grade-math', 'imp-place-value-and-decimals'],\n",
              " ['math',\n",
              "  'statistics-probability',\n",
              "  'advanced-regression-inference-transforming'],\n",
              " ['science', 'biology', 'crash-course-bio-ecology'],\n",
              " ['math', '7th-grade-foundations-engageny', '7th-m4-engage-ny-foundations'],\n",
              " ['science', 'in-in-class11th-physics', 'in-in-class11th-physics-fluids'],\n",
              " ['economics-finance-domain',\n",
              "  'ap-microeconomics',\n",
              "  'ap-consumer-producer-surplus'],\n",
              " ['math', 'in-in-class-7-math-india-icse', 'in-in-7-perimeter-and-area-icse'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-determinants'],\n",
              " ['math', '8th-engage-ny', 'engage-8th-module-4'],\n",
              " ['math', 'algebra2', 'x2ec2f6f830c9fb89:transformations'],\n",
              " ['math', 'in-in-class-7-math-kv', 'xb054af1c35872e64:in-in-7-kv-integers'],\n",
              " ['science', 'physics', 'review-for-ap-physics-1-exam'],\n",
              " ['math', 'algebra-home', 'alg-basic-eq-ineq'],\n",
              " ['math', 'cc-fifth-grade-math', 'divide-decimals'],\n",
              " ['math', 'old-integral-calculus', 'convergence-tests-integral-calc'],\n",
              " ['economics-finance-domain',\n",
              "  'ap-microeconomics',\n",
              "  'production-cost-and-the-perfect-competition-model-temporary'],\n",
              " ['science', 'organic-chemistry', 'carboxylic-acids-derivatives'],\n",
              " ['math', 'linear-algebra', 'vectors-and-spaces'],\n",
              " ['math',\n",
              "  '8th-grade-illustrative-math',\n",
              "  'unit-8-pythagorean-theorem-and-irrational-numbers'],\n",
              " ['math', 'math-for-fun-and-glory', 'puzzles'],\n",
              " ['science', 'health-and-medicine', 'nervous-system-and-sensory-infor'],\n",
              " ['math', 'cc-seventh-grade-math', 'cc-7th-fractions-decimals'],\n",
              " ['math', 'old-ap-calculus-bc', 'bc-diff-equations'],\n",
              " ['math', 'old-integral-calculus', 'indefinite-integrals'],\n",
              " ['science', 'ap-chemistry', 'buffers-titrations-solubility-equilibria-ap'],\n",
              " ['math', 'statistics-probability', 'analysis-of-variance-anova-library'],\n",
              " ['math', 'algebra2', 'x2ec2f6f830c9fb89:rational'],\n",
              " ['math', 'old-ap-calculus-ab', 'ab-limits-continuity'],\n",
              " ['math', 'pre-algebra', 'pre-algebra-fractions'],\n",
              " ['science', 'organic-chemistry', 'stereochemistry-topic'],\n",
              " ['math',\n",
              "  'statistics-probability',\n",
              "  'inference-categorical-data-chi-square-tests'],\n",
              " ['math', 'ab-sixth-grade-math', 'shape-space'],\n",
              " ['economics-finance-domain', 'ap-microeconomics', 'basic-economic-concepts'],\n",
              " ['math', 'in-in-class-3rd-math-cbse', 'x80b2f4aa70819288:imp-fractions'],\n",
              " ['science',\n",
              "  'in-in-class10th-physics',\n",
              "  'in-in-10th-physics-light-reflection-refraction'],\n",
              " ['math', 'algebra-home', 'alg-modeling'],\n",
              " ['math', 'ap-statistics', 'tests-significance-ap'],\n",
              " ['math', 'in-in-grade-11-ncert', 'in-in-class11-limits'],\n",
              " ['math', 'cc-fifth-grade-math', 'powers-of-ten'],\n",
              " ['math', 'algebra-home', 'alg-absolute-value'],\n",
              " ['science', 'biology', 'water-acids-and-bases'],\n",
              " ['math', 'engageny-precalc', 'precalc-4'],\n",
              " ['math', 'math2', 'xe2ae2386aa2e13d6:solids'],\n",
              " ['science', 'health-and-medicine', 'respiratory-system'],\n",
              " ['economics-finance-domain', 'macroeconomics', 'monetary-system-topic'],\n",
              " ['math',\n",
              "  'in-in-class-5th-math-cbse',\n",
              "  'x91a8f6d2871c8046:imp-place-value-and-decimals'],\n",
              " ['math', 'algebra-home', 'alg-functions'],\n",
              " ['math', 'statistics-probability', 'designing-studies'],\n",
              " ['math', 'trigonometry', 'unit-circle-trig-func'],\n",
              " ['math', 'engageny-alg2', 'alg2-3'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-integrals'],\n",
              " ['science', 'health-and-medicine', 'respiratory-system-diseases'],\n",
              " ['science', 'biology', 'human-biology'],\n",
              " ['science', 'ap-chemistry', 'thermodynamics-ap'],\n",
              " ['science', 'physics', 'electric-charge-electric-force-and-voltage'],\n",
              " ['science', 'physics', 'quantum-physics'],\n",
              " ['math', 'calculus-2', 'cs2-differential-equations'],\n",
              " ['science', 'organic-chemistry', 'aldehydes-ketones'],\n",
              " ['math', 'arithmetic', 'fraction-arithmetic'],\n",
              " ['math', 'arithmetic-home', 'multiply-divide'],\n",
              " ['math', 'geometry-home', 'cc-geometry-circles'],\n",
              " ['math', 'early-math', 'cc-early-math-add-sub-basics'],\n",
              " ['math', 'differential-calculus', 'dc-diff-intro'],\n",
              " ['math', 'in-in-class-7-math-india-icse', 'in-in-7-fractions-icse'],\n",
              " ['math', 'in-in-class-7-math-india-icse', 'in-in-7-pair-of-angles-icse'],\n",
              " ['science', 'ap-chemistry', 'kinetics-ap'],\n",
              " ['math', 'math1', 'x89d82521517266d4:algebra-foundation'],\n",
              " ['math', 'engageny-alg2', 'alg2-4'],\n",
              " ['math', 'multivariable-calculus', 'greens-theorem-and-stokes-theorem'],\n",
              " ['science', 'physics', 'discoveries'],\n",
              " ['math', 'multivariable-calculus', 'multivariable-derivatives'],\n",
              " ['science', 'physics', 'special-relativity'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-continuity-differentiability'],\n",
              " ['math', 'cc-eighth-grade-math', 'cc-8th-numbers-operations'],\n",
              " ['science', 'in-in-class11th-physics', 'in-in-phy-kinetic-theory'],\n",
              " ['science', 'organic-chemistry', 'alcohols-ethers-epoxides-sulfides'],\n",
              " ['math', 'statistics-probability', 'summarizing-quantitative-data'],\n",
              " ['math',\n",
              "  'in-in-class-9-math-kv',\n",
              "  'xab8772e5b9da7f89:in-in-8-kv-number-systems'],\n",
              " ['math', 'geometry-home', 'geometry-lines'],\n",
              " ['math',\n",
              "  '8th-grade-illustrative-math',\n",
              "  'unit-4-linear-equations-and-linear-systems'],\n",
              " ['math', 'engageny-geo', 'geo-1'],\n",
              " ['science', 'chemistry', 'thermodynamics-chemistry'],\n",
              " ['math', 'in-in-grade-11-ncert', 'in-in-class11-linear-inequalities'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:quadratics-multiplying-factoring'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:inequalities-systems-graphs'],\n",
              " ['math', 'cc-kindergarten-math', 'cc-kindergarten-add-subtract'],\n",
              " ['science', 'class-10-biology', 'in-in-life-processes'],\n",
              " ['science', 'physics', 'mechanical-waves-and-sound'],\n",
              " ['science', 'ap-chemistry', 'stoichiometry-and-molecular-composition-ap'],\n",
              " ['science', 'physics', 'centripetal-force-and-gravitation'],\n",
              " ['science',\n",
              "  'in-in-class-10-chemistry-india',\n",
              "  'x87dd2847d57ee419:in-in-acids-bases-and-salts'],\n",
              " ['math', 'old-ap-calculus-bc', 'bc-antiderivatives-ftc'],\n",
              " ['math', 'precalculus', 'x9e81a4f98389efdf:matrices'],\n",
              " ['economics-finance-domain',\n",
              "  'macroeconomics',\n",
              "  'income-and-expenditure-topic'],\n",
              " ['math', '4th-engage-ny', 'engage-4th-module-7'],\n",
              " ['science', 'biology', 'ecology'],\n",
              " ['math', 'cc-fourth-grade-math', '4th-multiply-fractions'],\n",
              " ['math', 'cc-fourth-grade-math', 'imp-decimals'],\n",
              " ['math', 'cc-fourth-grade-math', 'imp-geometry-2'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-definite-integrals'],\n",
              " ['science', 'biology', 'history-of-life-on-earth'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:solve-equations-inequalities'],\n",
              " ['science', 'ap-physics-1', 'ap-forces-newtons-laws'],\n",
              " ['math', 'ap-calculus-bc', 'bc-differentiation-2-new'],\n",
              " ['math', 'math3', 'x5549cc1686316ba5:poly-arithmetic'],\n",
              " ['science', 'biology', 'her'],\n",
              " ['science', 'health-and-medicine', 'gastrointestinal-system-diseases'],\n",
              " ['economics-finance-domain', 'ap-microeconomics', 'factor-markets'],\n",
              " ['math', '6th-engage-ny', 'engage-6th-module-2'],\n",
              " ['math',\n",
              "  'cc-fourth-grade-math',\n",
              "  'comparing-fractions-and-equivalent-fractions'],\n",
              " ['math',\n",
              "  'multivariable-calculus',\n",
              "  'applications-of-multivariable-derivatives'],\n",
              " ['math', 'ap-statistics', 'random-variables-ap'],\n",
              " ['math', 'multivariable-calculus', 'integrating-multivariable-functions'],\n",
              " ['math', 'old-ap-calculus-ab', 'ab-derivative-rules'],\n",
              " ['math', 'old-ap-calculus-bc', 'bc-limits-continuity'],\n",
              " ['math', 'multivariable-calculus', 'thinking-about-multivariable-function'],\n",
              " ['math', 'cc-fifth-grade-math', 'imp-fractions-3'],\n",
              " ['economics-finance-domain', 'microeconomics', 'elasticity-tutorial'],\n",
              " ['math', 'algebra-home', 'alg-system-of-equations'],\n",
              " ['math', 'pre-algebra', 'pre-algebra-exponents-radicals'],\n",
              " ['math', '5th-engage-ny', 'engage-5th-module-4'],\n",
              " ['math', 'algebra-home', 'alg-matrices'],\n",
              " ['math', 'ab-sixth-grade-math', 'ab-patterns-relations'],\n",
              " ['science', 'cosmology-and-astronomy', 'earth-history-topic'],\n",
              " ['math', 'ap-calculus-bc', 'bc-differentiation-1-new'],\n",
              " ['math', 'old-integral-calculus', 'definite-integrals-intro-ic'],\n",
              " ['math',\n",
              "  'statistics-probability',\n",
              "  'significance-tests-confidence-intervals-two-samples'],\n",
              " ['science', 'cosmology-and-astronomy', 'life-earth-universe'],\n",
              " ['math', 'old-integral-calculus', 'area-and-arc-length-ic'],\n",
              " ['science', 'cosmology-and-astronomy', 'stellar-life-topic'],\n",
              " ['math', 'engageny-alg2', 'alg2-1'],\n",
              " ['math', 'ap-statistics', 'two-sample-inference'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:sequences'],\n",
              " ['math', 'arithmetic-home', 'addition-subtraction'],\n",
              " ['math', 'old-differential-calculus', 'derivative-intro-dc'],\n",
              " ['math', 'algebra-home', 'alg-conic-sections'],\n",
              " ['math', 'geometry', 'hs-geo-trig'],\n",
              " ['science', 'in-in-class9th-physics-india', 'in-in-work-energy'],\n",
              " ['math', 'geometry', 'hs-geo-transformations'],\n",
              " ['science', 'chemistry', 'oxidation-reduction'],\n",
              " ['math', 'old-ap-calculus-ab', 'ab-derivatives-advanced'],\n",
              " ['math', 'engageny-geo', 'geo-2'],\n",
              " ['math', '8th-grade-foundations-engageny', '8th-m4-engage-ny-foundations'],\n",
              " ['science', 'health-and-medicine', 'infectious-diseases'],\n",
              " ['math', '6th-grade-illustrative-math', 'unit-1-area-and-surface-area'],\n",
              " ['math', '5th-grade-foundations-engageny', '5th-m1-engage-ny-foundations'],\n",
              " ['science', 'health-and-medicine', 'health-care-system'],\n",
              " ['math', 'cc-sixth-grade-math', 'cc-6th-geometry-topic'],\n",
              " ['math', 'precalculus', 'x9e81a4f98389efdf:complex'],\n",
              " ['math', 'algebra-home', 'alg-intro-to-algebra'],\n",
              " ['math', 'cc-sixth-grade-math', 'cc-6th-arithmetic-operations'],\n",
              " ['math', 'algebra-home', 'alg-linear-eq-func'],\n",
              " ['science', 'health-and-medicine', 'nervous-system-diseases'],\n",
              " ['math', 'geometry-home', 'geometry-coordinate-plane'],\n",
              " ['math', 'in-in-class-3rd-math-cbse', 'x80b2f4aa70819288:3rd-perimeter'],\n",
              " ['economics-finance-domain',\n",
              "  'macroeconomics',\n",
              "  'macro-long-run-consequences-of-stabilization-policies'],\n",
              " ['science', 'ap-chemistry', 'studying-for-ap-chemistry-exam-class'],\n",
              " ['science', 'ap-chemistry', 'periodic-table-ap'],\n",
              " ['math', 'linear-algebra', 'matrix-transformations'],\n",
              " ['economics-finance-domain',\n",
              "  'macroeconomics',\n",
              "  'aggregate-supply-demand-topic'],\n",
              " ['math', 'math1', 'x89d82521517266d4:two-var-eq'],\n",
              " ['math', 'ap-calculus-bc', 'bc-applications-of-integration-new'],\n",
              " ['math', '5th-grade-foundations-engageny', '5th-m3-engage-ny-foundations'],\n",
              " ['math', 'ap-statistics', 'bivariate-data-ap'],\n",
              " ['math',\n",
              "  'in-in-class-2nd-math-cbse',\n",
              "  'x41ed04e12bec59cd:cc-2nd-measurement-data'],\n",
              " ['math', 'algebra-home', 'alg-trig-functions'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:systems-of-equations'],\n",
              " ['math',\n",
              "  'in-in-class-8-math-india-hindi',\n",
              "  'in-in-8-algebraic-expressions-and-identities-hindi'],\n",
              " ['math', '5th-grade-foundations-engageny', '5th-m4-engage-ny-foundations'],\n",
              " ['science', 'physics', 'thermodynamics'],\n",
              " ['science', 'ap-biology', 'chemistry-of-life'],\n",
              " ['science', 'high-school-biology', 'hs-human-body-systems'],\n",
              " ['math', 'old-ap-calculus-bc', 'bc-derivatives-analyze-functions'],\n",
              " ['science', 'biology', 'cellular-respiration-and-fermentation'],\n",
              " ['science', 'chemistry', 'acid-base-equilibrium'],\n",
              " ['math', 'math-for-fun-and-glory', 'amc-10'],\n",
              " ['math', '6th-grade-illustrative-math', 'unit-6-expressions-and-equations'],\n",
              " ['math', 'math1', 'x89d82521517266d4:exp-func'],\n",
              " ['math', 'cc-third-grade-math', 'imp-addition-and-subtraction'],\n",
              " ['math', 'math2', 'xe2ae2386aa2e13d6:circles'],\n",
              " ['math', 'differential-equations', 'second-order-differential-equations'],\n",
              " ['science', 'organic-chemistry', 'spectroscopy-jay'],\n",
              " ['math', 'in-in-class-7-math-india-icse', 'in-in-7-linear-inequalities-icse'],\n",
              " ['math', 'statistics-probability', 'modeling-distributions-of-data'],\n",
              " ['science', 'high-school-biology', 'hs-ecology'],\n",
              " ['math', 'differential-calculus', 'dc-chain'],\n",
              " ['math', 'math1', 'x89d82521517266d4:functions'],\n",
              " ['math', 'ap-calculus-ab', 'ab-applications-of-integration-new'],\n",
              " ['math', 'statistics-probability', 'random-variables-stats-library'],\n",
              " ['math', 'cc-sixth-grade-math', 'cc-6th-negative-number-topic'],\n",
              " ['economics-finance-domain', 'microeconomics', 'choices-opp-cost-tutorial'],\n",
              " ['science', 'organic-chemistry', 'substitution-elimination-reactions'],\n",
              " ['math',\n",
              "  'statistics-probability',\n",
              "  'describing-relationships-quantitative-data'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:working-units'],\n",
              " ['science', 'ap-chemistry', 'chemical-bonds-ap'],\n",
              " ['math', 'ap-calculus-bc', 'bc-advanced-functions-new'],\n",
              " ['math', 'integral-calculus', 'ic-integration'],\n",
              " ['science', 'electrical-engineering', 'lego-robotics'],\n",
              " ['math', 'algebra2', 'x2ec2f6f830c9fb89:poly-factor'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-advanced-differentiation-two'],\n",
              " ['economics-finance-domain',\n",
              "  'macroeconomics',\n",
              "  'macro-basic-economics-concepts'],\n",
              " ['math', 'cc-fifth-grade-math', 'multi-digit-multiplication-and-division'],\n",
              " ['math', 'cc-fourth-grade-math', 'imp-fractions-2'],\n",
              " ['math', 'engageny-alg-1', 'alg1-1'],\n",
              " ['math', 'old-integral-calculus', 'fundamental-theorem-of-calculus-ic'],\n",
              " ['math',\n",
              "  'in-in-class-5th-math-cbse',\n",
              "  'x91a8f6d2871c8046:multi-digit-multiplication-and-division'],\n",
              " ['math', 'pre-algebra', 'pre-algebra-factors-multiples'],\n",
              " ['math', '7th-grade-illustrative-math', 'unit-1-scale-drawings'],\n",
              " ['math', 'in-in-class-6-math-india-icse', 'in-in-6-fractions-icse'],\n",
              " ['math', 'algebra-home', 'alg-complex-numbers'],\n",
              " ['math', 'math-for-fun-and-glory', 'vi-hart'],\n",
              " ['math', 'math3', 'x5549cc1686316ba5:factorization'],\n",
              " ['science', 'health-and-medicine', 'circulatory-system-diseases'],\n",
              " ['science', 'physics', 'one-dimensional-motion'],\n",
              " ['science', 'organic-chemistry', 'aromatic-compounds'],\n",
              " ['economics-finance-domain', 'microeconomics', 'consumer-producer-surplus'],\n",
              " ['science',\n",
              "  'in-in-class11th-physics',\n",
              "  'in-in-class11th-physics-motion-in-a-plane'],\n",
              " ['math', 'statistics-probability', 'probability-library'],\n",
              " ['math', 'calculus-2', 'cs2-series'],\n",
              " ['math', 'statistics-probability', 'counting-permutations-and-combinations'],\n",
              " ['science', 'health-and-medicine', 'executive-systems-of-the-brain'],\n",
              " ['math', 'precalculus', 'x9e81a4f98389efdf:prob-comb'],\n",
              " ['math', 'early-math', 'cc-early-math-add-sub-100'],\n",
              " ['economics-finance-domain', 'ap-microeconomics', 'unit-2-supply-and-demnd'],\n",
              " ['math', 'in-in-class-6-math-india-icse', 'in-in-6-decimals-icse'],\n",
              " ['science',\n",
              "  'in-in-class11th-physics',\n",
              "  'in-in-class11th-physics-laws-of-motion'],\n",
              " ['science', 'ap-biology', 'cell-communication-and-cell-cycle'],\n",
              " ['math', 'old-ap-calculus-ab', 'ab-solved-exams'],\n",
              " ['science', 'ap-biology', 'cellular-energetics'],\n",
              " ['math', 'ap-statistics', 'inference-slope-linear-regression'],\n",
              " ['math', 'geometry-home', 'geometry-miscellaneous'],\n",
              " ['science', 'chemistry', 'chemical-reactions-stoichiome'],\n",
              " ['science', 'ap-biology', 'ecology-ap'],\n",
              " ['science', 'biology', 'properties-of-carbon'],\n",
              " ['math', 'calculus-all-old', 'series-calc'],\n",
              " ['science', 'ap-biology', 'gene-expression-and-regulation'],\n",
              " ['science', 'health-and-medicine', 'mental-health'],\n",
              " ['math', 'ap-statistics', 'sampling-distribution-ap'],\n",
              " ['science', 'in-in-class9th-physics-india', 'in-in-motion'],\n",
              " ['science', 'ap-physics-2', 'ap-fluids'],\n",
              " ['math', 'in-in-class-7-math-india-icse', 'in-in-7-symmetry-icse'],\n",
              " ['math', 'cc-eighth-grade-math', 'cc-8th-systems-topic'],\n",
              " ['math', '7th-grade-illustrative-math', 'unit-5-rational-number-arithmetic'],\n",
              " ['science', 'health-and-medicine', 'advanced-endocrine-system'],\n",
              " ['economics-finance-domain',\n",
              "  'macroeconomics',\n",
              "  'macro-economic-indicators-and-the-business-cycle'],\n",
              " ['math',\n",
              "  'in-in-class-1st-math-cbse',\n",
              "  'xdedfdde82b722757:cc-1st-add-subtract'],\n",
              " ['math', 'algebra2', 'x2ec2f6f830c9fb89:logs'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:linear-equations-graphs'],\n",
              " ['science', 'biology', 'biodiversity-and-conservation'],\n",
              " ['math', 'in-in-grade-12-ncert', 'in-in-differential-equations'],\n",
              " ['science', 'organic-chemistry', 'gen-chem-review'],\n",
              " ['math', 'geometry-home', 'geometry-angles'],\n",
              " ['science',\n",
              "  'in-in-class-12th-physics-india',\n",
              "  'in-in-electromagnetic-induction'],\n",
              " ['math', 'geometry-home', 'transformations'],\n",
              " ['math', 'cc-2nd-grade-math', 'cc-2nd-add-subtract-100'],\n",
              " ['math', 'calculus-2', 'cs2-integration-techniques'],\n",
              " ['math', 'old-ap-calculus-bc', 'bc-derivatives-advanced'],\n",
              " ['math', 'geometry', 'hs-geo-foundations'],\n",
              " ['math', 'cc-eighth-grade-math', 'cc-8th-linear-equations-functions'],\n",
              " ['math', '8th-grade-foundations-engageny', '8th-m5-engage-ny-foundations'],\n",
              " ['math', 'algebra-home', 'alg-sequences'],\n",
              " ['math', 'ap-calculus-bc', 'bc-series-new'],\n",
              " ['math', '8th-engage-ny', 'engage-8th-module-2'],\n",
              " ['math', 'math3', 'x5549cc1686316ba5:equations'],\n",
              " ['science', 'organic-chemistry', 'organic-structures'],\n",
              " ['math', 'algebra', 'x2f8bb11595b61c86:forms-of-linear-equations'],\n",
              " ['science', 'ap-physics-2', 'ap-thermodynamics'],\n",
              " ['science', 'organic-chemistry', 'ochem-alpha-carbon-chemistry'],\n",
              " ['science', 'health-and-medicine', 'healthcare-misc'],\n",
              " ['science', 'high-school-biology', 'hs-evolution'],\n",
              " ['science', 'in-in-class-12th-physics-india', 'nuclei'],\n",
              " ['math', 'calculus-all-old', 'ap-calc-topic'],\n",
              " ['math', 'statistics-probability', 'confidence-intervals-one-sample'],\n",
              " ['math', 'cc-seventh-grade-math', 'cc-7th-geometry']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W36EDNakNuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "732e83fb-5c91-40be-e1c4-e10657600a39"
      },
      "source": [
        "poincare_embedding =  [exponential_map(np.expand_dims( np.hstack(  [ poincare_model.kv.get_vector(str(x)) for x in taxonomy ] ),axis=0)) for taxonomy in poincare_emb_data ]\n",
        "np.linalg.norm(poincare_embedding[311])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8989007520441563"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIjAYTvykNuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a874fba-22cb-4e52-a491-aa6ef6dfa2c5"
      },
      "source": [
        "max_val = 0\n",
        "max_emb =None\n",
        "for embedding in poincare_embedding:\n",
        "  val = embedding.shape[1]\n",
        "  if val >max_val:\n",
        "    max_val=val\n",
        "    max_emb =embedding\n",
        "max_val\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYDH1p3okNuW"
      },
      "source": [
        "concatenated_embedding = []\n",
        "for embedding in poincare_embedding:\n",
        "  if embedding.shape[1] < max_val_train:\n",
        "    new_embedding = np.append(embedding, np.expand_dims(np.zeros(max_val_train-embedding.shape[1]),axis=0),axis=1)\n",
        "  else:\n",
        "    new_embedding = embedding\n",
        "  concatenated_embedding.append(np.squeeze(new_embedding,axis=0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxK7a0SUkNuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1cd8d19-70bd-42a8-9eb2-1699a293b993"
      },
      "source": [
        "poincare_embeddings_final = np.stack(concatenated_embedding, axis=0)\n",
        "poincare_embeddings_final.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(416, 60)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nso39n1N_po_"
      },
      "source": [
        "# model2 = MulticlassClassifier('bert-base-uncased')\n",
        "# model2.load_state_dict(torch.load('model_hyperbolic_round_2/model_weights'))\n",
        "# model2.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe4qYkV2C4fX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b904fb48-ba88-4a0c-a807-04285d0d3d34"
      },
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in test_features:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "# labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "test_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "# prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_poincare_tensor)\n",
        "# prediction_sampler = SequentialSampler(prediction_data)\n",
        "# prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNdlve8AJcCO"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "test_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6aMBHkAQZjT"
      },
      "source": [
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "def dist_without_grad( u, v):\n",
        "  sqdist = torch.sum((u - v) ** 2, dim=-1)\n",
        "  squnorm = torch.sum(u ** 2, dim=-1)\n",
        "  sqvnorm = torch.sum(v ** 2, dim=-1)\n",
        "  x = 1 + 2 * sqdist / ((1 - squnorm) * (1 - sqvnorm)) + 1e-7\n",
        "  z = torch.sqrt(x ** 2 - 1)\n",
        "  return torch.log(x + z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe0otXOPg7z0"
      },
      "source": [
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfbTB6VHhKci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed43b7a-f8f1-4872-8027-4dab3204ec4d"
      },
      "source": [
        "torch.topk(dist_without_grad(model2(test_input_ids[0].to('cuda').reshape(1,-1),test_attention_masks[0].to('cuda').reshape(1,-1)),test_poincare_tensor),3,largest=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(values=tensor([3.1841, 3.1841, 3.1841], device='cuda:0', grad_fn=<TopkBackward>), indices=tensor([839, 682,  15], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8sPJhJJk_gQ"
      },
      "source": [
        "def precision(actual, predicted, k):\n",
        "    act_set = set(actual)\n",
        "    pred_set = set(predicted[:k])\n",
        "    result = len(act_set & pred_set) / float(k)\n",
        "    return result\n",
        "\n",
        "def recall(actual, predicted, k):\n",
        "    act_set = set(actual)\n",
        "    pred_set = set(predicted[:k])\n",
        "    result = len(act_set & pred_set) / float(len(act_set))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPCktQT9DVT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0808c59c-320c-4383-8aa1-87c9cc96a8c3"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "input_ids = test_input_ids.to('cuda')\n",
        "attention_masks = test_attention_masks.to('cuda')\n",
        "test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(input_ids, attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "  distances,indices = torch.topk(dist_without_grad(outputs,test_poincare_tensor),5,largest=False)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))\n",
        "  # max_distance =100000000000000\n",
        "  # label=None\n",
        "  # for index,test_poincare in enumerate(test_poincare_tensor):\n",
        "\n",
        "  #   distance = distanceTo(test_poincare, outputs)\n",
        "  #   if distance < max_distance:\n",
        "  #     max_distance = distance\n",
        "  #     label = index\n",
        "  # predictions.append(labels[label])\n",
        "    \n",
        "# Predict \n",
        "# for batch in prediction_dataloader:\n",
        "#   # Add batch to GPU\n",
        "#   batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "#   # Unpack the inputs from our dataloader\n",
        "#   b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "#   # Telling the model not to compute or store gradients, saving memory and \n",
        "#   # speeding up prediction\n",
        "#   with torch.no_grad():\n",
        "#       # Forward pass, calculate logit predictions\n",
        "#       outputs = model(b_input_ids,b_input_mask)\n",
        "\n",
        "#   logits = outputs\n",
        "#   for logit in logits:\n",
        "#     max_similarity = 0\n",
        "\n",
        "\n",
        "#   # Move logits and labels to CPU\n",
        "#   logits = logits.detach().cpu().numpy()\n",
        "#   label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "#   # Store predictions and true labels\n",
        "#   predictions.append(logits)\n",
        "#   true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')\n",
        "# predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,047 test sentences...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1047\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJCr2Bi89Zw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec113e63-91bf-4afc-9397-da967d0200ed"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (53.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5wj2gXYFkrQ"
      },
      "source": [
        "labels=test_data['hierarchy'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2BANBZnDigR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yrV51kL43SI",
        "outputId": "5b8381ee-36e5-4c13-af05-b64c9555e063"
      },
      "source": [
        "for index,label in enumerate(labels):\n",
        "  if label not in list(train_data[\"hierarchy\"].values):\n",
        "    print(index,label)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61 math>>ap-statistics>>density-curves-normal-distribution-ap\n",
            "314 math>>ap-statistics>>density-curves-normal-distribution-ap\n",
            "385 math>>arithmetic>>fraction-arithmetic\n",
            "436 math>>old-integral-calculus>>convergence-tests-integral-calc\n",
            "444 math>>ap-statistics>>density-curves-normal-distribution-ap\n",
            "879 math>>old-integral-calculus>>convergence-tests-integral-calc\n",
            "926 math>>arithmetic>>fraction-arithmetic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuZEbp-tFaD8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOt8hvs-CZfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3290ad6c-e9b0-480b-c696-bb17b6f9ee85"
      },
      "source": [
        "from sklearn .preprocessing import LabelEncoder\n",
        "LE= LabelEncoder()\n",
        "labels = LE.fit_transform(labels)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([238, 254,   2, ..., 386, 283, 324])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adb4gTNgGKUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe6f539-1078-40f5-e683-bd2b001c5cce"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([238, 254,   2, ..., 386, 283, 324])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZIY_54gHm7M",
        "outputId": "7ed481ac-363a-4066-b396-3b822c28ab3f"
      },
      "source": [
        "np.where(labels==325)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 787, 1034]),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azkCfK-bCoXd"
      },
      "source": [
        "final_predictions = []\n",
        "for prediction in predictions:\n",
        "  final_predictions.append(LE.transform(prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T9K5Gd6MMWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e2a8f55-bf2e-477e-f61a-fa79f9de985a"
      },
      "source": [
        "y_true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Identity_6:0' shape=(1400,) dtype=int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSDt3De5myle",
        "outputId": "d7caa739-d87c-4d6a-8780-916097858a41"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqmxvWkg2vNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86cd685-78af-4772-e297-9a4a96d41849"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 20\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=20)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 20)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 20) (1047,)\n",
            "update_recall:  0.562559694364852\n",
            "recall 0.562559694364852\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 589.0, 458.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[243, 242, 241, ..., 177, 176,  29],\n",
            "       [226, 225, 224, ...,  89,  88,  87],\n",
            "       [ 20,  19,  18, ...,   2,   1,   0],\n",
            "       ...,\n",
            "       [279, 278, 277, ...,  91,  28,  26],\n",
            "       [284, 283, 282, ...,  79,  78,  77],\n",
            "       [334, 332, 331, ..., 301, 300, 299]]), indices=array([[ 5,  6,  4, ...,  7,  9,  3],\n",
            "       [ 2,  0,  1, ...,  9,  7, 10],\n",
            "       [ 8, 13, 14, ...,  9,  7, 12],\n",
            "       ...,\n",
            "       [16, 15, 17, ..., 10, 18, 19],\n",
            "       [ 4,  3,  2, ..., 10, 12, 13],\n",
            "       [12, 13, 14, ...,  3,  0,  2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI1jhndp6cEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2707b8b7-f01f-4a7e-8c4e-0feb2f7cea22"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 15)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 15)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 15)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 15) (1047,)\n",
            "update_recall:  0.5243553008595988\n",
            "recall 0.5243553008595988\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 549.0, 498.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[243, 242, 241, ..., 177, 176,  29],\n",
            "       [226, 225, 224, ...,  89,  88,  87],\n",
            "       [ 20,  19,  18, ...,   2,   1,   0],\n",
            "       ...,\n",
            "       [276, 182, 181, ...,  93,  92,  91],\n",
            "       [284, 283, 282, ...,  79,  78,  77],\n",
            "       [334, 332, 331, ..., 301, 300, 299]]), indices=array([[ 5,  6,  4, ...,  7,  9,  3],\n",
            "       [ 2,  0,  1, ...,  9,  7, 10],\n",
            "       [ 8, 13, 14, ...,  9,  7, 12],\n",
            "       ...,\n",
            "       [14,  2,  5, ...,  9, 13, 10],\n",
            "       [ 4,  3,  2, ..., 10, 12, 13],\n",
            "       [12, 13, 14, ...,  3,  0,  2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUXWWk3jpKCM",
        "outputId": "9649e859-886c-4f94-e150-b2a436df15a3"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 10)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 10)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 10)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 10) (1047,)\n",
            "update_recall:  0.44508118433619864\n",
            "recall 0.44508118433619864\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 466.0, 581.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[243, 242, 241, ..., 177, 176,  29],\n",
            "       [226, 225, 224, ...,  90,  89,  88],\n",
            "       [ 20,  16,  15, ...,  10,   2,   1],\n",
            "       ...,\n",
            "       [182, 181, 180, ...,  95,  94,  93],\n",
            "       [284, 283, 282, ...,  82,  81,  80],\n",
            "       [326, 323, 321, ..., 301, 300, 299]]), indices=array([[5, 6, 4, ..., 7, 9, 3],\n",
            "       [2, 0, 1, ..., 8, 9, 7],\n",
            "       [8, 6, 4, ..., 1, 9, 7],\n",
            "       ...,\n",
            "       [2, 5, 4, ..., 8, 7, 9],\n",
            "       [4, 3, 2, ..., 6, 5, 8],\n",
            "       [8, 7, 9, ..., 3, 0, 2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_khOH3epEC1x",
        "outputId": "e01f08f7-d793-48be-f9e2-5ac65aba7b10"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 5)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 5)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 5)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 5) (1047,)\n",
            "update_recall:  0.3037249283667622\n",
            "recall 0.3037249283667622\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 318.0, 729.0, 0.0, 0.0]\n",
            "TMP_RANK:  TopKV2(values=array([[241, 191, 190, 189,  29],\n",
            "       [226, 225, 224, 133, 132],\n",
            "       [ 15,  14,  13,  11,  10],\n",
            "       ...,\n",
            "       [182, 180, 179, 178, 177],\n",
            "       [284, 283, 282, 281, 280],\n",
            "       [305, 302, 301, 300, 299]]), indices=array([[4, 1, 2, 0, 3],\n",
            "       [2, 0, 1, 4, 3],\n",
            "       [4, 3, 2, 0, 1],\n",
            "       ...,\n",
            "       [2, 4, 0, 1, 3],\n",
            "       [4, 3, 2, 1, 0],\n",
            "       [1, 4, 3, 0, 2]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15UfS3-JunMW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}