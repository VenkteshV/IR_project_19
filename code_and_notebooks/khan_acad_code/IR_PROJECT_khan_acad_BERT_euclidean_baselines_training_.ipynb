{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IR_PROJECT_khan_acad_BERT_euclidean_baselines_training .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2b7f889d2c442e9bb9a6ff6918bf9ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d3dd53d8b41d46cda3fd0a7316c563a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_93e82f983a4247758f4d5e61ead14fde",
              "IPY_MODEL_b7aba79a7cfd4e16a51c20df4d32905d"
            ]
          }
        },
        "d3dd53d8b41d46cda3fd0a7316c563a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93e82f983a4247758f4d5e61ead14fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9540bd040bf493fb19699d450ee2208",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d728545a237418eaeeec91b2ee4d6f6"
          }
        },
        "b7aba79a7cfd4e16a51c20df4d32905d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d96a6725c2fe44e585a4e85e9c21e346",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.15MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6d0b7d2bfcd43f3932f4ef60cb82b76"
          }
        },
        "f9540bd040bf493fb19699d450ee2208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d728545a237418eaeeec91b2ee4d6f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d96a6725c2fe44e585a4e85e9c21e346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6d0b7d2bfcd43f3932f4ef60cb82b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR9av2JU3kf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c5d435-fd29-4eac-9819-e3f03e8f35b2"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di-2CSfib17y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d53df0e-863f-4009-c452-bfbc876c7d03"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzKeqoCs3kgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdb3185-9820-4d1f-9312-c7dad2575eb4"
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573kB 17.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 43.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 52.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7MB 52.8MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/88/788fa9c12396c9b468d3cbddb4dd9196d08abd8a8cddf093d170b5d83b5c/boto3-1.17.12.tar.gz (100kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 15.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.0.0)\n",
            "Collecting botocore<1.21.0,>=1.20.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/78/b71baa2fa2dac70638a360ec6fdb00960134a1b68e895acc12b8f6916da2/botocore-1.20.12-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.2MB 51.9MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.12->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses, boto3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=40ede09eff4d1c2babf5c06b394dcacce2fb62436c7e1d2d0d8e148f32fe3510\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.17.12-py2.py3-none-any.whl size=128775 sha256=c70408932f6b9c866e46aca90e29e8560e10d1c0b26a996ceb67f59afab93ae5\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/00/be/27e37811c37fe384b780ce89ce3b8e021493b1ab7d19fa71ef\n",
            "Successfully built sacremoses boto3\n",
            "\u001b[31mERROR: botocore 1.20.12 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, jmespath, botocore, s3transfer, boto3, transformers\n",
            "Successfully installed boto3-1.17.12 botocore-1.20.12 jmespath-0.10.0 s3transfer-0.3.4 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mNG6fA-53zb",
        "outputId": "32203e7c-efed-4093-dfa8-8a9d1558258e"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92.5MB 46kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368kB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (53.0.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Installing collected packages: tensorboard, keras-applications, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr_TarOVYOjm"
      },
      "source": [
        "#Preprocessing script required for first time\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/khan_acad_ir_project.csv\" /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAdSVuPgZS8Q"
      },
      "source": [
        "import re\n",
        "def clean_sentence(question):\n",
        "  # print(question)\n",
        "  question = re.sub('<[^>]*>', ' ',question)\n",
        "  question = re.sub(' +', ' ', question)\n",
        "  question = re.sub('\\xa0','',question)\n",
        "  question = question.rstrip()\n",
        "  question = re.sub('nan','',question)\n",
        "  question = re.sub(u'\\u2004','',question)\n",
        "  question = re.sub(u'\\u2009','',question)\n",
        "  question = re.sub(\"Voiceover:\",\"\",question)\n",
        "  question = re.sub(\"â€¢\",\"\",question)\n",
        "  question = re.sub(\"\\d:\\d\\d\",\"\",question)\n",
        "\n",
        "  question = re.sub(\"\\[Voiceover\\]\",\"\",question)\n",
        "  question = re.sub(\"\\[Instructor\\]\",\"\",question)\n",
        "  question = re.sub(\"\\[Narrator\\]\",\"\",question)\n",
        "\n",
        "  # question = question.decode(\"utf-8\")\n",
        "  # question = question.replace(u'\\u200\\d*','').encode(\"utf-8\")\n",
        "  question = re.sub('&nbsp','',question)\n",
        "  question = re.sub('&ndash','',question)\n",
        "  question = re.sub('\\r','',question)\n",
        "  question = re.sub('\\t','',question)\n",
        "  question = re.sub('\\n',' ',question)\n",
        "\n",
        "  question = re.sub('MathType@.*','',question)\n",
        "  question = re.sub('&thinsp','',question)\n",
        "  question = re.sub('&times','',question)\n",
        "  question = re.sub('\\u200b','',question)\n",
        "  question = re.sub('&rarr;;;','',question)\n",
        "\n",
        "  return question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "cRZxPmqzY4xp",
        "outputId": "0cb7584a-3364-461e-c7fe-df4b121d24e8"
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"khan_acad_ir_project.csv\")\n",
        "data\n",
        "output = data[\"video_transcripts\"].apply(lambda x: clean_sentence(x))\n",
        "data[\"video_transcripts\"] = output\n",
        "data = data[[\"video_transcripts\",\"hierarchy\"]]\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Here's an example of a nucleophilic addition...</td>\n",
              "      <td>science&gt;&gt;organic-chemistry&gt;&gt;aldehydes-ketones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In the previous video, we saw how to make hy...</td>\n",
              "      <td>science&gt;&gt;organic-chemistry&gt;&gt;aldehydes-ketones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We've already seen how to form hydrates and ...</td>\n",
              "      <td>science&gt;&gt;organic-chemistry&gt;&gt;aldehydes-ketones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If we react an aldehyde, or a ketone, with a...</td>\n",
              "      <td>science&gt;&gt;organic-chemistry&gt;&gt;aldehydes-ketones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We've already seen how to form acetals. If w...</td>\n",
              "      <td>science&gt;&gt;organic-chemistry&gt;&gt;aldehydes-ketones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6154</th>\n",
              "      <td>-  Let's multiply 40 times 70. So 40 times, w...</td>\n",
              "      <td>math&gt;&gt;arithmetic-home&gt;&gt;multiply-divide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6155</th>\n",
              "      <td>-  Let's talk about multiplying by 10, 100, a...</td>\n",
              "      <td>math&gt;&gt;arithmetic-home&gt;&gt;multiply-divide</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6156</th>\n",
              "      <td>When we were first exposed to multiplication ...</td>\n",
              "      <td>math&gt;&gt;arithmetic-home&gt;&gt;arith-review-fractions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6157</th>\n",
              "      <td>My wife and I have recently purchased an asso...</td>\n",
              "      <td>math&gt;&gt;arithmetic-home&gt;&gt;arith-review-fractions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6158</th>\n",
              "      <td>We've already seen that a fraction like 2/9 c...</td>\n",
              "      <td>math&gt;&gt;arithmetic-home&gt;&gt;arith-review-fractions</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6159 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts                                      hierarchy\n",
              "0       Here's an example of a nucleophilic addition...  science>>organic-chemistry>>aldehydes-ketones\n",
              "1       In the previous video, we saw how to make hy...  science>>organic-chemistry>>aldehydes-ketones\n",
              "2       We've already seen how to form hydrates and ...  science>>organic-chemistry>>aldehydes-ketones\n",
              "3       If we react an aldehyde, or a ketone, with a...  science>>organic-chemistry>>aldehydes-ketones\n",
              "4       We've already seen how to form acetals. If w...  science>>organic-chemistry>>aldehydes-ketones\n",
              "...                                                 ...                                            ...\n",
              "6154   -  Let's multiply 40 times 70. So 40 times, w...         math>>arithmetic-home>>multiply-divide\n",
              "6155   -  Let's talk about multiplying by 10, 100, a...         math>>arithmetic-home>>multiply-divide\n",
              "6156   When we were first exposed to multiplication ...  math>>arithmetic-home>>arith-review-fractions\n",
              "6157   My wife and I have recently purchased an asso...  math>>arithmetic-home>>arith-review-fractions\n",
              "6158   We've already seen that a fraction like 2/9 c...  math>>arithmetic-home>>arith-review-fractions\n",
              "\n",
              "[6159 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX-cUMCrhCAG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,val = train_test_split(data,test_size=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz0UT3xrnuiP"
      },
      "source": [
        "train,test = train_test_split(train,test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "5c9B_ZRdm_nV",
        "outputId": "625b01d4-4888-4a75-f650-d634b8475429"
      },
      "source": [
        "val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2111</th>\n",
              "      <td>Find the probability of rolling doubles on tw...</td>\n",
              "      <td>math&gt;&gt;precalculus&gt;&gt;x9e81a4f98389efdf:prob-comb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1633</th>\n",
              "      <td>After the food is swallowed, it leaves the m...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5365</th>\n",
              "      <td>Let's now talk about what is easily one of th...</td>\n",
              "      <td>math&gt;&gt;geometry&gt;&gt;hs-geo-trig</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2517</th>\n",
              "      <td>The goal in this video is to essentially prov...</td>\n",
              "      <td>science&gt;&gt;chemistry&gt;&gt;thermodynamics-chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3665</th>\n",
              "      <td>A line goes through the points (-1, 6) and (5...</td>\n",
              "      <td>math&gt;&gt;in-in-grade-11-ncert&gt;&gt;in-in-class11-stra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2465</th>\n",
              "      <td>-  As long as human beings have been around I...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;gene-expression-and-regul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5214</th>\n",
              "      <td>Let's see if we can multiply 9 times 0.6. Or ...</td>\n",
              "      <td>math&gt;&gt;6th-engage-ny&gt;&gt;engage-6th-module-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5480</th>\n",
              "      <td>-  So let's say that we have y is equal to th...</td>\n",
              "      <td>math&gt;&gt;calculus-all-old&gt;&gt;taking-derivatives-calc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5600</th>\n",
              "      <td>-  What I want to do in this video is see if ...</td>\n",
              "      <td>math&gt;&gt;old-ap-calculus-ab&gt;&gt;ab-accumulation-riem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>-  In this video, I'm going to introduce you ...</td>\n",
              "      <td>math&gt;&gt;cc-fifth-grade-math&gt;&gt;powers-of-ten</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>924 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts                                          hierarchy\n",
              "2111   Find the probability of rolling doubles on tw...     math>>precalculus>>x9e81a4f98389efdf:prob-comb\n",
              "1633    After the food is swallowed, it leaves the m...  science>>health-and-medicine>>human-anatomy-an...\n",
              "5365   Let's now talk about what is easily one of th...                        math>>geometry>>hs-geo-trig\n",
              "2517   The goal in this video is to essentially prov...       science>>chemistry>>thermodynamics-chemistry\n",
              "3665   A line goes through the points (-1, 6) and (5...  math>>in-in-grade-11-ncert>>in-in-class11-stra...\n",
              "...                                                 ...                                                ...\n",
              "2465   -  As long as human beings have been around I...  science>>ap-biology>>gene-expression-and-regul...\n",
              "5214   Let's see if we can multiply 9 times 0.6. Or ...           math>>6th-engage-ny>>engage-6th-module-2\n",
              "5480   -  So let's say that we have y is equal to th...    math>>calculus-all-old>>taking-derivatives-calc\n",
              "5600   -  What I want to do in this video is see if ...  math>>old-ap-calculus-ab>>ab-accumulation-riem...\n",
              "1552   -  In this video, I'm going to introduce you ...           math>>cc-fifth-grade-math>>powers-of-ten\n",
              "\n",
              "[924 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Q0_s4Mznn2MU",
        "outputId": "b6af3aa2-72ef-4fa6-c3fc-214e4b6876db"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>In the last couple of videos we saw that we c...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3556</th>\n",
              "      <td>-  What we're going to do in this video is gi...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;natural-selection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5435</th>\n",
              "      <td>So once again, we have three equal, or we say...</td>\n",
              "      <td>math&gt;&gt;pre-algebra&gt;&gt;pre-algebra-equations-expre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2184</th>\n",
              "      <td>-  Liz's math test included a survey question...</td>\n",
              "      <td>math&gt;&gt;engageny-alg-1&gt;&gt;alg1-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5848</th>\n",
              "      <td>- The following two equations form a linear s...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-system-of-equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>-  Hello everyone. So this is what I might ca...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4886</th>\n",
              "      <td>-  Let's try now to subtract some two-digit n...</td>\n",
              "      <td>math&gt;&gt;early-math&gt;&gt;cc-early-math-add-sub-100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5371</th>\n",
              "      <td>- Let's say that I have a circle. My best att...</td>\n",
              "      <td>math&gt;&gt;engageny-geo&gt;&gt;geo-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5247</th>\n",
              "      <td>- So let's look at the female reproductive cy...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2912</th>\n",
              "      <td>-  We're now in the home stretch. We just hav...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;greens-theorem-a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4188 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts                                          hierarchy\n",
              "166    In the last couple of videos we saw that we c...  math>>multivariable-calculus>>multivariable-de...\n",
              "3556   -  What we're going to do in this video is gi...             science>>ap-biology>>natural-selection\n",
              "5435   So once again, we have three equal, or we say...  math>>pre-algebra>>pre-algebra-equations-expre...\n",
              "2184   -  Liz's math test included a survey question...                       math>>engageny-alg-1>>alg1-2\n",
              "5848   - The following two equations form a linear s...        math>>algebra-home>>alg-system-of-equations\n",
              "...                                                 ...                                                ...\n",
              "1498   -  Hello everyone. So this is what I might ca...  math>>multivariable-calculus>>multivariable-de...\n",
              "4886   -  Let's try now to subtract some two-digit n...        math>>early-math>>cc-early-math-add-sub-100\n",
              "5371   - Let's say that I have a circle. My best att...                          math>>engageny-geo>>geo-5\n",
              "5247   - So let's look at the female reproductive cy...  science>>health-and-medicine>>human-anatomy-an...\n",
              "2912   -  We're now in the home stretch. We just hav...  math>>multivariable-calculus>>greens-theorem-a...\n",
              "\n",
              "[4188 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "LLDKtNTnmwre",
        "outputId": "d160b868-b0c8-4627-dac4-2c7522de57c5"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5576</th>\n",
              "      <td>-  What I hope to do in this video is get fam...</td>\n",
              "      <td>math&gt;&gt;math1&gt;&gt;x89d82521517266d4:functions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4317</th>\n",
              "      <td>In the last video we were able to set up this...</td>\n",
              "      <td>math&gt;&gt;old-ap-calculus-ab&gt;&gt;ab-applications-defi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3261</th>\n",
              "      <td>-  In previous videos we talk about GDP as th...</td>\n",
              "      <td>economics-finance-domain&gt;&gt;ap-macroeconomics&gt;&gt;e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2342</th>\n",
              "      <td>-  So what we're gonna do in this video is se...</td>\n",
              "      <td>math&gt;&gt;old-integral-calculus&gt;&gt;definite-integral...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3892</th>\n",
              "      <td>-  So I've said that if you have a vector fie...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>In the last video, we saw that if a vector fi...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;integrating-mult...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3733</th>\n",
              "      <td>So I have the function g of x is equal to 9 t...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-sequences</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>Let's see if we can get a little bit more pra...</td>\n",
              "      <td>science&gt;&gt;in-in-class11th-physics&gt;&gt;in-in-system...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4802</th>\n",
              "      <td>Find the probability of rolling even numbers ...</td>\n",
              "      <td>math&gt;&gt;precalculus&gt;&gt;x9e81a4f98389efdf:prob-comb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>-  In the last video we began to see some pre...</td>\n",
              "      <td>science&gt;&gt;biology&gt;&gt;dna-as-the-genetic-material</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1047 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts                                          hierarchy\n",
              "5576   -  What I hope to do in this video is get fam...           math>>math1>>x89d82521517266d4:functions\n",
              "4317   In the last video we were able to set up this...  math>>old-ap-calculus-ab>>ab-applications-defi...\n",
              "3261   -  In previous videos we talk about GDP as th...  economics-finance-domain>>ap-macroeconomics>>e...\n",
              "2342   -  So what we're gonna do in this video is se...  math>>old-integral-calculus>>definite-integral...\n",
              "3892   -  So I've said that if you have a vector fie...  math>>multivariable-calculus>>multivariable-de...\n",
              "...                                                 ...                                                ...\n",
              "439    In the last video, we saw that if a vector fi...  math>>multivariable-calculus>>integrating-mult...\n",
              "3733   So I have the function g of x is equal to 9 t...                  math>>algebra-home>>alg-sequences\n",
              "2767   Let's see if we can get a little bit more pra...  science>>in-in-class11th-physics>>in-in-system...\n",
              "4802   Find the probability of rolling even numbers ...     math>>precalculus>>x9e81a4f98389efdf:prob-comb\n",
              "508    -  In the last video we began to see some pre...      science>>biology>>dna-as-the-genetic-material\n",
              "\n",
              "[1047 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dp9M3U2oLqQ"
      },
      "source": [
        "train.to_csv(\"train_khan_acad.csv\",index=False)\n",
        "test.to_csv(\"test_khan_acad.csv\",index=False)\n",
        "val.to_csv(\"val_khan_acad.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYy-5y0WoW8l"
      },
      "source": [
        "!cp train_khan_acad.csv \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/\"\n",
        "!cp test_khan_acad.csv \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/\"\n",
        "!cp val_khan_acad.csv \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqzhuo9vbqmb"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/QC_science/train_khan_acad.csv\" /content\n",
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/QC_science/test_khan_acad.csv\" /content\n",
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/QC_science/val_khan_acad.csv\" /content\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsADhaO93kgD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "cda0e44b-df76-4cc6-c95c-7bb7f8e34e46"
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"train_khan_acad.csv\")\n",
        "test_data = pd.read_csv(\"test_khan_acad.csv\")\n",
        "val_data = pd.read_csv(\"val_khan_acad.csv\")\n",
        "train_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the last couple of videos we saw that we c...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-  What we're going to do in this video is gi...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;natural-selection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So once again, we have three equal, or we say...</td>\n",
              "      <td>math&gt;&gt;pre-algebra&gt;&gt;pre-algebra-equations-expre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  Liz's math test included a survey question...</td>\n",
              "      <td>math&gt;&gt;engageny-alg-1&gt;&gt;alg1-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>- The following two equations form a linear s...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-system-of-equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4183</th>\n",
              "      <td>-  Hello everyone. So this is what I might ca...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4184</th>\n",
              "      <td>-  Let's try now to subtract some two-digit n...</td>\n",
              "      <td>math&gt;&gt;early-math&gt;&gt;cc-early-math-add-sub-100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4185</th>\n",
              "      <td>- Let's say that I have a circle. My best att...</td>\n",
              "      <td>math&gt;&gt;engageny-geo&gt;&gt;geo-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4186</th>\n",
              "      <td>- So let's look at the female reproductive cy...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4187</th>\n",
              "      <td>-  We're now in the home stretch. We just hav...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;greens-theorem-a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4188 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts                                          hierarchy\n",
              "0      In the last couple of videos we saw that we c...  math>>multivariable-calculus>>multivariable-de...\n",
              "1      -  What we're going to do in this video is gi...             science>>ap-biology>>natural-selection\n",
              "2      So once again, we have three equal, or we say...  math>>pre-algebra>>pre-algebra-equations-expre...\n",
              "3      -  Liz's math test included a survey question...                       math>>engageny-alg-1>>alg1-2\n",
              "4      - The following two equations form a linear s...        math>>algebra-home>>alg-system-of-equations\n",
              "...                                                 ...                                                ...\n",
              "4183   -  Hello everyone. So this is what I might ca...  math>>multivariable-calculus>>multivariable-de...\n",
              "4184   -  Let's try now to subtract some two-digit n...        math>>early-math>>cc-early-math-add-sub-100\n",
              "4185   - Let's say that I have a circle. My best att...                          math>>engageny-geo>>geo-5\n",
              "4186   - So let's look at the female reproductive cy...  science>>health-and-medicine>>human-anatomy-an...\n",
              "4187   -  We're now in the home stretch. We just hav...  math>>multivariable-calculus>>greens-theorem-a...\n",
              "\n",
              "[4188 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQhO6qqt6lge"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWOeBaD23kga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622df3e8-f498-4a38-c78f-d180cc5d9d39"
      },
      "source": [
        "train_data[\"hierarchy\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "science>>health-and-medicine>>circulatory-system-diseases             99\n",
              "science>>health-and-medicine>>human-anatomy-and-physiology            65\n",
              "science>>health-and-medicine>>respiratory-system-diseases             55\n",
              "science>>health-and-medicine>>circulatory-system                      54\n",
              "science>>health-and-medicine>>infectious-diseases                     52\n",
              "                                                                      ..\n",
              "math>>algebra-basics>>alg-basics-expressions-with-exponents            1\n",
              "science>>ap-physics-1>>ap-one-dimensional-motion                       1\n",
              "math>>precalculus>>x9e81a4f98389efdf:vectors                           1\n",
              "math>>4th-grade-foundations-engageny>>4th-m5-engage-ny-foundations     1\n",
              "math>>old-ap-calculus-ab>>ab-derivatives-advanced                      1\n",
              "Name: hierarchy, Length: 569, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNrGNk8f3kgh"
      },
      "source": [
        "# final_data_1 = final_data.loc[0:71003,:]\n",
        "# final_data_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gDXqGDyKQoy"
      },
      "source": [
        "!mv model_save_categorized_reduced_khan_acad \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoqsPzCDGj50"
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_save_categorized_reduced_nov\" /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrS5sxE3kgk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315,
          "referenced_widgets": [
            "b2b7f889d2c442e9bb9a6ff6918bf9ee",
            "d3dd53d8b41d46cda3fd0a7316c563a3",
            "93e82f983a4247758f4d5e61ead14fde",
            "b7aba79a7cfd4e16a51c20df4d32905d",
            "f9540bd040bf493fb19699d450ee2208",
            "6d728545a237418eaeeec91b2ee4d6f6",
            "d96a6725c2fe44e585a4e85e9c21e346",
            "a6d0b7d2bfcd43f3932f4ef60cb82b76"
          ]
        },
        "outputId": "18d2e744-7acb-4285-c8f6-9d1f555ed70e"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2b7f889d2c442e9bb9a6ff6918bf9ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mgc72PQYV1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72fbbc58-67c5-4b03-8abe-fbf4cdaff99f"
      },
      "source": [
        "test_data[\"hierarchy\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "science>>health-and-medicine>>human-anatomy-and-physiology    24\n",
              "science>>health-and-medicine>>circulatory-system-diseases     22\n",
              "science>>health-and-medicine>>circulatory-system              17\n",
              "math>>algebra-home>>alg-polynomials                           11\n",
              "science>>biology>>crash-course-bio-ecology                    11\n",
              "                                                              ..\n",
              "math>>ap-statistics>>inference-slope-linear-regression         1\n",
              "science>>physics>>thermodynamics                               1\n",
              "math>>old-integral-calculus>>indefinite-integrals              1\n",
              "math>>ap-calculus-ab>>ab-limits-new                            1\n",
              "science>>chemistry>>chemical-bonds                             1\n",
              "Name: hierarchy, Length: 416, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3lYOb2K3kgy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f3a780b4-b547-471f-b288-cbe51b1a14f4"
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "LE = LabelEncoder()\n",
        "LE.fit_transform(pd.concat([train_data['hierarchy'],test_data['hierarchy']]))\n",
        "train_data['label'] = LE.transform(train_data['hierarchy'])\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the last couple of videos we saw that we c...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-  What we're going to do in this video is gi...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;natural-selection</td>\n",
              "      <td>422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So once again, we have three equal, or we say...</td>\n",
              "      <td>math&gt;&gt;pre-algebra&gt;&gt;pre-algebra-equations-expre...</td>\n",
              "      <td>384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  Liz's math test included a survey question...</td>\n",
              "      <td>math&gt;&gt;engageny-alg-1&gt;&gt;alg1-2</td>\n",
              "      <td>231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>- The following two equations form a linear s...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-system-of-equations</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   video_transcripts  ... label\n",
              "0   In the last couple of videos we saw that we c...  ...   354\n",
              "1   -  What we're going to do in this video is gi...  ...   422\n",
              "2   So once again, we have three equal, or we say...  ...   384\n",
              "3   -  Liz's math test included a survey question...  ...   231\n",
              "4   - The following two equations form a linear s...  ...    99\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp64MkNB3kg1"
      },
      "source": [
        "def get_labels(prediction):\n",
        "    predicted_label =  LE.inverse_transform([prediction])\n",
        "    return predicted_label[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPgTmJPS3kg4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fe416a4-80bd-437d-81b6-20942119121a"
      },
      "source": [
        "get_labels(204)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math>>cc-seventh-grade-math>>cc-7th-fractions-decimals'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_UpqLMG3kg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1947905d-c552-4ffc-8a3c-bce540cc6803"
      },
      "source": [
        "train_data.iloc[14,1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'economics-finance-domain>>macroeconomics>>monetary-system-topic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5a3P7jSZZ6B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "044a7c29-c006-44dc-c249-86147a41c162"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the last couple of videos we saw that we c...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-  What we're going to do in this video is gi...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;natural-selection</td>\n",
              "      <td>422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So once again, we have three equal, or we say...</td>\n",
              "      <td>math&gt;&gt;pre-algebra&gt;&gt;pre-algebra-equations-expre...</td>\n",
              "      <td>384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  Liz's math test included a survey question...</td>\n",
              "      <td>math&gt;&gt;engageny-alg-1&gt;&gt;alg1-2</td>\n",
              "      <td>231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>- The following two equations form a linear s...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-system-of-equations</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4183</th>\n",
              "      <td>-  Hello everyone. So this is what I might ca...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4184</th>\n",
              "      <td>-  Let's try now to subtract some two-digit n...</td>\n",
              "      <td>math&gt;&gt;early-math&gt;&gt;cc-early-math-add-sub-100</td>\n",
              "      <td>226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4185</th>\n",
              "      <td>- Let's say that I have a circle. My best att...</td>\n",
              "      <td>math&gt;&gt;engageny-geo&gt;&gt;geo-5</td>\n",
              "      <td>240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4186</th>\n",
              "      <td>- So let's look at the female reproductive cy...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "      <td>497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4187</th>\n",
              "      <td>-  We're now in the home stretch. We just hav...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;greens-theorem-a...</td>\n",
              "      <td>352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4188 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts  ... label\n",
              "0      In the last couple of videos we saw that we c...  ...   354\n",
              "1      -  What we're going to do in this video is gi...  ...   422\n",
              "2      So once again, we have three equal, or we say...  ...   384\n",
              "3      -  Liz's math test included a survey question...  ...   231\n",
              "4      - The following two equations form a linear s...  ...    99\n",
              "...                                                 ...  ...   ...\n",
              "4183   -  Hello everyone. So this is what I might ca...  ...   354\n",
              "4184   -  Let's try now to subtract some two-digit n...  ...   226\n",
              "4185   - Let's say that I have a circle. My best att...  ...   240\n",
              "4186   - So let's look at the female reproductive cy...  ...   497\n",
              "4187   -  We're now in the home stretch. We just hav...  ...   352\n",
              "\n",
              "[4188 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv66iiNHYcDP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "b341b22c-1de1-4909-efa4-c495feb8e15d"
      },
      "source": [
        "# LE_test = LabelEncoder()\n",
        "\n",
        "test_data['label'] = LE.transform(test_data['hierarchy'])\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-  What I hope to do in this video is get fam...</td>\n",
              "      <td>math&gt;&gt;math1&gt;&gt;x89d82521517266d4:functions</td>\n",
              "      <td>335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In the last video we were able to set up this...</td>\n",
              "      <td>math&gt;&gt;old-ap-calculus-ab&gt;&gt;ab-applications-defi...</td>\n",
              "      <td>357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-  In previous videos we talk about GDP as th...</td>\n",
              "      <td>economics-finance-domain&gt;&gt;ap-macroeconomics&gt;&gt;e...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  So what we're gonna do in this video is se...</td>\n",
              "      <td>math&gt;&gt;old-integral-calculus&gt;&gt;definite-integral...</td>\n",
              "      <td>378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-  So I've said that if you have a vector fie...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   video_transcripts  ... label\n",
              "0   -  What I hope to do in this video is get fam...  ...   335\n",
              "1   In the last video we were able to set up this...  ...   357\n",
              "2   -  In previous videos we talk about GDP as th...  ...     3\n",
              "3   -  So what we're gonna do in this video is se...  ...   378\n",
              "4   -  So I've said that if you have a vector fie...  ...   354\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Cvydgc-oAe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "30c63746-1d2f-4428-c5d8-de4a70d4d2b8"
      },
      "source": [
        "val_data['label'] = LE.transform(val_data['hierarchy'])\n",
        "val_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Find the probability of rolling doubles on tw...</td>\n",
              "      <td>math&gt;&gt;precalculus&gt;&gt;x9e81a4f98389efdf:prob-comb</td>\n",
              "      <td>395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>After the food is swallowed, it leaves the m...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "      <td>497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Let's now talk about what is easily one of th...</td>\n",
              "      <td>math&gt;&gt;geometry&gt;&gt;hs-geo-trig</td>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The goal in this video is to essentially prov...</td>\n",
              "      <td>science&gt;&gt;chemistry&gt;&gt;thermodynamics-chemistry</td>\n",
              "      <td>472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A line goes through the points (-1, 6) and (5...</td>\n",
              "      <td>math&gt;&gt;in-in-grade-11-ncert&gt;&gt;in-in-class11-stra...</td>\n",
              "      <td>304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   video_transcripts  ... label\n",
              "0   Find the probability of rolling doubles on tw...  ...   395\n",
              "1    After the food is swallowed, it leaves the m...  ...   497\n",
              "2   Let's now talk about what is easily one of th...  ...   256\n",
              "3   The goal in this video is to essentially prov...  ...   472\n",
              "4   A line goes through the points (-1, 6) and (5...  ...   304\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqTUkuPo3khA"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_data[\"video_transcripts\"],test_data[\"video_transcripts\"],train_data[\"label\"],test_data[\"label\"]\n",
        "val_features,val_labels = val_data[\"video_transcripts\"], val_data[\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prM_km_83khD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2ddbce-790d-47c9-ab8f-6cd3279331c1"
      },
      "source": [
        "train_labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "489    99\n",
              "497    65\n",
              "505    55\n",
              "488    54\n",
              "498    52\n",
              "       ..\n",
              "382     1\n",
              "359     1\n",
              "195     1\n",
              "471     1\n",
              "216     1\n",
              "Name: label, Length: 569, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfhPstXJ03oz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa86095-67f8-46cc-8f63-c712751909bd"
      },
      "source": [
        "test_labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "497    24\n",
              "489    22\n",
              "488    17\n",
              "93     11\n",
              "485    11\n",
              "       ..\n",
              "192     1\n",
              "191     1\n",
              "398     1\n",
              "187     1\n",
              "291     1\n",
              "Name: label, Length: 416, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "teV3IYQbLIks",
        "outputId": "236b7c8b-31be-4300-aabc-111a819fb82c"
      },
      "source": [
        "get_labels(268)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math>>in-in-class-3rd-math-cbse>>x80b2f4aa70819288:represent-and-interpret-data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkyM7gqv3khI"
      },
      "source": [
        "question_answer = train_features.values\n",
        "categories = train_labels.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFkS_H_83khL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9fc47b-f1fb-482c-b437-608e70b811de"
      },
      "source": [
        "question_answer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\" In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\",\n",
              "       \" -  What we're going to do in this video is give ourselves a little bit of a tour of eukaryotic cells. And the first place to start is just to remind ourselves what it means for a cell to be eukaryotic. It means that inside the cell, there are membrane-bound organelles. Now, what does that mean? Well, you could view it as sub-compartments within the cell. Membrane-bound organelles. And in this video in particular, we're going to highlight some of these membrane-bound organelles that make the cells eukaryotic. So let's just start with some of the ingredients that we know is true of all cells. So you'll have your cellular membrane here. I drew it big, so that we have a lot of space to draw things in. So this is our cellular membrane. I'll do some nice shading so you appreciate that it'll actually be three-dimensional. We see so many slices of cells that sometimes we forget that they are more spherical, or that they have three-dimensional shape to them. They're not all spherical. They can have different shapes. Now all cells, and there are some exceptions that we've talked about in previous videos. I should say, most cells will have some genetic information in them in the form of DNA. So that is our DNA, right over there. Now, one of the key characteristics of a eukaryotic cell is that the genetic information is going to be inside a membrane-bound organelle. And that membrane-bound organelle, or the membrane that surrounds the DNA here, that is the nuclear membrane. So let me draw the nuclear membrane right over here, and I'll put some shading in to appreciate that that also is going to be in three dimensions, around the DNA. So that is the first membrane-bound organelle that we're gonna discuss, the nucleus. Now the nucleus, it turns out, is connected to another membrane-bound organelle. And we're gonna study this in future videos, but right here I'm drawing holes or pores in the nuclear membrane. And those pores connect to something, it's a very fancy word called the endoplasmic reticulum. And the endoplasmic reticulum is essentially these layers of these membranes. So I'm gonna do my best job at trying to draw an endoplasmic reticulum. Imagine extending from these pores, going into a space that has really these layered membranes that have a lot of surface area. And I'm not gonna go all the way around this nucleus, but in many cells it will go around, all the way around the nucleus. And this right over here, and this is just a rough diagram. That is our endoplasmic, endoplasmic... Not blasmic, endoplasmic... Endoplasmic reticulum, which I've mentioned in previous videos would be an excellent name for a band. And what goes on in the endoplasmic reticulum is when you are in the process of taking that genetic information from DNA, and as we talk about in other videos it gets transcribed into mRNA. So that mRNA is now containing that information. That mRNA will make its way out of that nuclear membrane through one of these pores, and then make its way to a ribosome that is attached to the membrane of the endoplasmic reticulum. And so that's a ribosome there. I'm gonna do a bunch of ribosomes. And so as we've talked about in previous videos, ribosomes are really where you take that genetic information from that mRNA, and then you translate it into a protein. So the ribosomes are the protein synthesis, so let me label that. So this right over here is a ribosome. And some ribosomes might be attached to the endoplasmic reticulum. Some of them might just be floating out here in the cytoplasm, so that would be a free ribosome. Free ribosome. And even from the point of view of the endoplasmic reticulum, the parts of the endoplasmic reticulum where you have ribosomes attached, this is known as rough endoplasmic reticulum. It's the ribosomes that are making them rough. It looks that way in a microscope. So I'll just say rough ER, for endoplasmic reticulum for short. And then you also have parts of the endoplasmic reticulum where you do not have ribosomes attached. And because that looks smooth through our microscope, it has been called, you can imagine, smooth endoplasmic reticulum. There are things known as golgi bodies. Once again, another fascinating name. You gotta love these names in biology. That look kind of like an endoplasmic reticulum, but detached from the nuclear membrane. So let's say it's something like that. That's my best drawing there. That's a golgi body. And these are really good at packaging molecules, even proteins that might've just been produced, and packaging them so that they can be used outside of the cell, for example. And we'll go into detail in other videos, where a protein might go to the golgi body, get a little envelope around it, get some little processing going on, and then make its way outside of a cell. Now another, and this is maybe one of the most famous membrane-bound organelles outside of the nucleus, is what's known as the powerhouse of the cell, and that is the mitochondria. So I'll draw this mitochondria in magenta, because that's a nice powerful color. So mitochondria. And I love mitochondria because it's fascinating how they even came to be. Mitochondria actually have their own DNA, and all of your mitochondrial DNA comes from your mother. So that's actually very interesting for tracing maternal lineage. But mitochondria, this is where your, I'm gonna say let's see what we could see inside of this. This is where you ATP is produced. This is your mitochondria. It's really the powerhouse of the cell. What's interesting about mitochondria is evolutionary biologists believe that the ancestors of mitochondria, because mitochondria have their own DNA, they might've been independent organisms, independent cells. And at some point in our evolutionary past, they started living in symbiosis inside of what would be the ancestors of our cells. And over time, they became so codependent that they started to replicate together. And mitochondria, in fact, became part of these eukaryotic cells. Now if this eukaryotic cell was a plant cell or maybe an algae cell, you would have something called chloroplasts there. We don't have them because we don't have photosynthesis, but this is a chloroplast. And if you could see inside, you could see the little thylakoid stacks right over here. You could see the thylakoids if you could see inside. And so this right over here is a chloroplast. Chloroplast. And this would be plants and algae. Animals do not have these. And these are where you have your photosynthesis take place. Photosynthesis. Now there's also some membrane-bound organelles that are maybe less famous than the mitochondria or the chloroplast, or for sure the nucleus, and that might be something like a vacuole. And in plants, vacuoles tend to be very big. I could draw it, this is three-dimensional so I'll draw it on top of something that I've drawn before. So if a vacuole right over here, this is a... And in a plant it could be a fairly significant compartment inside. And in fact, it can even give structure to the plant itself because it is so big. And it contains water and enzymes. It's viewed as kind of a storage compartment. But it can also contain enzymes that help digest things, that help break things down so that they can be used in some way. So that is a vacuole. And they don't just exist in plants. They can also exist in animal cells. But in plant cells, they can be very, very, very visible. Now, something that is somewhat related to some of the function that a vacuole plays, that are most associated with animal cells but now there's evidence that they also exist in plant cells, is the idea of a lysosome. So a lysosome right over here, that also is a compartment. And it's going to contain a whole series of enzymes in it that is useful for lysing, you could say, that is useful for breaking down either waste products as the cell lives, or even foreign substances that might not be helpful for the cells. So it's gonna contain a bunch of enzymes, and it helps break down things. Now, I'll leave you there. These aren't all of the structures in eukaryotic cells, but these are enough of the structures so that you can appreciate that there are a lot of membrane-bound organelles in eukaryotic cells. And to be clear, even if I were to show all of the membrane-bound structures, that's not all the complexity of the cell. The big thing to appreciate is that cells are incredibly complex. There's all sorts of structures in here that help transport things and move things around. If you could shrink yourself down and look inside of a cell, it would look more complex than the most complex cities. There's all sorts of activities, things being moved around, shuttled around. The cell itself is replicating and copying things. And so this is just the beginning. We're just starting to scratch the surface of the complexity of the most basic unit of life.\",\n",
              "       \" So once again, we have three equal, or we say three identical objects. They all have the same mass, but we don't know what the mass is of each of them. But what we do know is that if you total up their mass, it's the same exact mass as these nine objects right over here. And each of these nine objects have a mass of 1 kilograms. So in total, you have 9 kilograms on this side. And over here, you have three objects. They all have the same mass. And we don't know what it is. We're just calling that mass x. And what I want to do here is try to tackle this a little bit more symbolically. In the last video, we said, hey, why don't we just multiply 1/3 of this and multiply 1/3 of this? And then, essentially, we're going to keep things balanced, because we're taking 1/3 of the same mass. This total is the same as this total. That's why the scale is balanced. Now, let's think about how we can represent this symbolically. So the first thing I want you to think about is, can we set up an equation that expresses that we have these three things of mass x, and that in total, their mass is equal to the total mass over here? Can we express that as an equation? And I'll give you a few seconds to do it. Well, let's think about it. Over here, we have three things with mass x. So their total mass, we could write as-- we could write their total mass as x plus x plus x. And over here, we have nine things with mass of 1 kilogram. I guess we could write 1 plus 1 plus 1. That's 3. Plus 1 plus 1 plus 1 plus 1. How many is that? 1, 2, 3, 4, 5, 6, 7, 8, 9. And actually, this is a mathematical representation. If we set it up as an equation, it's an algebraic representation. It's not the simplest possible way we can do it, but it is a reasonable way to do it. If we want, we can say, well, if I have an x plus another x plus another x, I have three x's. So I could rewrite this as 3x. And 3x will be equal to? Well, if I sum up all of these 1's right over here-- 1 plus 1 plus 1. We're doing that. We have 9 of them, so we get 3x is equal to 9. And let me make sure I did that. 1, 2, 3, 4, 5, 6, 7, 8, 9. So that's how we would set it up. And so the next question is, what would we do? What can we do mathematically? Actually, to either one of these equations, but we'll focus on this one right now. What can we do mathematically in order to essentially solve for the x? In order to figure out what that mystery mass actually is? And I'll give you another second or two to think about it. Well, when we did it the last time with just the scales we said, OK, we've got three of these x's here. We want to have just one x here. So we can say, whatever this x is, if the scale stays balanced, it's going to be the same as whatever we have there. There might be a temptation to subtract two of the x's maybe from this side, but that won't help us. And we can even see it mathematically over here. If we subtract two x's from both sides, on the left-hand side you're going to have 3x minus 2x. And on the right-hand side, you're going to have 9 minus 2x. And you're just going to be left with 3 of something minus 2 something is just 1 of something. So you will just have an x there if you get rid of two of them. But on the right-hand side, you're going to get 9 minus 2 x's. So the x's still didn't help you out. You still have a mystery mass on the right-hand side. So that doesn't help. So instead, what we say is-- and we did this the last time. We said, well, what if we took 1/3 of these things? If we take 1/3 of these things and take 1/3 of these things, we should still get the same mass on both sides because the original things had the same mass. And the equivalent of doing that mathematically is to say, why don't we multiply both sides by 1/3? Or another way to say it is we could divide both sides by 3. Multiplying by 1/3 is the same thing as dividing by 3. So we're going to multiply both sides by 1/3. When you multiply both sides by 1/3-- visually over here, if you had three x's, you multiply it by 1/3, you're only going to have one x left. If you have nine of these one-kilogram boxes, you multiply it by 1/3, you're only going to have three left. And over here, you can even visually-- if you divide by 3, which is the same thing as multiplying by 1/3, you divide by 3. So you divide by 3. You have an x is equal to a 1 plus 1 plus 1. An x is equal to 3. Or you see here, an x is equal to 3. Over here you do the math. 1/3 times 3 is 1. You're left with 1x. So you're left with x is equal to 9 times 1/3. Or you could even view it as 9 divided by 3, which is equal to 3.\",\n",
              "       ...,\n",
              "       ' - Let\\'s say that I have a circle. My best attempt to draw a reasonably perfect circle. So, there you go, not too bad, it\\'s a little bit of a hairy circle but you get the idea. So, this is a circle, this is the center of the circle, and let\\'s say that I have an arc along this circle. So, I\\'ll do the arc in green. So, I have an arc that is part of the circle, and it subtends an angle, so that\\'s my arc. Right over there, and it subtends an angle, and the angle that it subtends, so what I mean subtends, you take each of the endpoints of the arc, go to the center of the circle, go to the center of the circle just like this, and so it subtends angle theta, right over here, so it subtends angle theta, and let\\'s say that we know that angle theta is equal to two radians. So my question to you is what fraction of the entire circumference is this green arc? What fraction of the entire circumference is this green arc? And like always, pause the video, and give it a go. (laughs) All right, so let\\'s think through it a little bit. So, you might say well how do I know that, I don\\'t know what the radius of this thing is, I don\\'t, how do I think through this? And we just have to remind ourselves what radians mean, what radians mean. If an arc subtends the angle of two radians, that means that the arc itself is two \"radiuseseses\" long. (laughs) So, this right over here, let me make this a little clearer, so this, if the radius is r, if this radius is, I already used that color, if this radius... I have trouble switching colors (laughs) all right. If this radius is length r, then the length, if this angle is two radians, then the arc that subtends it is going to be two radiuses long, so this length right over here, is two radiuses. Now, what fraction of the entire circumference is that? Well, the entire circumference, we know, we know this from basic geometry, the entire circumference is two pi times the radius, or you can say it\\'s two pi radii, two pi \"radiuseses\", (laughs) two pi radii is the correct way to say it. So, what fraction is it? It\\'s two radii, it\\'s two radii, over two pi radii, over two pi radii, twos cancel out, rs cancel out, and so it is one \"pith\", (laughs) I guess you could say, it is one over pi of the total circumference.',\n",
              "       \" - So let's look at the female reproductive cycle. The female reproductive cycle refers to the maturation of eggs within the ovaries. The ovaries initially created these eggs during gestation. In other words, when a baby girl is in her mother's womb, the baby girl's entire egg supply will be created but will remain in an inactive state. This process of egg creation is called oogenesis. Then, once she grows up a bit and reaches puberty, her reproductive cycle will start, and one egg in that egg supply in her ovaries will mature or become activated each month, and that allows it to be fertilized by sperm. By the way, another word for egg is oocyte. After an egg matures, it's pushed out of the ovary in a process called ovulation. The other major function of the ovaries is to secrete the female sex hormones, estrogen, progesterone, and one called inhibin, and we'll talk about their functions a little bit later on. So let's first discuss how the eggs are made in the ovary in the first place. So early in uteral development, precursor germ cells, which are called oogonia, and those are homologous to spermatagonia in males, these oogonia undergo a ton of mitotic divisions to make more of themselves. And then, at about the 7th month of development, these divisions stop, and all of the ones that have been produced, which is actually about two to four million, are all she'll have for the rest of her life, and that turns out to be about one to two million per ovary. So while she's still in fetal development, all of these oogonia that have been produced, they all develop into the next stage, which is a primary oocyte. And just remember that the two oo's refers to egg, and the cyte, C-Y-T-E, refers to cell. So this just means egg cell, in case you were wondering. And let me also just mention, on a chromosomal level these oogonia, the germ cells, they're 2n, which means they have two copies of each chromosome, and the primary oocytes are the same. They're also 2n. And then these primary oocytes, they begin meiosis 1, and meiosis is what our germ cells use to reduce our chromosome copy number, and by that, I just mean the number of copies of DNA that we have. So they start this process of meiosis 1, but they don't actually finish it. They just kind of get about halfway through, and then they stop. So they're stuck as these big cells. So they're still primary oocytes, but they're said to be in meiotic arrest. So when the female who's been developing in her mom's womb, when she's born, her primary oocytes are in meiotic arrest. So the question is, do they stay like this? And the answer is, some do and some don't. Let's zoom in on this reproductive system to try to explain. So this is just a closeup of the major parts of the female reproductive system, and I've cut away parts of the uterus and the uterine tubes and the ovaries so you can see sort of the inside and the outsides of both structures. And this is our key organ here. This is the ovary. So the question was, do these primary oocytes that are stuck in meiotic arrest, do they stay like that? So the answer is that the ones that are sort of destined to be ovulated, that is, to be pushed out of the ovary right about here and then to be picked up by the fimbriae and then travel along the uterine tube here, those ones get past meiotic arrest. But most of them sort of die off while they're still stuck in that meiotic arrest phase as a primary oocyte. So I've mentioned the ones that sort of get out of the meiotic arrest phase and move on to develop into secondary oocytes that are able to then fuse with sperm, but when exactly does that happen? Well, it starts at puberty. So they actually stay in this phase as primary oocytes up here, in meiotic arrest for like 12 to 13 years, give or take, and only then do they start moving forward with development by finishing off that first part of meiosis that they started and splitting into two secondary oocytes. And actually, that's not exactly true, even though that's what we'd expect. What actually happens is one primary occyte it attempts to split into two secondary oocytes, but that's not exactly what happens. What does happen is that one of the developing daughter cells develops beautifully into a normal secondary oocyte from the primary oocyte, but it turns out that when they do complete the first part of meiosis, something really interesting happens. One of these cells receives basically all the cytoplasm. So the chromosome copy number is halved, but basically all the cytoplasm is kept in one cell. So this little guy over here that didn't get much cytoplasm, it still has a full complement of chromosomes, but it still ends up being pretty small and not really very functional. So it kind of withers away and dies, and it's called a polar body. So you end up with this really large secondary oocyte, and this is what ends up getting ovulated. And so now you might be thinking, well, meiosis is two steps, right? When does the second step happen? And that's a good question. So again, ovulation happens roughly here with the secondary oocyte coming out, and this secondary oocyte sort of just hangs out in the uterine tubes, and a sperm comes along and fertilizes the egg. So let's look at that down here. So you have your uterine tube here, and you have your egg. That's a secondary oocyte now. And then a sperm is coming along, and the sperm fuses with the egg after fertilizing it. And so the sperm fertilizes the egg and fuses with it. And so, let's just zoom in on what's happening there. So here you have your big secondary oocyte, and then you have your sperm that sort of, let's say that the nucleus of the sperm is right here. It's inside the egg already. This is the nucleus of the sperm. And here's the nucleus of the secondary oocyte. Well this is when meiosis 2 happens, so the second half of meiosis. So as this sperm nucleus is traveling toward the egg nucleus to create a joint nucleus, meiosis 2 occurs, and the oocyte reduces its chromosome copy number by creating another polar body, so a second polar body that kind of divides off the cell. So the oocyte cuts its chromosome copy number in half again, and so this little bit of DNA here that's just an extra copy of the DNA the egg already has, it divides off the cell in the form of another polar body that doesn't really have that much cytoplasm, just like the first one. So again, it leaves its nutrient-rich cytoplasm behind for the sperm and the egg. And by the way, the egg has changed its name now from a secondary oocyte to an ovum, but it won't be an ovum for long. Once the sperm nucleus fuses with the egg nucleus, then it becomes a zygote. So let me just clarify that if the egg doesn't get fertilized by a sperm that comes along, then it doesn't complete that second meiotic division that it did right here, and it just gets discharged from the body in menstruation as a secondary oocyte and not as an ovum, because the name ovum is reserved for the oocyte only once it's been fertilized. So those are the basic concepts behind what goes on with egg development.\",\n",
              "       \" -  We're now in the home stretch. We just have to evaluate the curl of f and then this dot product and then evaluate this double integral. So let's work on the curl of F. So the curl of f is going to be equal to, and I just remember it as the determit, so we have our i, j, k components, and it's really you could imagine it's the del operator crossed with the actual vector. So the del operator, I'll write this in a different color just to ease the monotony, so this is partial with respect to x, partial with respect to y, partial with respect to z, and then our vector field, I copied and pasted it right over here. It is just equal to negative y squared, is our i component, x is our j component, and Z squared is our k component. And so this is going to be equal to, this is going to be equal to i, is going to be equal to i times the partial of Z squared with respect to y. Well, there's the Z squared is just a constant with respect to y so the partial of Z squared with respect to y is just going to be zero, so this is going to be zero. Minus the partial of x with respect to z. Well, once again this is just a constant when you think in terms of z, so that's just going to be zero. So that's nice simplification, and then we're gonna have minus j, we need our little checkerboard patterns, we put a negative in front of the j, minus j and so we'll have the partial of x, the partial of z squared with respect to x, that's zero again, and then minus the partial of negative y squared with respect to z, well that's zero again, and then finally we have our k component, k, so plus, plus k, and k, we're gonna have the partial of x with respect to x, well that actually gives us a value that's just gonna be one minus the partial of negative y squared with respect to y. So the partial of negative y squared with respect to y is negative two y and we're subtracting that, so it's going to be plus, plus, two y. So curl of f simplifies to just, all of this is just zero up here, is just one plus two y times k or k times one plus two y. And so if we go back to this right up here, if we go back up to that, we're going to get let me re-write the integral so zero to one and that's our r, our r parameter is gonna go from zero to one, theta is gonna go from zero to two pi. And now curl of f has simplified to, and I won't skip any steps although it's tempting, it's one plus two y, and actually instead of writing two y, let me write it in terms of the parameters. We saw it up here, y was r sine theta, if I remember correctly, right, y was r sine theta. So let me write y that way. Two times r sine theta k. And we're gonna dot this, we're gonna take the dot product of that with this right over here, with r times j plus r times k, d theto d r. And so we take the dot product, this thing only has a k component, the j component is zero, so when you take the dot product with this j component you're gonna get zero. And neither of them you actually even have an i component. And so the inside is just going to simplify to this piece right over here is going to simplify to, we're just gonna have to think about the k components, cause everything else is zero, so it's gonna be r times this and we're done! So it's gonna be r plus two r squared sine theta, d theta d r, d theta d r and, once again, theta goes from zero to two pi and r goes from zero to one. And now this is just a straight-up double integral. We just have to evaluate this thing. And so, first we take the antiderivative with respect to theta, so the antiderivative with respect to theta is going to give us, so this is going to be giving, so we're going to focus on theta first, so the antiderivative of r with respect to theta is just r theta, you can just do r as a constant, and then the antiderivative of this, antiderivative of sine of theta is negative cosine of theta. So this is gonna be negative two r squared cosine of theta. And we're gonna evaluate it from zero to two pi. And then we have the outside integral, which I will, I'll re-color in yellow, re-color in yellow, so we'll still have to integrate with respect to r and r's gonna go from zero to one. But inside right over here, if we evaluate all of this business right over here at two pi, we get two pi r, two pi r, that's that right over there, minus... Cosine of two pi is just one. So it's minus two r squared and then from that, we're going to subtract from that, we're gonna subtract this evaluated zero. Well r times zero is just zero, and then cosine of zero is one. So it's just minus two r squared, or negative two r squared, negative two r squared. And at this negative and this negative, you get a positive, and but then you have a negative two r squared and then a plus two r squared it's just going to cancel out, that and that cancel out, and so this whole thing has simplified quite nicely to a simple definite integral, zero to one of two pi, two pi r dr, and the antiderivative of this is just going to be pi r squared, so we're just gonna evaluate pi r squared from zero to one, when you evaluate it at one, you get pi; when you evaluate it at zero, you just get zero, so you get pi minus zero, which is equal to, and now we deserve a drumroll 'cause we've been doing a lot of work over many videos, this is equal to pi. So just to remind ourselves what we've done over the last few videos, we had this line integral that we were trying to figure out, and instead of directly evaluating the line integral, which we could do and I encourage you to do so, and if I have time, I might do it in the next video, instead of directly evaluating that line integral, we used Stokes theorem to say, oh we could actually instead say that that's the same thing as a surface integral over a piecewise-smooth boundary over piecewise-smooth surface that this path is the boundary of, and so we evaluated this surface intergal and eventually, with a good bit of, little bit of calculation, we got to evaluating it to be equal to pi.\"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ian7gSDE3khR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1581a6a1-dcbd-4434-ef9d-dee4b57208a7"
      },
      "source": [
        "len(categories)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ZeuHc63khU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a4582c-584c-4348-a149-11f65b5b3e9b"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in question_answer:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', question_answer[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\n",
            "Token IDs: tensor([  101,  1999,  1996,  2197,  3232,  1997,  6876,  2057,  2387,  2008,\n",
            "         2057,  2064,  6235,  1037, 10543,  2011,  1037,  2597,  9207,  1011,\n",
            "        11126,  3853,  1012,  1998,  1999,  2200,  2236,  3408,  1010,  2009,\n",
            "         2052,  2022,  1996,  1060,  2597,  2004,  1037,  3853,  1997,  2051,\n",
            "         2335,  1996,  3131,  9207,  1999,  1996,  9876,  3257,  1012,  4606,\n",
            "         1996,  1061,  2597,  2004,  1037,  3853,  1997,  2051,  2335,  1996,\n",
            "         3131,  5125,  1999,  1996,  7471,  3257,  1012,  1998,  2023,  2097,\n",
            "         7687,  6235,  2023,  1011,  1011,  2295,  1010,  2065,  2017,  2064,\n",
            "         5674,  1037, 10811,  1998,  2292,  1005,  1055,  2360,  1996, 16381,\n",
            "         1056,  5836,  2051,  1012,  2009,  1005,  2222,  6235,  2073,  1996,\n",
            "        10811,  2003,  2012,  2151,  2445,  2051,  1012,  1998,  2065,  2057,\n",
            "         2359,  1037,  3327,  7774,  2057,  2064,  2360,  1010,  2092,  1010,\n",
            "         2023,  2069, 12033,  2005,  2070,  7774,  1011,  1011,  2057,  1005,\n",
            "         2128,  7149,  1010,  2009,  1005,  1055,  1054,  1997,  1056,  1012,\n",
            "         1998,  2009,  1005,  1055,  2069, 12711,  2090,  1056,  2108,  3618,\n",
            "         2084,  1037,  1998,  2625,  2084,  1038,  1012,  1998,  2017,  2113,\n",
            "         1010,  2008,  2052,  6235,  2070,  7774,  1999,  2048,  9646,  1012,\n",
            "         2074,  2033,  2074,  4009,  2009,  2182,  1012,  2023,  2003,  2035,\n",
            "         1037,  3319,  1997,  2428,  1010,  1996,  2197,  2048,  6876,  1012,\n",
            "         2061,  2023,  7774,  1010,  2009,  2453,  2298,  2242,  2066,  2008,\n",
            "         2073,  2023,  2003,  2073,  1056,  2003,  5020,  2000,  1037,  1012,\n",
            "         2008,  1005,  1055,  2073,  1056,  2003,  5020,  2000,  1038,  1012,\n",
            "         1998,  2061,  1054,  1997,  1037,  2097,  2022,  2023,  9207,  2157,\n",
            "         2182,  2008,  4515,  2012,  2008,  2391,  1012,  1998,  2059,  2004,\n",
            "         1056,  2030,  2065,  2017,  2064,  5674,  1996, 16381,  2108,  2051,\n",
            "         1010,  2009,  2987,  1005,  1056,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1mg7WyWKi6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be12576-8c03-4af8-dc9f-10e0faba6299"
      },
      "source": [
        "input_ids_val = []\n",
        "attention_masks_val = []\n",
        "\n",
        "for sent in val_features:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids_val.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks_val.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
        "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', question_answer[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\n",
            "Token IDs: tensor([  101,  1999,  1996,  2197,  3232,  1997,  6876,  2057,  2387,  2008,\n",
            "         2057,  2064,  6235,  1037, 10543,  2011,  1037,  2597,  9207,  1011,\n",
            "        11126,  3853,  1012,  1998,  1999,  2200,  2236,  3408,  1010,  2009,\n",
            "         2052,  2022,  1996,  1060,  2597,  2004,  1037,  3853,  1997,  2051,\n",
            "         2335,  1996,  3131,  9207,  1999,  1996,  9876,  3257,  1012,  4606,\n",
            "         1996,  1061,  2597,  2004,  1037,  3853,  1997,  2051,  2335,  1996,\n",
            "         3131,  5125,  1999,  1996,  7471,  3257,  1012,  1998,  2023,  2097,\n",
            "         7687,  6235,  2023,  1011,  1011,  2295,  1010,  2065,  2017,  2064,\n",
            "         5674,  1037, 10811,  1998,  2292,  1005,  1055,  2360,  1996, 16381,\n",
            "         1056,  5836,  2051,  1012,  2009,  1005,  2222,  6235,  2073,  1996,\n",
            "        10811,  2003,  2012,  2151,  2445,  2051,  1012,  1998,  2065,  2057,\n",
            "         2359,  1037,  3327,  7774,  2057,  2064,  2360,  1010,  2092,  1010,\n",
            "         2023,  2069, 12033,  2005,  2070,  7774,  1011,  1011,  2057,  1005,\n",
            "         2128,  7149,  1010,  2009,  1005,  1055,  1054,  1997,  1056,  1012,\n",
            "         1998,  2009,  1005,  1055,  2069, 12711,  2090,  1056,  2108,  3618,\n",
            "         2084,  1037,  1998,  2625,  2084,  1038,  1012,  1998,  2017,  2113,\n",
            "         1010,  2008,  2052,  6235,  2070,  7774,  1999,  2048,  9646,  1012,\n",
            "         2074,  2033,  2074,  4009,  2009,  2182,  1012,  2023,  2003,  2035,\n",
            "         1037,  3319,  1997,  2428,  1010,  1996,  2197,  2048,  6876,  1012,\n",
            "         2061,  2023,  7774,  1010,  2009,  2453,  2298,  2242,  2066,  2008,\n",
            "         2073,  2023,  2003,  2073,  1056,  2003,  5020,  2000,  1037,  1012,\n",
            "         2008,  1005,  1055,  2073,  1056,  2003,  5020,  2000,  1038,  1012,\n",
            "         1998,  2061,  1054,  1997,  1037,  2097,  2022,  2023,  9207,  2157,\n",
            "         2182,  2008,  4515,  2012,  2008,  2391,  1012,  1998,  2059,  2004,\n",
            "         1056,  2030,  2065,  2017,  2064,  5674,  1996, 16381,  2108,  2051,\n",
            "         1010,  2009,  2987,  1005,  1056,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVGvVZb13kha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9017d8c-de6f-4262-d200-d0b614c62984"
      },
      "source": [
        "print('Original: ', question_answer[1])\n",
        "print('Token IDs:', input_ids[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   -  What we're going to do in this video is give ourselves a little bit of a tour of eukaryotic cells. And the first place to start is just to remind ourselves what it means for a cell to be eukaryotic. It means that inside the cell, there are membrane-bound organelles. Now, what does that mean? Well, you could view it as sub-compartments within the cell. Membrane-bound organelles. And in this video in particular, we're going to highlight some of these membrane-bound organelles that make the cells eukaryotic. So let's just start with some of the ingredients that we know is true of all cells. So you'll have your cellular membrane here. I drew it big, so that we have a lot of space to draw things in. So this is our cellular membrane. I'll do some nice shading so you appreciate that it'll actually be three-dimensional. We see so many slices of cells that sometimes we forget that they are more spherical, or that they have three-dimensional shape to them. They're not all spherical. They can have different shapes. Now all cells, and there are some exceptions that we've talked about in previous videos. I should say, most cells will have some genetic information in them in the form of DNA. So that is our DNA, right over there. Now, one of the key characteristics of a eukaryotic cell is that the genetic information is going to be inside a membrane-bound organelle. And that membrane-bound organelle, or the membrane that surrounds the DNA here, that is the nuclear membrane. So let me draw the nuclear membrane right over here, and I'll put some shading in to appreciate that that also is going to be in three dimensions, around the DNA. So that is the first membrane-bound organelle that we're gonna discuss, the nucleus. Now the nucleus, it turns out, is connected to another membrane-bound organelle. And we're gonna study this in future videos, but right here I'm drawing holes or pores in the nuclear membrane. And those pores connect to something, it's a very fancy word called the endoplasmic reticulum. And the endoplasmic reticulum is essentially these layers of these membranes. So I'm gonna do my best job at trying to draw an endoplasmic reticulum. Imagine extending from these pores, going into a space that has really these layered membranes that have a lot of surface area. And I'm not gonna go all the way around this nucleus, but in many cells it will go around, all the way around the nucleus. And this right over here, and this is just a rough diagram. That is our endoplasmic, endoplasmic... Not blasmic, endoplasmic... Endoplasmic reticulum, which I've mentioned in previous videos would be an excellent name for a band. And what goes on in the endoplasmic reticulum is when you are in the process of taking that genetic information from DNA, and as we talk about in other videos it gets transcribed into mRNA. So that mRNA is now containing that information. That mRNA will make its way out of that nuclear membrane through one of these pores, and then make its way to a ribosome that is attached to the membrane of the endoplasmic reticulum. And so that's a ribosome there. I'm gonna do a bunch of ribosomes. And so as we've talked about in previous videos, ribosomes are really where you take that genetic information from that mRNA, and then you translate it into a protein. So the ribosomes are the protein synthesis, so let me label that. So this right over here is a ribosome. And some ribosomes might be attached to the endoplasmic reticulum. Some of them might just be floating out here in the cytoplasm, so that would be a free ribosome. Free ribosome. And even from the point of view of the endoplasmic reticulum, the parts of the endoplasmic reticulum where you have ribosomes attached, this is known as rough endoplasmic reticulum. It's the ribosomes that are making them rough. It looks that way in a microscope. So I'll just say rough ER, for endoplasmic reticulum for short. And then you also have parts of the endoplasmic reticulum where you do not have ribosomes attached. And because that looks smooth through our microscope, it has been called, you can imagine, smooth endoplasmic reticulum. There are things known as golgi bodies. Once again, another fascinating name. You gotta love these names in biology. That look kind of like an endoplasmic reticulum, but detached from the nuclear membrane. So let's say it's something like that. That's my best drawing there. That's a golgi body. And these are really good at packaging molecules, even proteins that might've just been produced, and packaging them so that they can be used outside of the cell, for example. And we'll go into detail in other videos, where a protein might go to the golgi body, get a little envelope around it, get some little processing going on, and then make its way outside of a cell. Now another, and this is maybe one of the most famous membrane-bound organelles outside of the nucleus, is what's known as the powerhouse of the cell, and that is the mitochondria. So I'll draw this mitochondria in magenta, because that's a nice powerful color. So mitochondria. And I love mitochondria because it's fascinating how they even came to be. Mitochondria actually have their own DNA, and all of your mitochondrial DNA comes from your mother. So that's actually very interesting for tracing maternal lineage. But mitochondria, this is where your, I'm gonna say let's see what we could see inside of this. This is where you ATP is produced. This is your mitochondria. It's really the powerhouse of the cell. What's interesting about mitochondria is evolutionary biologists believe that the ancestors of mitochondria, because mitochondria have their own DNA, they might've been independent organisms, independent cells. And at some point in our evolutionary past, they started living in symbiosis inside of what would be the ancestors of our cells. And over time, they became so codependent that they started to replicate together. And mitochondria, in fact, became part of these eukaryotic cells. Now if this eukaryotic cell was a plant cell or maybe an algae cell, you would have something called chloroplasts there. We don't have them because we don't have photosynthesis, but this is a chloroplast. And if you could see inside, you could see the little thylakoid stacks right over here. You could see the thylakoids if you could see inside. And so this right over here is a chloroplast. Chloroplast. And this would be plants and algae. Animals do not have these. And these are where you have your photosynthesis take place. Photosynthesis. Now there's also some membrane-bound organelles that are maybe less famous than the mitochondria or the chloroplast, or for sure the nucleus, and that might be something like a vacuole. And in plants, vacuoles tend to be very big. I could draw it, this is three-dimensional so I'll draw it on top of something that I've drawn before. So if a vacuole right over here, this is a... And in a plant it could be a fairly significant compartment inside. And in fact, it can even give structure to the plant itself because it is so big. And it contains water and enzymes. It's viewed as kind of a storage compartment. But it can also contain enzymes that help digest things, that help break things down so that they can be used in some way. So that is a vacuole. And they don't just exist in plants. They can also exist in animal cells. But in plant cells, they can be very, very, very visible. Now, something that is somewhat related to some of the function that a vacuole plays, that are most associated with animal cells but now there's evidence that they also exist in plant cells, is the idea of a lysosome. So a lysosome right over here, that also is a compartment. And it's going to contain a whole series of enzymes in it that is useful for lysing, you could say, that is useful for breaking down either waste products as the cell lives, or even foreign substances that might not be helpful for the cells. So it's gonna contain a bunch of enzymes, and it helps break down things. Now, I'll leave you there. These aren't all of the structures in eukaryotic cells, but these are enough of the structures so that you can appreciate that there are a lot of membrane-bound organelles in eukaryotic cells. And to be clear, even if I were to show all of the membrane-bound structures, that's not all the complexity of the cell. The big thing to appreciate is that cells are incredibly complex. There's all sorts of structures in here that help transport things and move things around. If you could shrink yourself down and look inside of a cell, it would look more complex than the most complex cities. There's all sorts of activities, things being moved around, shuttled around. The cell itself is replicating and copying things. And so this is just the beginning. We're just starting to scratch the surface of the complexity of the most basic unit of life.\n",
            "Token IDs: tensor([  101,  1011,  2054,  2057,  1005,  2128,  2183,  2000,  2079,  1999,\n",
            "         2023,  2678,  2003,  2507,  9731,  1037,  2210,  2978,  1997,  1037,\n",
            "         2778,  1997,  7327,  6673,  7677,  4588,  4442,  1012,  1998,  1996,\n",
            "         2034,  2173,  2000,  2707,  2003,  2074,  2000, 10825,  9731,  2054,\n",
            "         2009,  2965,  2005,  1037,  3526,  2000,  2022,  7327,  6673,  7677,\n",
            "         4588,  1012,  2009,  2965,  2008,  2503,  1996,  3526,  1010,  2045,\n",
            "         2024, 10804,  1011,  5391,  5812, 22869,  1012,  2085,  1010,  2054,\n",
            "         2515,  2008,  2812,  1029,  2092,  1010,  2017,  2071,  3193,  2009,\n",
            "         2004,  4942,  1011, 27998,  2306,  1996,  3526,  1012, 10804,  1011,\n",
            "         5391,  5812, 22869,  1012,  1998,  1999,  2023,  2678,  1999,  3327,\n",
            "         1010,  2057,  1005,  2128,  2183,  2000, 12944,  2070,  1997,  2122,\n",
            "        10804,  1011,  5391,  5812, 22869,  2008,  2191,  1996,  4442,  7327,\n",
            "         6673,  7677,  4588,  1012,  2061,  2292,  1005,  1055,  2074,  2707,\n",
            "         2007,  2070,  1997,  1996, 12760,  2008,  2057,  2113,  2003,  2995,\n",
            "         1997,  2035,  4442,  1012,  2061,  2017,  1005,  2222,  2031,  2115,\n",
            "        12562, 10804,  2182,  1012,  1045,  3881,  2009,  2502,  1010,  2061,\n",
            "         2008,  2057,  2031,  1037,  2843,  1997,  2686,  2000,  4009,  2477,\n",
            "         1999,  1012,  2061,  2023,  2003,  2256, 12562, 10804,  1012,  1045,\n",
            "         1005,  2222,  2079,  2070,  3835, 21146,  4667,  2061,  2017,  9120,\n",
            "         2008,  2009,  1005,  2222,  2941,  2022,  2093,  1011,  8789,  1012,\n",
            "         2057,  2156,  2061,  2116, 25609,  1997,  4442,  2008,  2823,  2057,\n",
            "         5293,  2008,  2027,  2024,  2062, 18970,  1010,  2030,  2008,  2027,\n",
            "         2031,  2093,  1011,  8789,  4338,  2000,  2068,  1012,  2027,  1005,\n",
            "         2128,  2025,  2035, 18970,  1012,  2027,  2064,  2031,  2367, 10466,\n",
            "         1012,  2085,  2035,  4442,  1010,  1998,  2045,  2024,  2070, 11790,\n",
            "         2008,  2057,  1005,  2310,  5720,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irMTimjf3khd"
      },
      "source": [
        "labels = torch.tensor(categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CICIofG9No7L",
        "outputId": "685e1294-908d-40b0-db1d-5e540a529673"
      },
      "source": [
        "get_labels(419)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'science>>ap-biology>>ecology-ap'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJJ0I8Ud3khf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d3258460-4953-428e-9289-a95eefd88c68"
      },
      "source": [
        "get_labels(311)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math>>in-in-grade-12-ncert>>in-in-determinants'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDW74Ny3khj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57e3037-5c00-48c7-ffbd-ec33e1073be3"
      },
      "source": [
        "num_classes = len(list(set(categories)))\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA2zYBezKi60"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n",
        "# train_poincare_tensor = torch.tensor(poincare_embeddings_final_train,dtype=torch.float)\n",
        "# val_poincare_tensor = torch.tensor(poincare_embeddings_final_val, dtype=torch.float)\n",
        "train_labels = torch.tensor(categories)\n",
        "val_labels = torch.tensor(val_labels.values)\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "train_dataset = TensorDataset(input_ids, attention_masks, train_labels)\n",
        "val_dataset = TensorDataset(input_ids_val,attention_masks_val,val_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo2jJuP1INTf"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_lTinod3kho"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SYosEkeZrpKF",
        "outputId": "10a9df97-d257-4aa3-fa28-23d41898e439"
      },
      "source": [
        "get_labels(571)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'science>>physics>>work-and-energy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS0czUKHTfQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2434966c-e971-4e8e-d9ff-4d2fc568a27f"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "# run this cell when training model\n",
        "# Loads BertForSequenceClassification, the pretrained BERT model with a single \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 572,   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=572, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2tmAMlw3khr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7569f3-623e-4747-8cbe-4c889ba7e025"
      },
      "source": [
        "\n",
        "# run this cell to prepare model for inference\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Loads BertForSequenceClassification, the pretrained BERT model with a single \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"model_save_categorized_reduced_QC\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 420,   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=420, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1E4ssLSpl4T"
      },
      "source": [
        "#freeze few layers for better training\n",
        "for param in model.bert.encoder.layer[0:6].parameters():\n",
        "    param.requires_grad=False\n",
        "for param in model.bert.embeddings.parameters():\n",
        "    param.requires_grad=False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awQ2Y9Jb3kht"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ys-M4-e3khv"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrYqErOD3khx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c724489-284f-4353-ff02-e995b61a8b5c"
      },
      "source": [
        "len(train_dataloader) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWVSE9LM3kh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9c8d89-26b7-4647-9ba0-d88716a52628"
      },
      "source": [
        "1935 * 32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61920"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT4e7q1bFC07"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcvxVVi63kh3"
      },
      "source": [
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUw3zm6g3kh5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta6zfUTa3kh7"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFq9gd5kQSHb"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsInxVoqbsFW"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LhAy2hZ3kh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0a33cd8-eb61-412b-ceac-f83a691d12c3"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "early_stopping = EarlyStopping(patience=2, verbose=True)\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    early_stopping(avg_val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break  \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "    output_dir = 'model_save_categorized_reduced_khan_acad/'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n",
        "\n",
        "    !rm -rf \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_save_categorized_reduced_khan_acad\"\n",
        "    !mv model_save_categorized_reduced_khan_acad \"/content/drive/My Drive/research_lo_content_taxonomy_classification/\"\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 30 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    131.    Elapsed: 0:00:46.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:14.\n",
            "\n",
            "  Average training loss: 6.25\n",
            "  Training epcoh took: 0:02:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.02\n",
            "Validation loss decreased (inf --> 6.087848).  Saving model ...\n",
            "  Validation Loss: 6.09\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 2 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 5.86\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.08\n",
            "Validation loss decreased (6.087848 --> 5.726779).  Saving model ...\n",
            "  Validation Loss: 5.73\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 3 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 5.50\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.10\n",
            "Validation loss decreased (5.726779 --> 5.415551).  Saving model ...\n",
            "  Validation Loss: 5.42\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 4 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 5.20\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.12\n",
            "Validation loss decreased (5.415551 --> 5.167103).  Saving model ...\n",
            "  Validation Loss: 5.17\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 5 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 4.94\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.15\n",
            "Validation loss decreased (5.167103 --> 4.940587).  Saving model ...\n",
            "  Validation Loss: 4.94\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 6 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 4.72\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.17\n",
            "Validation loss decreased (4.940587 --> 4.766076).  Saving model ...\n",
            "  Validation Loss: 4.77\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 7 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 4.52\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.18\n",
            "Validation loss decreased (4.766076 --> 4.619394).  Saving model ...\n",
            "  Validation Loss: 4.62\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 8 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 4.33\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "Validation loss decreased (4.619394 --> 4.488397).  Saving model ...\n",
            "  Validation Loss: 4.49\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 9 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 4.17\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.19\n",
            "Validation loss decreased (4.488397 --> 4.367824).  Saving model ...\n",
            "  Validation Loss: 4.37\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 10 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 4.03\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.21\n",
            "Validation loss decreased (4.367824 --> 4.268995).  Saving model ...\n",
            "  Validation Loss: 4.27\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 11 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.89\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.21\n",
            "Validation loss decreased (4.268995 --> 4.182886).  Saving model ...\n",
            "  Validation Loss: 4.18\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 12 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.77\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.23\n",
            "Validation loss decreased (4.182886 --> 4.103265).  Saving model ...\n",
            "  Validation Loss: 4.10\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 13 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.66\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.24\n",
            "Validation loss decreased (4.103265 --> 4.040951).  Saving model ...\n",
            "  Validation Loss: 4.04\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 14 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.55\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.25\n",
            "Validation loss decreased (4.040951 --> 3.979984).  Saving model ...\n",
            "  Validation Loss: 3.98\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 15 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.46\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.26\n",
            "Validation loss decreased (3.979984 --> 3.924088).  Saving model ...\n",
            "  Validation Loss: 3.92\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 16 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.38\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.26\n",
            "Validation loss decreased (3.924088 --> 3.869368).  Saving model ...\n",
            "  Validation Loss: 3.87\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 17 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.30\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.27\n",
            "Validation loss decreased (3.869368 --> 3.831113).  Saving model ...\n",
            "  Validation Loss: 3.83\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 18 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.22\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.27\n",
            "Validation loss decreased (3.831113 --> 3.790466).  Saving model ...\n",
            "  Validation Loss: 3.79\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 19 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.15\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "Validation loss decreased (3.790466 --> 3.749205).  Saving model ...\n",
            "  Validation Loss: 3.75\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 20 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.10\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.28\n",
            "Validation loss decreased (3.749205 --> 3.720948).  Saving model ...\n",
            "  Validation Loss: 3.72\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 21 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.05\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.29\n",
            "Validation loss decreased (3.720948 --> 3.695385).  Saving model ...\n",
            "  Validation Loss: 3.70\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 22 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 3.00\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.29\n",
            "Validation loss decreased (3.695385 --> 3.668060).  Saving model ...\n",
            "  Validation Loss: 3.67\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 23 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 2.95\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.29\n",
            "Validation loss decreased (3.668060 --> 3.646502).  Saving model ...\n",
            "  Validation Loss: 3.65\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 24 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 2.91\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.30\n",
            "Validation loss decreased (3.646502 --> 3.627740).  Saving model ...\n",
            "  Validation Loss: 3.63\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 25 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 2.89\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.30\n",
            "Validation loss decreased (3.627740 --> 3.611735).  Saving model ...\n",
            "  Validation Loss: 3.61\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 26 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 2.86\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.30\n",
            "Validation loss decreased (3.611735 --> 3.602254).  Saving model ...\n",
            "  Validation Loss: 3.60\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 27 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 2.83\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.30\n",
            "Validation loss decreased (3.602254 --> 3.590796).  Saving model ...\n",
            "  Validation Loss: 3.59\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 28 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 2.81\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.30\n",
            "Validation loss decreased (3.590796 --> 3.583625).  Saving model ...\n",
            "  Validation Loss: 3.58\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 29 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 2.80\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.30\n",
            "Validation loss decreased (3.583625 --> 3.580743).  Saving model ...\n",
            "  Validation Loss: 3.58\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "======== Epoch 30 / 30 ========\n",
            "Training...\n",
            "  Batch    40  of    131.    Elapsed: 0:00:45.\n",
            "  Batch    80  of    131.    Elapsed: 0:01:29.\n",
            "  Batch   120  of    131.    Elapsed: 0:02:13.\n",
            "\n",
            "  Average training loss: 2.79\n",
            "  Training epcoh took: 0:02:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.30\n",
            "Validation loss decreased (3.580743 --> 3.577659).  Saving model ...\n",
            "  Validation Loss: 3.58\n",
            "  Validation took: 0:00:18\n",
            "Saving model to model_save_categorized_reduced_khan_acad/\n",
            "\n",
            "Training complete!\n",
            "Total training took 1:23:18 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttvYWxutB7qM"
      },
      "source": [
        "# !mv model_save_categorized_reduced_freeze.zip \"/content/drive/My Drive/research_lo_content_taxonomy_classification/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RACcsko3kh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "outputId": "05f2c443-27fb-41d7-fa01-3a4abafcb56e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.25</td>\n",
              "      <td>6.09</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0:02:26</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.86</td>\n",
              "      <td>5.73</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.50</td>\n",
              "      <td>5.42</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.20</td>\n",
              "      <td>5.17</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.94</td>\n",
              "      <td>4.94</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.72</td>\n",
              "      <td>4.77</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4.52</td>\n",
              "      <td>4.62</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.33</td>\n",
              "      <td>4.49</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.17</td>\n",
              "      <td>4.37</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4.03</td>\n",
              "      <td>4.27</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3.89</td>\n",
              "      <td>4.18</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3.77</td>\n",
              "      <td>4.10</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3.66</td>\n",
              "      <td>4.04</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3.55</td>\n",
              "      <td>3.98</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3.46</td>\n",
              "      <td>3.92</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3.38</td>\n",
              "      <td>3.87</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3.30</td>\n",
              "      <td>3.83</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3.22</td>\n",
              "      <td>3.79</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3.15</td>\n",
              "      <td>3.75</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3.10</td>\n",
              "      <td>3.72</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3.05</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3.00</td>\n",
              "      <td>3.67</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2.95</td>\n",
              "      <td>3.65</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2.91</td>\n",
              "      <td>3.63</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2.89</td>\n",
              "      <td>3.61</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2.86</td>\n",
              "      <td>3.60</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2.83</td>\n",
              "      <td>3.59</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2.81</td>\n",
              "      <td>3.58</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2.80</td>\n",
              "      <td>3.58</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2.79</td>\n",
              "      <td>3.58</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0:02:25</td>\n",
              "      <td>0:00:18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               6.25         6.09           0.02       0:02:26         0:00:18\n",
              "2               5.86         5.73           0.08       0:02:25         0:00:18\n",
              "3               5.50         5.42           0.10       0:02:25         0:00:18\n",
              "4               5.20         5.17           0.12       0:02:25         0:00:18\n",
              "5               4.94         4.94           0.15       0:02:25         0:00:18\n",
              "6               4.72         4.77           0.17       0:02:25         0:00:18\n",
              "7               4.52         4.62           0.18       0:02:25         0:00:18\n",
              "8               4.33         4.49           0.19       0:02:25         0:00:18\n",
              "9               4.17         4.37           0.19       0:02:25         0:00:18\n",
              "10              4.03         4.27           0.21       0:02:25         0:00:18\n",
              "11              3.89         4.18           0.21       0:02:25         0:00:18\n",
              "12              3.77         4.10           0.23       0:02:25         0:00:18\n",
              "13              3.66         4.04           0.24       0:02:25         0:00:18\n",
              "14              3.55         3.98           0.25       0:02:25         0:00:18\n",
              "15              3.46         3.92           0.26       0:02:25         0:00:18\n",
              "16              3.38         3.87           0.26       0:02:25         0:00:18\n",
              "17              3.30         3.83           0.27       0:02:25         0:00:18\n",
              "18              3.22         3.79           0.27       0:02:25         0:00:18\n",
              "19              3.15         3.75           0.28       0:02:25         0:00:18\n",
              "20              3.10         3.72           0.28       0:02:25         0:00:18\n",
              "21              3.05         3.70           0.29       0:02:25         0:00:18\n",
              "22              3.00         3.67           0.29       0:02:25         0:00:18\n",
              "23              2.95         3.65           0.29       0:02:25         0:00:18\n",
              "24              2.91         3.63           0.30       0:02:25         0:00:18\n",
              "25              2.89         3.61           0.30       0:02:25         0:00:18\n",
              "26              2.86         3.60           0.30       0:02:25         0:00:18\n",
              "27              2.83         3.59           0.30       0:02:25         0:00:18\n",
              "28              2.81         3.58           0.30       0:02:25         0:00:18\n",
              "29              2.80         3.58           0.30       0:02:25         0:00:18\n",
              "30              2.79         3.58           0.30       0:02:25         0:00:18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5TicdiP3kiC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "00f146c0-8313-41da-b2d8-95f16c79f725"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbiTDpVv3kiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912d3597-6ef1-4506-8340-100d0b9cbd56"
      },
      "source": [
        "import os\n",
        "#no need to run during inference\n",
        "\n",
        "output_dir = 'model_save_categorized_reduced_khan_acad/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to model_save_categorized_reduced_khan_acad/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('model_save_categorized_reduced_khan_acad/vocab.txt',\n",
              " 'model_save_categorized_reduced_khan_acad/special_tokens_map.json',\n",
              " 'model_save_categorized_reduced_khan_acad/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U2UQ29a3kiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c587db-a020-4993-8626-849586586fd1"
      },
      "source": [
        "!pip install joblib\n",
        "import joblib\n",
        "joblib.dump(LE, \"label_encoder\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label_encoder']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpBqtI0wVe0i"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpGY8vSDI6u4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c626ca2-5f1c-48c8-fbb7-47e000048fe5"
      },
      "source": [
        "!zip -r model_save_categorized_reduced_khan_acad.zip model_save_categorized_reduced_khan_acad\n",
        "# files.download('model_save_categorized_reduced_freeze.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: model_save_categorized_reduced_khan_acad/ (stored 0%)\n",
            "  adding: model_save_categorized_reduced_khan_acad/tokenizer_config.json (stored 0%)\n",
            "  adding: model_save_categorized_reduced_khan_acad/pytorch_model.bin (deflated 7%)\n",
            "  adding: model_save_categorized_reduced_khan_acad/special_tokens_map.json (deflated 40%)\n",
            "  adding: model_save_categorized_reduced_khan_acad/config.json (deflated 78%)\n",
            "  adding: model_save_categorized_reduced_khan_acad/vocab.txt (deflated 53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvFDCDIxKDOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020ce06a-3844-49ba-8baa-414c6e63d2fe"
      },
      "source": [
        "!zip -r label_encoder_categorized_reduced_QC.zip label_encoder\n",
        "# files.download('label_encoder_categorized_reduced_round_2.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: label_encoder (deflated 71%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXAqaCWDNxK6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}