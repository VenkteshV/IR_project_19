{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IR_PROJECT_khan_acad_BERT_euclidean_baselines_inference .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c62b44584daa4f2e90367d282f48875f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2ec67418307543338ecad98c75063d41",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd7e4f6a3831414aa103b46ea9741b76",
              "IPY_MODEL_0cf994f5b12d4b469837c013dd5a6235"
            ]
          }
        },
        "2ec67418307543338ecad98c75063d41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd7e4f6a3831414aa103b46ea9741b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7c23583c19b340a28f1848daab31b14b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d011334125244696bc3f700f33ad3df2"
          }
        },
        "0cf994f5b12d4b469837c013dd5a6235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_87a1319166794985b8ba0de7bb446278",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 805kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9bb43dabea2240609d3c08fa0fdd6d69"
          }
        },
        "7c23583c19b340a28f1848daab31b14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d011334125244696bc3f700f33ad3df2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87a1319166794985b8ba0de7bb446278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9bb43dabea2240609d3c08fa0fdd6d69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR9av2JU3kf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9868dc-a1aa-4f2f-f183-2766738b8066"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzKeqoCs3kgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870f895c-692f-4bed-80c3-b7b35e22a4a0"
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 8.6MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/3d/386cc84db1e57aa7782eed00bcbdb884e496bdb1689c7f4c09a22572846d/boto3-1.17.35-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 18.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 18.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 58.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Collecting botocore<1.21.0,>=1.20.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/1d/e6ea5f2f856262415eb252b035fdb3524eeff4bb27864f7363a9bce5439f/botocore-1.20.35-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 54.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.35->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=e47871f9b25fc04cba75ea190f8ae0c62a43f62cca9fa9b3566da383f8085ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.35 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed boto3-1.17.35 botocore-1.20.35 jmespath-0.10.0 s3transfer-0.3.6 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mNG6fA-53zb",
        "outputId": "9400abca-25e8-43a0-8600-dcfe7c728db9"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (54.1.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.1)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, keras-applications, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4DAz3OrZkg4"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYhgMj0KZgsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08914b00-299b-4233-f225-fd3501691be7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfA2l9aK2_HC"
      },
      "source": [
        "!cp -r \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/model_save_categorized_reduced_khan_acad\" /content"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bogobkyTZmcV"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/train_khan_acad.csv\" /content\n",
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/test_khan_acad.csv\" /content\n",
        "!cp \"/content/drive/MyDrive/Information_retrieval_project/khan_acad/val_khan_acad.csv\" /content\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsADhaO93kgD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "0a5888fb-6597-496e-93c0-ed861ef78309"
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"train_khan_acad.csv\")\n",
        "test_data = pd.read_csv(\"test_khan_acad.csv\")\n",
        "val_data = pd.read_csv(\"val_khan_acad.csv\")\n",
        "train_data\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the last couple of videos we saw that we c...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-  What we're going to do in this video is gi...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;natural-selection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So once again, we have three equal, or we say...</td>\n",
              "      <td>math&gt;&gt;pre-algebra&gt;&gt;pre-algebra-equations-expre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  Liz's math test included a survey question...</td>\n",
              "      <td>math&gt;&gt;engageny-alg-1&gt;&gt;alg1-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>- The following two equations form a linear s...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-system-of-equations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4183</th>\n",
              "      <td>-  Hello everyone. So this is what I might ca...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4184</th>\n",
              "      <td>-  Let's try now to subtract some two-digit n...</td>\n",
              "      <td>math&gt;&gt;early-math&gt;&gt;cc-early-math-add-sub-100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4185</th>\n",
              "      <td>- Let's say that I have a circle. My best att...</td>\n",
              "      <td>math&gt;&gt;engageny-geo&gt;&gt;geo-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4186</th>\n",
              "      <td>- So let's look at the female reproductive cy...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4187</th>\n",
              "      <td>-  We're now in the home stretch. We just hav...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;greens-theorem-a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4188 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts                                          hierarchy\n",
              "0      In the last couple of videos we saw that we c...  math>>multivariable-calculus>>multivariable-de...\n",
              "1      -  What we're going to do in this video is gi...             science>>ap-biology>>natural-selection\n",
              "2      So once again, we have three equal, or we say...  math>>pre-algebra>>pre-algebra-equations-expre...\n",
              "3      -  Liz's math test included a survey question...                       math>>engageny-alg-1>>alg1-2\n",
              "4      - The following two equations form a linear s...        math>>algebra-home>>alg-system-of-equations\n",
              "...                                                 ...                                                ...\n",
              "4183   -  Hello everyone. So this is what I might ca...  math>>multivariable-calculus>>multivariable-de...\n",
              "4184   -  Let's try now to subtract some two-digit n...        math>>early-math>>cc-early-math-add-sub-100\n",
              "4185   - Let's say that I have a circle. My best att...                          math>>engageny-geo>>geo-5\n",
              "4186   - So let's look at the female reproductive cy...  science>>health-and-medicine>>human-anatomy-an...\n",
              "4187   -  We're now in the home stretch. We just hav...  math>>multivariable-calculus>>greens-theorem-a...\n",
              "\n",
              "[4188 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQhO6qqt6lge"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD4IAgG1XQGp"
      },
      "source": [
        "import re\n",
        "def clean_sentence(question):\n",
        "  # print(question)\n",
        "  question = re.sub('<[^>]*>', ' ',question)\n",
        "  question = re.sub(' +', ' ', question)\n",
        "  question = re.sub('\\xa0','',question)\n",
        "  question = question.rstrip()\n",
        "  question = re.sub('nan','',question)\n",
        "  question = re.sub(u'\\u2004','',question)\n",
        "  question = re.sub(u'\\u2009','',question)\n",
        "\n",
        "  # question = question.decode(\"utf-8\")\n",
        "  # question = question.replace(u'\\u200\\d*','').encode(\"utf-8\")\n",
        "  question = re.sub('&nbsp','',question)\n",
        "  question = re.sub('&ndash','',question)\n",
        "  question = re.sub('\\r','',question)\n",
        "  question = re.sub('\\t','',question)\n",
        "  question = re.sub('\\n',' ',question)\n",
        "\n",
        "  question = re.sub('MathType@.*','',question)\n",
        "  question = re.sub('&thinsp','',question)\n",
        "  question = re.sub('&times','',question)\n",
        "  question = re.sub('\\u200b','',question)\n",
        "  question = re.sub('&rarr;;;','',question)\n",
        "\n",
        "  return question"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWOeBaD23kga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c29e504-687a-4302-e73d-6e9dd5be3012"
      },
      "source": [
        "train_data[\"hierarchy\"].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "science>>health-and-medicine>>circulatory-system-diseases     99\n",
              "science>>health-and-medicine>>human-anatomy-and-physiology    65\n",
              "science>>health-and-medicine>>respiratory-system-diseases     55\n",
              "science>>health-and-medicine>>circulatory-system              54\n",
              "science>>health-and-medicine>>infectious-diseases             52\n",
              "                                                              ..\n",
              "math>>engageny-geo>>geo-3                                      1\n",
              "science>>ap-physics-1>>ap-one-dimensional-motion               1\n",
              "science>>ap-physics-1>>ap-forces-newtons-laws                  1\n",
              "math>>old-ap-calculus-ab>>ab-existence-theorems                1\n",
              "math>>old-integral-calculus>>riemann-sums-ic                   1\n",
              "Name: hierarchy, Length: 569, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNrGNk8f3kgh"
      },
      "source": [
        "# final_data_1 = final_data.loc[0:71003,:]\n",
        "# final_data_1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrS5sxE3kgk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315,
          "referenced_widgets": [
            "c62b44584daa4f2e90367d282f48875f",
            "2ec67418307543338ecad98c75063d41",
            "fd7e4f6a3831414aa103b46ea9741b76",
            "0cf994f5b12d4b469837c013dd5a6235",
            "7c23583c19b340a28f1848daab31b14b",
            "d011334125244696bc3f700f33ad3df2",
            "87a1319166794985b8ba0de7bb446278",
            "9bb43dabea2240609d3c08fa0fdd6d69"
          ]
        },
        "outputId": "fcef93b9-4854-46fd-8c7d-32f12fbc55eb"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c62b44584daa4f2e90367d282f48875f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mgc72PQYV1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e6a5fb-66af-4f8c-b78f-55863598d4bc"
      },
      "source": [
        "test_data[\"hierarchy\"].value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "science>>health-and-medicine>>human-anatomy-and-physiology    24\n",
              "science>>health-and-medicine>>circulatory-system-diseases     22\n",
              "science>>health-and-medicine>>circulatory-system              17\n",
              "math>>algebra-home>>alg-polynomials                           11\n",
              "science>>health-and-medicine>>infectious-diseases             11\n",
              "                                                              ..\n",
              "math>>engageny-alg-1>>alg1-1                                   1\n",
              "math>>precalculus>>x9e81a4f98389efdf:complex                   1\n",
              "science>>physics>>thermodynamics                               1\n",
              "math>>geometry-home>>geometry-coordinate-plane                 1\n",
              "math>>old-integral-calculus>>riemann-sums-ic                   1\n",
              "Name: hierarchy, Length: 416, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3lYOb2K3kgy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "3ba20e90-fbc7-49f5-a122-8c892542cc26"
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "LE = LabelEncoder()\n",
        "LE.fit_transform(pd.concat([train_data['hierarchy'],test_data['hierarchy']]))\n",
        "train_data['label'] = LE.transform(train_data['hierarchy'])\n",
        "train_data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the last couple of videos we saw that we c...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-  What we're going to do in this video is gi...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;natural-selection</td>\n",
              "      <td>422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So once again, we have three equal, or we say...</td>\n",
              "      <td>math&gt;&gt;pre-algebra&gt;&gt;pre-algebra-equations-expre...</td>\n",
              "      <td>384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  Liz's math test included a survey question...</td>\n",
              "      <td>math&gt;&gt;engageny-alg-1&gt;&gt;alg1-2</td>\n",
              "      <td>231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>- The following two equations form a linear s...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-system-of-equations</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   video_transcripts  ... label\n",
              "0   In the last couple of videos we saw that we c...  ...   354\n",
              "1   -  What we're going to do in this video is gi...  ...   422\n",
              "2   So once again, we have three equal, or we say...  ...   384\n",
              "3   -  Liz's math test included a survey question...  ...   231\n",
              "4   - The following two equations form a linear s...  ...    99\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp64MkNB3kg1"
      },
      "source": [
        "def get_labels(prediction):\n",
        "    predicted_label =  LE.inverse_transform([prediction])\n",
        "    return predicted_label[0]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPgTmJPS3kg4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7775bc91-2f6d-4c5f-fe64-8c2376b2230d"
      },
      "source": [
        "get_labels(204)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math>>cc-seventh-grade-math>>cc-7th-fractions-decimals'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_UpqLMG3kg9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b75b32b9-07f1-4f56-dbf0-eb5635109cbe"
      },
      "source": [
        "train_data.iloc[14,1]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'economics-finance-domain>>macroeconomics>>monetary-system-topic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5a3P7jSZZ6B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "99368072-0380-4b09-aa53-12ab64252472"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In the last couple of videos we saw that we c...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-  What we're going to do in this video is gi...</td>\n",
              "      <td>science&gt;&gt;ap-biology&gt;&gt;natural-selection</td>\n",
              "      <td>422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So once again, we have three equal, or we say...</td>\n",
              "      <td>math&gt;&gt;pre-algebra&gt;&gt;pre-algebra-equations-expre...</td>\n",
              "      <td>384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  Liz's math test included a survey question...</td>\n",
              "      <td>math&gt;&gt;engageny-alg-1&gt;&gt;alg1-2</td>\n",
              "      <td>231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>- The following two equations form a linear s...</td>\n",
              "      <td>math&gt;&gt;algebra-home&gt;&gt;alg-system-of-equations</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4183</th>\n",
              "      <td>-  Hello everyone. So this is what I might ca...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4184</th>\n",
              "      <td>-  Let's try now to subtract some two-digit n...</td>\n",
              "      <td>math&gt;&gt;early-math&gt;&gt;cc-early-math-add-sub-100</td>\n",
              "      <td>226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4185</th>\n",
              "      <td>- Let's say that I have a circle. My best att...</td>\n",
              "      <td>math&gt;&gt;engageny-geo&gt;&gt;geo-5</td>\n",
              "      <td>240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4186</th>\n",
              "      <td>- So let's look at the female reproductive cy...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "      <td>497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4187</th>\n",
              "      <td>-  We're now in the home stretch. We just hav...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;greens-theorem-a...</td>\n",
              "      <td>352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4188 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      video_transcripts  ... label\n",
              "0      In the last couple of videos we saw that we c...  ...   354\n",
              "1      -  What we're going to do in this video is gi...  ...   422\n",
              "2      So once again, we have three equal, or we say...  ...   384\n",
              "3      -  Liz's math test included a survey question...  ...   231\n",
              "4      - The following two equations form a linear s...  ...    99\n",
              "...                                                 ...  ...   ...\n",
              "4183   -  Hello everyone. So this is what I might ca...  ...   354\n",
              "4184   -  Let's try now to subtract some two-digit n...  ...   226\n",
              "4185   - Let's say that I have a circle. My best att...  ...   240\n",
              "4186   - So let's look at the female reproductive cy...  ...   497\n",
              "4187   -  We're now in the home stretch. We just hav...  ...   352\n",
              "\n",
              "[4188 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv66iiNHYcDP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "26cc3cd3-b772-4e9d-d2bd-48d05ff5141e"
      },
      "source": [
        "# LE_test = LabelEncoder()\n",
        "\n",
        "test_data['label'] = LE.transform(test_data['hierarchy'])\n",
        "test_data.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-  What I hope to do in this video is get fam...</td>\n",
              "      <td>math&gt;&gt;math1&gt;&gt;x89d82521517266d4:functions</td>\n",
              "      <td>335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In the last video we were able to set up this...</td>\n",
              "      <td>math&gt;&gt;old-ap-calculus-ab&gt;&gt;ab-applications-defi...</td>\n",
              "      <td>357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-  In previous videos we talk about GDP as th...</td>\n",
              "      <td>economics-finance-domain&gt;&gt;ap-macroeconomics&gt;&gt;e...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-  So what we're gonna do in this video is se...</td>\n",
              "      <td>math&gt;&gt;old-integral-calculus&gt;&gt;definite-integral...</td>\n",
              "      <td>378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-  So I've said that if you have a vector fie...</td>\n",
              "      <td>math&gt;&gt;multivariable-calculus&gt;&gt;multivariable-de...</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   video_transcripts  ... label\n",
              "0   -  What I hope to do in this video is get fam...  ...   335\n",
              "1   In the last video we were able to set up this...  ...   357\n",
              "2   -  In previous videos we talk about GDP as th...  ...     3\n",
              "3   -  So what we're gonna do in this video is se...  ...   378\n",
              "4   -  So I've said that if you have a vector fie...  ...   354\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Cvydgc-oAe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "59d33353-3839-48c4-8573-bda3e548613f"
      },
      "source": [
        "val_data['label'] = LE.transform(val_data['hierarchy'])\n",
        "val_data.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_transcripts</th>\n",
              "      <th>hierarchy</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Find the probability of rolling doubles on tw...</td>\n",
              "      <td>math&gt;&gt;precalculus&gt;&gt;x9e81a4f98389efdf:prob-comb</td>\n",
              "      <td>395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>After the food is swallowed, it leaves the m...</td>\n",
              "      <td>science&gt;&gt;health-and-medicine&gt;&gt;human-anatomy-an...</td>\n",
              "      <td>497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Let's now talk about what is easily one of th...</td>\n",
              "      <td>math&gt;&gt;geometry&gt;&gt;hs-geo-trig</td>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The goal in this video is to essentially prov...</td>\n",
              "      <td>science&gt;&gt;chemistry&gt;&gt;thermodynamics-chemistry</td>\n",
              "      <td>472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A line goes through the points (-1, 6) and (5...</td>\n",
              "      <td>math&gt;&gt;in-in-grade-11-ncert&gt;&gt;in-in-class11-stra...</td>\n",
              "      <td>304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   video_transcripts  ... label\n",
              "0   Find the probability of rolling doubles on tw...  ...   395\n",
              "1    After the food is swallowed, it leaves the m...  ...   497\n",
              "2   Let's now talk about what is easily one of th...  ...   256\n",
              "3   The goal in this video is to essentially prov...  ...   472\n",
              "4   A line goes through the points (-1, 6) and (5...  ...   304\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqTUkuPo3khA"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_data[\"video_transcripts\"],test_data[\"video_transcripts\"],train_data[\"label\"],test_data[\"label\"]\n",
        "val_features,val_labels = val_data[\"video_transcripts\"], val_data[\"label\"]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prM_km_83khD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019629b1-13be-419c-fe4f-e56e65bbbc84"
      },
      "source": [
        "train_labels.value_counts()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "489    99\n",
              "497    65\n",
              "505    55\n",
              "488    54\n",
              "498    52\n",
              "       ..\n",
              "382     1\n",
              "359     1\n",
              "195     1\n",
              "471     1\n",
              "216     1\n",
              "Name: label, Length: 569, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfhPstXJ03oz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0662206f-ea6d-42a2-db32-5a726e70b1c8"
      },
      "source": [
        "test_labels.value_counts()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "497    24\n",
              "489    22\n",
              "488    17\n",
              "93     11\n",
              "485    11\n",
              "       ..\n",
              "192     1\n",
              "191     1\n",
              "398     1\n",
              "187     1\n",
              "291     1\n",
              "Name: label, Length: 416, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "teV3IYQbLIks",
        "outputId": "0cd8b532-5eef-49c6-caaa-793cbb37e7ee"
      },
      "source": [
        "get_labels(268)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math>>in-in-class-3rd-math-cbse>>x80b2f4aa70819288:represent-and-interpret-data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkyM7gqv3khI"
      },
      "source": [
        "question_answer = train_features.values\n",
        "categories = train_labels.values"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFkS_H_83khL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c7a121-cd86-48e7-adbd-783325b22ca3"
      },
      "source": [
        "question_answer"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\" In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\",\n",
              "       \" -  What we're going to do in this video is give ourselves a little bit of a tour of eukaryotic cells. And the first place to start is just to remind ourselves what it means for a cell to be eukaryotic. It means that inside the cell, there are membrane-bound organelles. Now, what does that mean? Well, you could view it as sub-compartments within the cell. Membrane-bound organelles. And in this video in particular, we're going to highlight some of these membrane-bound organelles that make the cells eukaryotic. So let's just start with some of the ingredients that we know is true of all cells. So you'll have your cellular membrane here. I drew it big, so that we have a lot of space to draw things in. So this is our cellular membrane. I'll do some nice shading so you appreciate that it'll actually be three-dimensional. We see so many slices of cells that sometimes we forget that they are more spherical, or that they have three-dimensional shape to them. They're not all spherical. They can have different shapes. Now all cells, and there are some exceptions that we've talked about in previous videos. I should say, most cells will have some genetic information in them in the form of DNA. So that is our DNA, right over there. Now, one of the key characteristics of a eukaryotic cell is that the genetic information is going to be inside a membrane-bound organelle. And that membrane-bound organelle, or the membrane that surrounds the DNA here, that is the nuclear membrane. So let me draw the nuclear membrane right over here, and I'll put some shading in to appreciate that that also is going to be in three dimensions, around the DNA. So that is the first membrane-bound organelle that we're gonna discuss, the nucleus. Now the nucleus, it turns out, is connected to another membrane-bound organelle. And we're gonna study this in future videos, but right here I'm drawing holes or pores in the nuclear membrane. And those pores connect to something, it's a very fancy word called the endoplasmic reticulum. And the endoplasmic reticulum is essentially these layers of these membranes. So I'm gonna do my best job at trying to draw an endoplasmic reticulum. Imagine extending from these pores, going into a space that has really these layered membranes that have a lot of surface area. And I'm not gonna go all the way around this nucleus, but in many cells it will go around, all the way around the nucleus. And this right over here, and this is just a rough diagram. That is our endoplasmic, endoplasmic... Not blasmic, endoplasmic... Endoplasmic reticulum, which I've mentioned in previous videos would be an excellent name for a band. And what goes on in the endoplasmic reticulum is when you are in the process of taking that genetic information from DNA, and as we talk about in other videos it gets transcribed into mRNA. So that mRNA is now containing that information. That mRNA will make its way out of that nuclear membrane through one of these pores, and then make its way to a ribosome that is attached to the membrane of the endoplasmic reticulum. And so that's a ribosome there. I'm gonna do a bunch of ribosomes. And so as we've talked about in previous videos, ribosomes are really where you take that genetic information from that mRNA, and then you translate it into a protein. So the ribosomes are the protein synthesis, so let me label that. So this right over here is a ribosome. And some ribosomes might be attached to the endoplasmic reticulum. Some of them might just be floating out here in the cytoplasm, so that would be a free ribosome. Free ribosome. And even from the point of view of the endoplasmic reticulum, the parts of the endoplasmic reticulum where you have ribosomes attached, this is known as rough endoplasmic reticulum. It's the ribosomes that are making them rough. It looks that way in a microscope. So I'll just say rough ER, for endoplasmic reticulum for short. And then you also have parts of the endoplasmic reticulum where you do not have ribosomes attached. And because that looks smooth through our microscope, it has been called, you can imagine, smooth endoplasmic reticulum. There are things known as golgi bodies. Once again, another fascinating name. You gotta love these names in biology. That look kind of like an endoplasmic reticulum, but detached from the nuclear membrane. So let's say it's something like that. That's my best drawing there. That's a golgi body. And these are really good at packaging molecules, even proteins that might've just been produced, and packaging them so that they can be used outside of the cell, for example. And we'll go into detail in other videos, where a protein might go to the golgi body, get a little envelope around it, get some little processing going on, and then make its way outside of a cell. Now another, and this is maybe one of the most famous membrane-bound organelles outside of the nucleus, is what's known as the powerhouse of the cell, and that is the mitochondria. So I'll draw this mitochondria in magenta, because that's a nice powerful color. So mitochondria. And I love mitochondria because it's fascinating how they even came to be. Mitochondria actually have their own DNA, and all of your mitochondrial DNA comes from your mother. So that's actually very interesting for tracing maternal lineage. But mitochondria, this is where your, I'm gonna say let's see what we could see inside of this. This is where you ATP is produced. This is your mitochondria. It's really the powerhouse of the cell. What's interesting about mitochondria is evolutionary biologists believe that the ancestors of mitochondria, because mitochondria have their own DNA, they might've been independent organisms, independent cells. And at some point in our evolutionary past, they started living in symbiosis inside of what would be the ancestors of our cells. And over time, they became so codependent that they started to replicate together. And mitochondria, in fact, became part of these eukaryotic cells. Now if this eukaryotic cell was a plant cell or maybe an algae cell, you would have something called chloroplasts there. We don't have them because we don't have photosynthesis, but this is a chloroplast. And if you could see inside, you could see the little thylakoid stacks right over here. You could see the thylakoids if you could see inside. And so this right over here is a chloroplast. Chloroplast. And this would be plants and algae. Animals do not have these. And these are where you have your photosynthesis take place. Photosynthesis. Now there's also some membrane-bound organelles that are maybe less famous than the mitochondria or the chloroplast, or for sure the nucleus, and that might be something like a vacuole. And in plants, vacuoles tend to be very big. I could draw it, this is three-dimensional so I'll draw it on top of something that I've drawn before. So if a vacuole right over here, this is a... And in a plant it could be a fairly significant compartment inside. And in fact, it can even give structure to the plant itself because it is so big. And it contains water and enzymes. It's viewed as kind of a storage compartment. But it can also contain enzymes that help digest things, that help break things down so that they can be used in some way. So that is a vacuole. And they don't just exist in plants. They can also exist in animal cells. But in plant cells, they can be very, very, very visible. Now, something that is somewhat related to some of the function that a vacuole plays, that are most associated with animal cells but now there's evidence that they also exist in plant cells, is the idea of a lysosome. So a lysosome right over here, that also is a compartment. And it's going to contain a whole series of enzymes in it that is useful for lysing, you could say, that is useful for breaking down either waste products as the cell lives, or even foreign substances that might not be helpful for the cells. So it's gonna contain a bunch of enzymes, and it helps break down things. Now, I'll leave you there. These aren't all of the structures in eukaryotic cells, but these are enough of the structures so that you can appreciate that there are a lot of membrane-bound organelles in eukaryotic cells. And to be clear, even if I were to show all of the membrane-bound structures, that's not all the complexity of the cell. The big thing to appreciate is that cells are incredibly complex. There's all sorts of structures in here that help transport things and move things around. If you could shrink yourself down and look inside of a cell, it would look more complex than the most complex cities. There's all sorts of activities, things being moved around, shuttled around. The cell itself is replicating and copying things. And so this is just the beginning. We're just starting to scratch the surface of the complexity of the most basic unit of life.\",\n",
              "       \" So once again, we have three equal, or we say three identical objects. They all have the same mass, but we don't know what the mass is of each of them. But what we do know is that if you total up their mass, it's the same exact mass as these nine objects right over here. And each of these nine objects have a mass of 1 kilograms. So in total, you have 9 kilograms on this side. And over here, you have three objects. They all have the same mass. And we don't know what it is. We're just calling that mass x. And what I want to do here is try to tackle this a little bit more symbolically. In the last video, we said, hey, why don't we just multiply 1/3 of this and multiply 1/3 of this? And then, essentially, we're going to keep things balanced, because we're taking 1/3 of the same mass. This total is the same as this total. That's why the scale is balanced. Now, let's think about how we can represent this symbolically. So the first thing I want you to think about is, can we set up an equation that expresses that we have these three things of mass x, and that in total, their mass is equal to the total mass over here? Can we express that as an equation? And I'll give you a few seconds to do it. Well, let's think about it. Over here, we have three things with mass x. So their total mass, we could write as-- we could write their total mass as x plus x plus x. And over here, we have nine things with mass of 1 kilogram. I guess we could write 1 plus 1 plus 1. That's 3. Plus 1 plus 1 plus 1 plus 1. How many is that? 1, 2, 3, 4, 5, 6, 7, 8, 9. And actually, this is a mathematical representation. If we set it up as an equation, it's an algebraic representation. It's not the simplest possible way we can do it, but it is a reasonable way to do it. If we want, we can say, well, if I have an x plus another x plus another x, I have three x's. So I could rewrite this as 3x. And 3x will be equal to? Well, if I sum up all of these 1's right over here-- 1 plus 1 plus 1. We're doing that. We have 9 of them, so we get 3x is equal to 9. And let me make sure I did that. 1, 2, 3, 4, 5, 6, 7, 8, 9. So that's how we would set it up. And so the next question is, what would we do? What can we do mathematically? Actually, to either one of these equations, but we'll focus on this one right now. What can we do mathematically in order to essentially solve for the x? In order to figure out what that mystery mass actually is? And I'll give you another second or two to think about it. Well, when we did it the last time with just the scales we said, OK, we've got three of these x's here. We want to have just one x here. So we can say, whatever this x is, if the scale stays balanced, it's going to be the same as whatever we have there. There might be a temptation to subtract two of the x's maybe from this side, but that won't help us. And we can even see it mathematically over here. If we subtract two x's from both sides, on the left-hand side you're going to have 3x minus 2x. And on the right-hand side, you're going to have 9 minus 2x. And you're just going to be left with 3 of something minus 2 something is just 1 of something. So you will just have an x there if you get rid of two of them. But on the right-hand side, you're going to get 9 minus 2 x's. So the x's still didn't help you out. You still have a mystery mass on the right-hand side. So that doesn't help. So instead, what we say is-- and we did this the last time. We said, well, what if we took 1/3 of these things? If we take 1/3 of these things and take 1/3 of these things, we should still get the same mass on both sides because the original things had the same mass. And the equivalent of doing that mathematically is to say, why don't we multiply both sides by 1/3? Or another way to say it is we could divide both sides by 3. Multiplying by 1/3 is the same thing as dividing by 3. So we're going to multiply both sides by 1/3. When you multiply both sides by 1/3-- visually over here, if you had three x's, you multiply it by 1/3, you're only going to have one x left. If you have nine of these one-kilogram boxes, you multiply it by 1/3, you're only going to have three left. And over here, you can even visually-- if you divide by 3, which is the same thing as multiplying by 1/3, you divide by 3. So you divide by 3. You have an x is equal to a 1 plus 1 plus 1. An x is equal to 3. Or you see here, an x is equal to 3. Over here you do the math. 1/3 times 3 is 1. You're left with 1x. So you're left with x is equal to 9 times 1/3. Or you could even view it as 9 divided by 3, which is equal to 3.\",\n",
              "       ...,\n",
              "       ' - Let\\'s say that I have a circle. My best attempt to draw a reasonably perfect circle. So, there you go, not too bad, it\\'s a little bit of a hairy circle but you get the idea. So, this is a circle, this is the center of the circle, and let\\'s say that I have an arc along this circle. So, I\\'ll do the arc in green. So, I have an arc that is part of the circle, and it subtends an angle, so that\\'s my arc. Right over there, and it subtends an angle, and the angle that it subtends, so what I mean subtends, you take each of the endpoints of the arc, go to the center of the circle, go to the center of the circle just like this, and so it subtends angle theta, right over here, so it subtends angle theta, and let\\'s say that we know that angle theta is equal to two radians. So my question to you is what fraction of the entire circumference is this green arc? What fraction of the entire circumference is this green arc? And like always, pause the video, and give it a go. (laughs) All right, so let\\'s think through it a little bit. So, you might say well how do I know that, I don\\'t know what the radius of this thing is, I don\\'t, how do I think through this? And we just have to remind ourselves what radians mean, what radians mean. If an arc subtends the angle of two radians, that means that the arc itself is two \"radiuseseses\" long. (laughs) So, this right over here, let me make this a little clearer, so this, if the radius is r, if this radius is, I already used that color, if this radius... I have trouble switching colors (laughs) all right. If this radius is length r, then the length, if this angle is two radians, then the arc that subtends it is going to be two radiuses long, so this length right over here, is two radiuses. Now, what fraction of the entire circumference is that? Well, the entire circumference, we know, we know this from basic geometry, the entire circumference is two pi times the radius, or you can say it\\'s two pi radii, two pi \"radiuseses\", (laughs) two pi radii is the correct way to say it. So, what fraction is it? It\\'s two radii, it\\'s two radii, over two pi radii, over two pi radii, twos cancel out, rs cancel out, and so it is one \"pith\", (laughs) I guess you could say, it is one over pi of the total circumference.',\n",
              "       \" - So let's look at the female reproductive cycle. The female reproductive cycle refers to the maturation of eggs within the ovaries. The ovaries initially created these eggs during gestation. In other words, when a baby girl is in her mother's womb, the baby girl's entire egg supply will be created but will remain in an inactive state. This process of egg creation is called oogenesis. Then, once she grows up a bit and reaches puberty, her reproductive cycle will start, and one egg in that egg supply in her ovaries will mature or become activated each month, and that allows it to be fertilized by sperm. By the way, another word for egg is oocyte. After an egg matures, it's pushed out of the ovary in a process called ovulation. The other major function of the ovaries is to secrete the female sex hormones, estrogen, progesterone, and one called inhibin, and we'll talk about their functions a little bit later on. So let's first discuss how the eggs are made in the ovary in the first place. So early in uteral development, precursor germ cells, which are called oogonia, and those are homologous to spermatagonia in males, these oogonia undergo a ton of mitotic divisions to make more of themselves. And then, at about the 7th month of development, these divisions stop, and all of the ones that have been produced, which is actually about two to four million, are all she'll have for the rest of her life, and that turns out to be about one to two million per ovary. So while she's still in fetal development, all of these oogonia that have been produced, they all develop into the next stage, which is a primary oocyte. And just remember that the two oo's refers to egg, and the cyte, C-Y-T-E, refers to cell. So this just means egg cell, in case you were wondering. And let me also just mention, on a chromosomal level these oogonia, the germ cells, they're 2n, which means they have two copies of each chromosome, and the primary oocytes are the same. They're also 2n. And then these primary oocytes, they begin meiosis 1, and meiosis is what our germ cells use to reduce our chromosome copy number, and by that, I just mean the number of copies of DNA that we have. So they start this process of meiosis 1, but they don't actually finish it. They just kind of get about halfway through, and then they stop. So they're stuck as these big cells. So they're still primary oocytes, but they're said to be in meiotic arrest. So when the female who's been developing in her mom's womb, when she's born, her primary oocytes are in meiotic arrest. So the question is, do they stay like this? And the answer is, some do and some don't. Let's zoom in on this reproductive system to try to explain. So this is just a closeup of the major parts of the female reproductive system, and I've cut away parts of the uterus and the uterine tubes and the ovaries so you can see sort of the inside and the outsides of both structures. And this is our key organ here. This is the ovary. So the question was, do these primary oocytes that are stuck in meiotic arrest, do they stay like that? So the answer is that the ones that are sort of destined to be ovulated, that is, to be pushed out of the ovary right about here and then to be picked up by the fimbriae and then travel along the uterine tube here, those ones get past meiotic arrest. But most of them sort of die off while they're still stuck in that meiotic arrest phase as a primary oocyte. So I've mentioned the ones that sort of get out of the meiotic arrest phase and move on to develop into secondary oocytes that are able to then fuse with sperm, but when exactly does that happen? Well, it starts at puberty. So they actually stay in this phase as primary oocytes up here, in meiotic arrest for like 12 to 13 years, give or take, and only then do they start moving forward with development by finishing off that first part of meiosis that they started and splitting into two secondary oocytes. And actually, that's not exactly true, even though that's what we'd expect. What actually happens is one primary occyte it attempts to split into two secondary oocytes, but that's not exactly what happens. What does happen is that one of the developing daughter cells develops beautifully into a normal secondary oocyte from the primary oocyte, but it turns out that when they do complete the first part of meiosis, something really interesting happens. One of these cells receives basically all the cytoplasm. So the chromosome copy number is halved, but basically all the cytoplasm is kept in one cell. So this little guy over here that didn't get much cytoplasm, it still has a full complement of chromosomes, but it still ends up being pretty small and not really very functional. So it kind of withers away and dies, and it's called a polar body. So you end up with this really large secondary oocyte, and this is what ends up getting ovulated. And so now you might be thinking, well, meiosis is two steps, right? When does the second step happen? And that's a good question. So again, ovulation happens roughly here with the secondary oocyte coming out, and this secondary oocyte sort of just hangs out in the uterine tubes, and a sperm comes along and fertilizes the egg. So let's look at that down here. So you have your uterine tube here, and you have your egg. That's a secondary oocyte now. And then a sperm is coming along, and the sperm fuses with the egg after fertilizing it. And so the sperm fertilizes the egg and fuses with it. And so, let's just zoom in on what's happening there. So here you have your big secondary oocyte, and then you have your sperm that sort of, let's say that the nucleus of the sperm is right here. It's inside the egg already. This is the nucleus of the sperm. And here's the nucleus of the secondary oocyte. Well this is when meiosis 2 happens, so the second half of meiosis. So as this sperm nucleus is traveling toward the egg nucleus to create a joint nucleus, meiosis 2 occurs, and the oocyte reduces its chromosome copy number by creating another polar body, so a second polar body that kind of divides off the cell. So the oocyte cuts its chromosome copy number in half again, and so this little bit of DNA here that's just an extra copy of the DNA the egg already has, it divides off the cell in the form of another polar body that doesn't really have that much cytoplasm, just like the first one. So again, it leaves its nutrient-rich cytoplasm behind for the sperm and the egg. And by the way, the egg has changed its name now from a secondary oocyte to an ovum, but it won't be an ovum for long. Once the sperm nucleus fuses with the egg nucleus, then it becomes a zygote. So let me just clarify that if the egg doesn't get fertilized by a sperm that comes along, then it doesn't complete that second meiotic division that it did right here, and it just gets discharged from the body in menstruation as a secondary oocyte and not as an ovum, because the name ovum is reserved for the oocyte only once it's been fertilized. So those are the basic concepts behind what goes on with egg development.\",\n",
              "       \" -  We're now in the home stretch. We just have to evaluate the curl of f and then this dot product and then evaluate this double integral. So let's work on the curl of F. So the curl of f is going to be equal to, and I just remember it as the determit, so we have our i, j, k components, and it's really you could imagine it's the del operator crossed with the actual vector. So the del operator, I'll write this in a different color just to ease the monotony, so this is partial with respect to x, partial with respect to y, partial with respect to z, and then our vector field, I copied and pasted it right over here. It is just equal to negative y squared, is our i component, x is our j component, and Z squared is our k component. And so this is going to be equal to, this is going to be equal to i, is going to be equal to i times the partial of Z squared with respect to y. Well, there's the Z squared is just a constant with respect to y so the partial of Z squared with respect to y is just going to be zero, so this is going to be zero. Minus the partial of x with respect to z. Well, once again this is just a constant when you think in terms of z, so that's just going to be zero. So that's nice simplification, and then we're gonna have minus j, we need our little checkerboard patterns, we put a negative in front of the j, minus j and so we'll have the partial of x, the partial of z squared with respect to x, that's zero again, and then minus the partial of negative y squared with respect to z, well that's zero again, and then finally we have our k component, k, so plus, plus k, and k, we're gonna have the partial of x with respect to x, well that actually gives us a value that's just gonna be one minus the partial of negative y squared with respect to y. So the partial of negative y squared with respect to y is negative two y and we're subtracting that, so it's going to be plus, plus, two y. So curl of f simplifies to just, all of this is just zero up here, is just one plus two y times k or k times one plus two y. And so if we go back to this right up here, if we go back up to that, we're going to get let me re-write the integral so zero to one and that's our r, our r parameter is gonna go from zero to one, theta is gonna go from zero to two pi. And now curl of f has simplified to, and I won't skip any steps although it's tempting, it's one plus two y, and actually instead of writing two y, let me write it in terms of the parameters. We saw it up here, y was r sine theta, if I remember correctly, right, y was r sine theta. So let me write y that way. Two times r sine theta k. And we're gonna dot this, we're gonna take the dot product of that with this right over here, with r times j plus r times k, d theto d r. And so we take the dot product, this thing only has a k component, the j component is zero, so when you take the dot product with this j component you're gonna get zero. And neither of them you actually even have an i component. And so the inside is just going to simplify to this piece right over here is going to simplify to, we're just gonna have to think about the k components, cause everything else is zero, so it's gonna be r times this and we're done! So it's gonna be r plus two r squared sine theta, d theta d r, d theta d r and, once again, theta goes from zero to two pi and r goes from zero to one. And now this is just a straight-up double integral. We just have to evaluate this thing. And so, first we take the antiderivative with respect to theta, so the antiderivative with respect to theta is going to give us, so this is going to be giving, so we're going to focus on theta first, so the antiderivative of r with respect to theta is just r theta, you can just do r as a constant, and then the antiderivative of this, antiderivative of sine of theta is negative cosine of theta. So this is gonna be negative two r squared cosine of theta. And we're gonna evaluate it from zero to two pi. And then we have the outside integral, which I will, I'll re-color in yellow, re-color in yellow, so we'll still have to integrate with respect to r and r's gonna go from zero to one. But inside right over here, if we evaluate all of this business right over here at two pi, we get two pi r, two pi r, that's that right over there, minus... Cosine of two pi is just one. So it's minus two r squared and then from that, we're going to subtract from that, we're gonna subtract this evaluated zero. Well r times zero is just zero, and then cosine of zero is one. So it's just minus two r squared, or negative two r squared, negative two r squared. And at this negative and this negative, you get a positive, and but then you have a negative two r squared and then a plus two r squared it's just going to cancel out, that and that cancel out, and so this whole thing has simplified quite nicely to a simple definite integral, zero to one of two pi, two pi r dr, and the antiderivative of this is just going to be pi r squared, so we're just gonna evaluate pi r squared from zero to one, when you evaluate it at one, you get pi; when you evaluate it at zero, you just get zero, so you get pi minus zero, which is equal to, and now we deserve a drumroll 'cause we've been doing a lot of work over many videos, this is equal to pi. So just to remind ourselves what we've done over the last few videos, we had this line integral that we were trying to figure out, and instead of directly evaluating the line integral, which we could do and I encourage you to do so, and if I have time, I might do it in the next video, instead of directly evaluating that line integral, we used Stokes theorem to say, oh we could actually instead say that that's the same thing as a surface integral over a piecewise-smooth boundary over piecewise-smooth surface that this path is the boundary of, and so we evaluated this surface intergal and eventually, with a good bit of, little bit of calculation, we got to evaluating it to be equal to pi.\"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ian7gSDE3khR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a23349-2d44-4b72-af7c-d4e9eb436402"
      },
      "source": [
        "len(categories)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ZeuHc63khU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b627a6-30da-40e8-ed53-728a2e928e80"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in question_answer:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', question_answer[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\n",
            "Token IDs: tensor([  101,  1999,  1996,  2197,  3232,  1997,  6876,  2057,  2387,  2008,\n",
            "         2057,  2064,  6235,  1037, 10543,  2011,  1037,  2597,  9207,  1011,\n",
            "        11126,  3853,  1012,  1998,  1999,  2200,  2236,  3408,  1010,  2009,\n",
            "         2052,  2022,  1996,  1060,  2597,  2004,  1037,  3853,  1997,  2051,\n",
            "         2335,  1996,  3131,  9207,  1999,  1996,  9876,  3257,  1012,  4606,\n",
            "         1996,  1061,  2597,  2004,  1037,  3853,  1997,  2051,  2335,  1996,\n",
            "         3131,  5125,  1999,  1996,  7471,  3257,  1012,  1998,  2023,  2097,\n",
            "         7687,  6235,  2023,  1011,  1011,  2295,  1010,  2065,  2017,  2064,\n",
            "         5674,  1037, 10811,  1998,  2292,  1005,  1055,  2360,  1996, 16381,\n",
            "         1056,  5836,  2051,  1012,  2009,  1005,  2222,  6235,  2073,  1996,\n",
            "        10811,  2003,  2012,  2151,  2445,  2051,  1012,  1998,  2065,  2057,\n",
            "         2359,  1037,  3327,  7774,  2057,  2064,  2360,  1010,  2092,  1010,\n",
            "         2023,  2069, 12033,  2005,  2070,  7774,  1011,  1011,  2057,  1005,\n",
            "         2128,  7149,  1010,  2009,  1005,  1055,  1054,  1997,  1056,  1012,\n",
            "         1998,  2009,  1005,  1055,  2069, 12711,  2090,  1056,  2108,  3618,\n",
            "         2084,  1037,  1998,  2625,  2084,  1038,  1012,  1998,  2017,  2113,\n",
            "         1010,  2008,  2052,  6235,  2070,  7774,  1999,  2048,  9646,  1012,\n",
            "         2074,  2033,  2074,  4009,  2009,  2182,  1012,  2023,  2003,  2035,\n",
            "         1037,  3319,  1997,  2428,  1010,  1996,  2197,  2048,  6876,  1012,\n",
            "         2061,  2023,  7774,  1010,  2009,  2453,  2298,  2242,  2066,  2008,\n",
            "         2073,  2023,  2003,  2073,  1056,  2003,  5020,  2000,  1037,  1012,\n",
            "         2008,  1005,  1055,  2073,  1056,  2003,  5020,  2000,  1038,  1012,\n",
            "         1998,  2061,  1054,  1997,  1037,  2097,  2022,  2023,  9207,  2157,\n",
            "         2182,  2008,  4515,  2012,  2008,  2391,  1012,  1998,  2059,  2004,\n",
            "         1056,  2030,  2065,  2017,  2064,  5674,  1996, 16381,  2108,  2051,\n",
            "         1010,  2009,  2987,  1005,  1056,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1mg7WyWKi6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6352263e-9f46-4b87-c355-996b433cf567"
      },
      "source": [
        "input_ids_val = []\n",
        "attention_masks_val = []\n",
        "\n",
        "for sent in val_features:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids_val.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks_val.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
        "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
        "\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', question_answer[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   In the last couple of videos we saw that we can describe a curves by a position vector-valued function. And in very general terms, it would be the x position as a function of time times the unit vector in the horizontal direction. Plus the y position as a function of time times the unit victor in the vertical direction. And this will essentially describe this-- though, if you can imagine a particle and let's say the parameter t represents time. It'll describe where the particle is at any given time. And if we wanted a particular curve we can say, well, this only applies for some curve-- we're dealing, it's r of t. And it's only applicable between t being greater than a and less than b. And you know, that would describe some curve in two dimensions. Just me just draw it here. This is all a review of really, the last two videos. So this curve, it might look something like that where this is where t is equal to a. That's where t is equal to b. And so r of a will be this vector right here that ends at that point. And then as t or if you can imagine the parameter being time, it doesn't have to be time, but that's a convenient one to visualize. Each corresponding as t gets larger and larger, we're just going to different-- we're specifying different points on the path. We saw that two videos ago. And in the last video we thought about, well, what does it mean to take the derivative of a vector-valued function? And we came up with this idea that-- and it wasn't an idea, we actually showed it to be true. We came up with a definition really. That the derivative-- I could call it r prime of t-- and it's going to be a vector. The derivative of a vector-valued function is once again going to be a derivative. But it was equal to-- the way we defined it-- x prime of t times i plus y prime of t times j. Or another way to write that and I'll just write all the different ways just so you get familiar with-- dr/dt is equal to dx/dt. This is just a standard derivative. x of t is a scalar function. So this is a standard derivative times i plus dy/dt times j. And if we wanted to think about the differential, one thing that we can think about-- and whenever I do the math for the differential it's a little bit hand wavy. I'm not being very rigorous. But if you imagine multiplying both sides of the equation by a very small dt or this exact dt, you would get dr is equal to-- I'll just leave it like this. dx/dt times dt. I could make these cancel out, but I'll just write it like this first. Times the unit vector i plus dy/dt times dt. Times the unit vector j. Or we could rewrite this. And I'm just rewriting it in all of the different ways that one can rewrite it. You could also write this as dr is equal to x prime of t dt times the unit vector i. So this was x prime of t dt. This is x prime of t right there times the unit vector i. Plus y prime of t. That's just that right there. Times dt. Times the unit vector j. And just to, I guess, complete the trifecta, the other way that we could write this is that dr is equal to-- if we just allowed these to cancel out, then we get is equal to dx times i plus dy times dy y times j. And that actually makes a lot of intuitive sense. That if I look at any dr, so let's say I look at the change between this vector and this vector. Let's say the super small change right there, that is our dr, and it's made up of-- it's our dx, our change in x is that right there. You can imagine it's that right there times-- but we're vectorizing it by multiplying it by the unit vector in the horizontal direction. Plus dy times the unit vector in the vertical direction. So when you multiply this distance times the unit vector, you're essentially getting this vector. And when you multiply this guy-- and actually our change in y here is negative-- you're going to get this vector right here. So when you add those together you'll get your change in your actual position vector. So that was all a little bit of background. And this might be somewhat useful-- a future video from now. Actually, I'm going to leave it there because really I just wanted to introduce this notation and get you familiar with it. In the next video, what I'm going to do is give you a little bit more intuition for what exactly does this thing mean? And how does it change depending on different parameterizations. And I'll do it with two different parameterizations for the same curve.\n",
            "Token IDs: tensor([  101,  1999,  1996,  2197,  3232,  1997,  6876,  2057,  2387,  2008,\n",
            "         2057,  2064,  6235,  1037, 10543,  2011,  1037,  2597,  9207,  1011,\n",
            "        11126,  3853,  1012,  1998,  1999,  2200,  2236,  3408,  1010,  2009,\n",
            "         2052,  2022,  1996,  1060,  2597,  2004,  1037,  3853,  1997,  2051,\n",
            "         2335,  1996,  3131,  9207,  1999,  1996,  9876,  3257,  1012,  4606,\n",
            "         1996,  1061,  2597,  2004,  1037,  3853,  1997,  2051,  2335,  1996,\n",
            "         3131,  5125,  1999,  1996,  7471,  3257,  1012,  1998,  2023,  2097,\n",
            "         7687,  6235,  2023,  1011,  1011,  2295,  1010,  2065,  2017,  2064,\n",
            "         5674,  1037, 10811,  1998,  2292,  1005,  1055,  2360,  1996, 16381,\n",
            "         1056,  5836,  2051,  1012,  2009,  1005,  2222,  6235,  2073,  1996,\n",
            "        10811,  2003,  2012,  2151,  2445,  2051,  1012,  1998,  2065,  2057,\n",
            "         2359,  1037,  3327,  7774,  2057,  2064,  2360,  1010,  2092,  1010,\n",
            "         2023,  2069, 12033,  2005,  2070,  7774,  1011,  1011,  2057,  1005,\n",
            "         2128,  7149,  1010,  2009,  1005,  1055,  1054,  1997,  1056,  1012,\n",
            "         1998,  2009,  1005,  1055,  2069, 12711,  2090,  1056,  2108,  3618,\n",
            "         2084,  1037,  1998,  2625,  2084,  1038,  1012,  1998,  2017,  2113,\n",
            "         1010,  2008,  2052,  6235,  2070,  7774,  1999,  2048,  9646,  1012,\n",
            "         2074,  2033,  2074,  4009,  2009,  2182,  1012,  2023,  2003,  2035,\n",
            "         1037,  3319,  1997,  2428,  1010,  1996,  2197,  2048,  6876,  1012,\n",
            "         2061,  2023,  7774,  1010,  2009,  2453,  2298,  2242,  2066,  2008,\n",
            "         2073,  2023,  2003,  2073,  1056,  2003,  5020,  2000,  1037,  1012,\n",
            "         2008,  1005,  1055,  2073,  1056,  2003,  5020,  2000,  1038,  1012,\n",
            "         1998,  2061,  1054,  1997,  1037,  2097,  2022,  2023,  9207,  2157,\n",
            "         2182,  2008,  4515,  2012,  2008,  2391,  1012,  1998,  2059,  2004,\n",
            "         1056,  2030,  2065,  2017,  2064,  5674,  1996, 16381,  2108,  2051,\n",
            "         1010,  2009,  2987,  1005,  1056,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVGvVZb13kha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bcc480-cfff-47f8-b340-1428791e9170"
      },
      "source": [
        "print('Original: ', question_answer[1])\n",
        "print('Token IDs:', input_ids[1])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   -  What we're going to do in this video is give ourselves a little bit of a tour of eukaryotic cells. And the first place to start is just to remind ourselves what it means for a cell to be eukaryotic. It means that inside the cell, there are membrane-bound organelles. Now, what does that mean? Well, you could view it as sub-compartments within the cell. Membrane-bound organelles. And in this video in particular, we're going to highlight some of these membrane-bound organelles that make the cells eukaryotic. So let's just start with some of the ingredients that we know is true of all cells. So you'll have your cellular membrane here. I drew it big, so that we have a lot of space to draw things in. So this is our cellular membrane. I'll do some nice shading so you appreciate that it'll actually be three-dimensional. We see so many slices of cells that sometimes we forget that they are more spherical, or that they have three-dimensional shape to them. They're not all spherical. They can have different shapes. Now all cells, and there are some exceptions that we've talked about in previous videos. I should say, most cells will have some genetic information in them in the form of DNA. So that is our DNA, right over there. Now, one of the key characteristics of a eukaryotic cell is that the genetic information is going to be inside a membrane-bound organelle. And that membrane-bound organelle, or the membrane that surrounds the DNA here, that is the nuclear membrane. So let me draw the nuclear membrane right over here, and I'll put some shading in to appreciate that that also is going to be in three dimensions, around the DNA. So that is the first membrane-bound organelle that we're gonna discuss, the nucleus. Now the nucleus, it turns out, is connected to another membrane-bound organelle. And we're gonna study this in future videos, but right here I'm drawing holes or pores in the nuclear membrane. And those pores connect to something, it's a very fancy word called the endoplasmic reticulum. And the endoplasmic reticulum is essentially these layers of these membranes. So I'm gonna do my best job at trying to draw an endoplasmic reticulum. Imagine extending from these pores, going into a space that has really these layered membranes that have a lot of surface area. And I'm not gonna go all the way around this nucleus, but in many cells it will go around, all the way around the nucleus. And this right over here, and this is just a rough diagram. That is our endoplasmic, endoplasmic... Not blasmic, endoplasmic... Endoplasmic reticulum, which I've mentioned in previous videos would be an excellent name for a band. And what goes on in the endoplasmic reticulum is when you are in the process of taking that genetic information from DNA, and as we talk about in other videos it gets transcribed into mRNA. So that mRNA is now containing that information. That mRNA will make its way out of that nuclear membrane through one of these pores, and then make its way to a ribosome that is attached to the membrane of the endoplasmic reticulum. And so that's a ribosome there. I'm gonna do a bunch of ribosomes. And so as we've talked about in previous videos, ribosomes are really where you take that genetic information from that mRNA, and then you translate it into a protein. So the ribosomes are the protein synthesis, so let me label that. So this right over here is a ribosome. And some ribosomes might be attached to the endoplasmic reticulum. Some of them might just be floating out here in the cytoplasm, so that would be a free ribosome. Free ribosome. And even from the point of view of the endoplasmic reticulum, the parts of the endoplasmic reticulum where you have ribosomes attached, this is known as rough endoplasmic reticulum. It's the ribosomes that are making them rough. It looks that way in a microscope. So I'll just say rough ER, for endoplasmic reticulum for short. And then you also have parts of the endoplasmic reticulum where you do not have ribosomes attached. And because that looks smooth through our microscope, it has been called, you can imagine, smooth endoplasmic reticulum. There are things known as golgi bodies. Once again, another fascinating name. You gotta love these names in biology. That look kind of like an endoplasmic reticulum, but detached from the nuclear membrane. So let's say it's something like that. That's my best drawing there. That's a golgi body. And these are really good at packaging molecules, even proteins that might've just been produced, and packaging them so that they can be used outside of the cell, for example. And we'll go into detail in other videos, where a protein might go to the golgi body, get a little envelope around it, get some little processing going on, and then make its way outside of a cell. Now another, and this is maybe one of the most famous membrane-bound organelles outside of the nucleus, is what's known as the powerhouse of the cell, and that is the mitochondria. So I'll draw this mitochondria in magenta, because that's a nice powerful color. So mitochondria. And I love mitochondria because it's fascinating how they even came to be. Mitochondria actually have their own DNA, and all of your mitochondrial DNA comes from your mother. So that's actually very interesting for tracing maternal lineage. But mitochondria, this is where your, I'm gonna say let's see what we could see inside of this. This is where you ATP is produced. This is your mitochondria. It's really the powerhouse of the cell. What's interesting about mitochondria is evolutionary biologists believe that the ancestors of mitochondria, because mitochondria have their own DNA, they might've been independent organisms, independent cells. And at some point in our evolutionary past, they started living in symbiosis inside of what would be the ancestors of our cells. And over time, they became so codependent that they started to replicate together. And mitochondria, in fact, became part of these eukaryotic cells. Now if this eukaryotic cell was a plant cell or maybe an algae cell, you would have something called chloroplasts there. We don't have them because we don't have photosynthesis, but this is a chloroplast. And if you could see inside, you could see the little thylakoid stacks right over here. You could see the thylakoids if you could see inside. And so this right over here is a chloroplast. Chloroplast. And this would be plants and algae. Animals do not have these. And these are where you have your photosynthesis take place. Photosynthesis. Now there's also some membrane-bound organelles that are maybe less famous than the mitochondria or the chloroplast, or for sure the nucleus, and that might be something like a vacuole. And in plants, vacuoles tend to be very big. I could draw it, this is three-dimensional so I'll draw it on top of something that I've drawn before. So if a vacuole right over here, this is a... And in a plant it could be a fairly significant compartment inside. And in fact, it can even give structure to the plant itself because it is so big. And it contains water and enzymes. It's viewed as kind of a storage compartment. But it can also contain enzymes that help digest things, that help break things down so that they can be used in some way. So that is a vacuole. And they don't just exist in plants. They can also exist in animal cells. But in plant cells, they can be very, very, very visible. Now, something that is somewhat related to some of the function that a vacuole plays, that are most associated with animal cells but now there's evidence that they also exist in plant cells, is the idea of a lysosome. So a lysosome right over here, that also is a compartment. And it's going to contain a whole series of enzymes in it that is useful for lysing, you could say, that is useful for breaking down either waste products as the cell lives, or even foreign substances that might not be helpful for the cells. So it's gonna contain a bunch of enzymes, and it helps break down things. Now, I'll leave you there. These aren't all of the structures in eukaryotic cells, but these are enough of the structures so that you can appreciate that there are a lot of membrane-bound organelles in eukaryotic cells. And to be clear, even if I were to show all of the membrane-bound structures, that's not all the complexity of the cell. The big thing to appreciate is that cells are incredibly complex. There's all sorts of structures in here that help transport things and move things around. If you could shrink yourself down and look inside of a cell, it would look more complex than the most complex cities. There's all sorts of activities, things being moved around, shuttled around. The cell itself is replicating and copying things. And so this is just the beginning. We're just starting to scratch the surface of the complexity of the most basic unit of life.\n",
            "Token IDs: tensor([  101,  1011,  2054,  2057,  1005,  2128,  2183,  2000,  2079,  1999,\n",
            "         2023,  2678,  2003,  2507,  9731,  1037,  2210,  2978,  1997,  1037,\n",
            "         2778,  1997,  7327,  6673,  7677,  4588,  4442,  1012,  1998,  1996,\n",
            "         2034,  2173,  2000,  2707,  2003,  2074,  2000, 10825,  9731,  2054,\n",
            "         2009,  2965,  2005,  1037,  3526,  2000,  2022,  7327,  6673,  7677,\n",
            "         4588,  1012,  2009,  2965,  2008,  2503,  1996,  3526,  1010,  2045,\n",
            "         2024, 10804,  1011,  5391,  5812, 22869,  1012,  2085,  1010,  2054,\n",
            "         2515,  2008,  2812,  1029,  2092,  1010,  2017,  2071,  3193,  2009,\n",
            "         2004,  4942,  1011, 27998,  2306,  1996,  3526,  1012, 10804,  1011,\n",
            "         5391,  5812, 22869,  1012,  1998,  1999,  2023,  2678,  1999,  3327,\n",
            "         1010,  2057,  1005,  2128,  2183,  2000, 12944,  2070,  1997,  2122,\n",
            "        10804,  1011,  5391,  5812, 22869,  2008,  2191,  1996,  4442,  7327,\n",
            "         6673,  7677,  4588,  1012,  2061,  2292,  1005,  1055,  2074,  2707,\n",
            "         2007,  2070,  1997,  1996, 12760,  2008,  2057,  2113,  2003,  2995,\n",
            "         1997,  2035,  4442,  1012,  2061,  2017,  1005,  2222,  2031,  2115,\n",
            "        12562, 10804,  2182,  1012,  1045,  3881,  2009,  2502,  1010,  2061,\n",
            "         2008,  2057,  2031,  1037,  2843,  1997,  2686,  2000,  4009,  2477,\n",
            "         1999,  1012,  2061,  2023,  2003,  2256, 12562, 10804,  1012,  1045,\n",
            "         1005,  2222,  2079,  2070,  3835, 21146,  4667,  2061,  2017,  9120,\n",
            "         2008,  2009,  1005,  2222,  2941,  2022,  2093,  1011,  8789,  1012,\n",
            "         2057,  2156,  2061,  2116, 25609,  1997,  4442,  2008,  2823,  2057,\n",
            "         5293,  2008,  2027,  2024,  2062, 18970,  1010,  2030,  2008,  2027,\n",
            "         2031,  2093,  1011,  8789,  4338,  2000,  2068,  1012,  2027,  1005,\n",
            "         2128,  2025,  2035, 18970,  1012,  2027,  2064,  2031,  2367, 10466,\n",
            "         1012,  2085,  2035,  4442,  1010,  1998,  2045,  2024,  2070, 11790,\n",
            "         2008,  2057,  1005,  2310,  5720,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irMTimjf3khd"
      },
      "source": [
        "labels = torch.tensor(categories)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CICIofG9No7L",
        "outputId": "f292f429-6e8d-4bc1-ab68-a0d87d165ac4"
      },
      "source": [
        "get_labels(419)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'science>>ap-biology>>ecology-ap'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJJ0I8Ud3khf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7f0318e1-e540-4f06-85db-6652218ef71c"
      },
      "source": [
        "get_labels(311)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math>>in-in-grade-12-ncert>>in-in-determinants'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDW74Ny3khj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588129d3-96ff-4b14-f546-8793e91197c6"
      },
      "source": [
        "num_classes = len(list(set(categories)))\n",
        "num_classes"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "569"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA2zYBezKi60"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "# train_poincare_tensor = torch.tensor(poincare_embeddings_final,dtype=torch.float)\n",
        "# train_poincare_tensor = torch.tensor(poincare_embeddings_final_train,dtype=torch.float)\n",
        "# val_poincare_tensor = torch.tensor(poincare_embeddings_final_val, dtype=torch.float)\n",
        "train_labels = torch.tensor(categories)\n",
        "val_labels = torch.tensor(val_labels.values)\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "train_dataset = TensorDataset(input_ids, attention_masks, train_labels)\n",
        "val_dataset = TensorDataset(input_ids_val,attention_masks_val,val_labels)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo2jJuP1INTf"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_lTinod3kho"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2tmAMlw3khr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7089984-7c63-4577-93f2-b97e4cba71d4"
      },
      "source": [
        "\n",
        "# run this cell to prepare model for inference\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Loads BertForSequenceClassification, the pretrained BERT model with a single \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"model_save_categorized_reduced_khan_acad\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 572,   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=572, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUw3zm6g3kh5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta6zfUTa3kh7"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFq9gd5kQSHb"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4178_yLFMWmx"
      },
      "source": [
        "test_features = test_features.values\n",
        "labels = test_labels.values"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vqVYz4O3xcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661cad2a-5f94-4f54-f265-fa61c5f91593"
      },
      "source": [
        "test_features"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' -  What I hope to do in this video is get familiar with the notion of an interval, and also think about ways that we can show an interval, or interval notation. Right over here I have a number line. Let\\'s say I wanted to talk about the interval on the number line that goes from negative three to two. So I care about this-- Let me use a different color. Let\\'s say I care about this interval right over here. I care about all the numbers from negative three to two. So in order to be more precise, I have to be clear. Am I including negative three and two, or am I not including negative three and two, or maybe I\\'m just including one of them. So if I\\'m including negative three and two, then I would fill them in. So this right over here, I\\'m filling negative three and two in, which means that negative three and two are part of this interval. And when you include the endpoints, this is called a closed interval. Closed interval. And I just showed you how I can depict it on a number line, by actually filling in the endpoints and there\\'s multiple ways to talk about this interval mathematically. I could say that this is all of the... Let\\'s say this number line is showing different values for x. I could say these are all of the x\\'s that are between negative three and two. And notice, I have negative three is less than or equal to x so that\\'s telling us that x could be equal to, that x could be equal to negative three. And then we have x is less than or equal to positive two, so that means that x could be equal to positive two, so it is a closed interval. Another way that we could depict this closed interval is we could say, okay, we\\'re talking about the interval between, and we can use brackets because it\\'s a closed interval, negative three and two, and once again I\\'m using brackets here, these brackets tell us that we include, this bracket on the left says that we include negative three, and this bracket on the right says that we include positive two in our interval. Sometimes you might see things written a little bit more math-y. You might see x is a member of the real numbers such that... And I could put these curly brackets around like this. These curly brackets say that we\\'re talking about a set of values, and we\\'re saying that the set of all x\\'s that are a member of the real number, so this is just fancy math notation, it\\'s a member of the real numbers. I\\'m using the Greek letter epsilon right over here. It\\'s a member of the real numbers such that. This vertical line here means \"such that,\" negative three is less x is less than-- negative three is less than or equal to x, is less than or equal to two. I could also write it this way. I could write x is a member of the real numbers such that x is a member, such that x is a member of this closed set, I\\'m including the endpoints here. So these are all different ways of denoting or depicting the same interval. Let\\'s do some more examples here. So let\\'s-- Let me draw a number line again. So, a number line. And now let me do-- Let me just do an open interval. An open interval just so that we clearly can see the difference. Let\\'s say that I want to talk about the values between negative one and four. Let me use a different color. So the values between negative one and four, but I don\\'t want to include negative one and four. So this is going to be an open interval. So I\\'m not going to include four, and I\\'m not going to include negative one. Notice I have open circles here. Over here had closed circles, the closed circles told me that I included negative three and two. Now I have open circles here, so that says that I\\'m not, it\\'s all the values in between negative one and four. Negative .999999 is going to be included, but negative one is not going to be included. And 3.9999999 is going to be included, but four is not going to be included. So how would we-- What would be the notation for this? Well, here we could say x is going to be a member of the real numbers such that negative one-- I\\'m not going to say less than or equal to because x can\\'t be equal to negative one, so negative one is strictly less than x, is strictly less than four. Notice not less than or equal, because I can\\'t be equal to four, four is not included. So that\\'s one way to say it. Another way I could write it like this. x is a member of the real numbers such that x is a member of... Now the interval is from negative one to four but I\\'m not gonna use these brackets. These brackets say, \"Hey, let me include the endpoint,\" but I\\'m not going to include them, so I\\'m going to put the parentheses right over here. Parentheses. So this tells us that we\\'re dealing with an open interval. This right over here, let me make it clear, this is an open interval. Now you\\'re probably wondering, okay, in this case both endpoints were included, it\\'s a closed interval. In this case both endpoints were excluded, it\\'s an open interval. Can you have things that have one endpoint included and one point excluded, and the answer is absolutely. Let\\'s see an example of that. I\\'ll get another number line here. Another number line. And let\\'s say that we want to-- Actually, let me do it the other way around. Let me write it first, and then I\\'ll graph it. So let\\'s say we\\'re thinking about all of the x\\'s that are a member of the real numbers such that let\\'s say negative four is not included, is less than x, is less than or equal to negative one. So now negative one is included. So we\\'re not going to include negative four. Negative four is strictly less than, not less than or equal to, so x can\\'t be equal to negative four, open circle there. But x could be equal to negative one. It has to be less than or equal to negative one. It could be equal to negative one so I\\'m going to fill that in right over there. And it\\'s everything in between. If I want to write it with this notation I could write x is a member of the real numbers such that x is a member of the interval, so it\\'s going to go between negative four and negative one, but we\\'re not including negative four. We have an open circle here so I\\'m gonna put a parentheses on that side, but we are including negative one. We are including negative one. So we put a bracket on that side. That right over there would be the notation. Now there\\'s other things that you could do with interval notation. You could say, well hey, everything except for some values. Let me give another example. Let\\'s get another example here. Let\\'s say that we wanna talk about all the real numbers except for one. We want to include all of the real numbers. All of the real numbers except for one. Except for one, so we\\'re gonna exclude one right over here, open circle, but it can be any other real number. So how would we denote this? Well, we could write x is a member of the real numbers such that x does not equal one. So here I\\'m saying x can be a member of the real numbers but x cannot be equal to one. It can be anything else, but it cannot be equal to one. And there\\'s other ways of denoting this exact same interval. You could say x is a member of the real numbers such that x is less than one, or x is greater than one. So you could write it just like that. Or you could do something interesting. This is the one that I would use, this is the shortest and it makes it very clear. You say hey, everything except for one. But you could even do something fancy, like you could say x is a member of the real numbers such that x is a member of the set going from negative infinity to one, not including one, or x is a member of the set going from-- or a member of the interval going from one, not including one, all the way to positive, all the way to positive infinity. And when we\\'re talking about negative infinity or positive infinity, you always put a parentheses. And the view there is you could never include everything all the way up to infinity. It needs to be at least open at that endpoint because infinity just keeps going on and on. So you always want to put a parentheses if you\\'re talking about infinity or negative infinity. It\\'s not really an endpoint, it keeps going on and on forever. So you use the notation for open interval, at least at that end, and notice we\\'re not including, we\\'re not including one either, so if x is a member of this interval or that interval, it essentially could be anything other than one. But this would have been the simplest notation to describe that.',\n",
              "       \" In the last video we were able to set up this definite integral using the shell or the hollow cylinder method in order to figure out the volume of this solid of revolution. And so now let's just evaluate this thing. And really the main thing we have to do here is just to multiply what we have here out. So multiply this expression out. So this is going to be equal to-- I'll take the 2 pi out of the integral. 2 pi times the integral from 0 to 1. Let's see, 2 times the square root of x is 2-- I'll write it as 2 square roots of x. But I'll write it as 2x to the 1/2. It'll make it a little bit easier to take the antiderivative conceptually, or at least in our brain. So two times the square root of x is 2x to the 1/2. 2 times negative x squared is negative 2 x squared. And then we have negative x times the square root of x. Well, that's x to the first times x to the 1/2. That's going to be negative x to the 3/2 power. And then we have negative x times negative x squared that's going to be positive x to the third power. And all of that dx. And so now we're ready to take the antiderivative. So this is going to be equal to 2 pi times the antiderivative of all of this business evaluated at 1 and at 0. So the antiderivative of 2 times x to the 1/2 is going to be 2-- it's going to be-- let's see. We're going to take x to the 3/2 times 2/3. So it's going to be 4/3 x to the 3/2. And then for this term right over here it's going to be negative 2/3 x to the third. And you could take the derivative here to verify that you actually do get this. And then right over here, let's see, if we incremented this, you get x to the 5/2. And so we're going to want to multiply by 2/5. So minus-- let me do this in another color. Let's see, so this one right over here, it's going to be minus 2/5 x to the 5/2 power. Yep, that works out. And then finally you're going to have x to the fourth over 4 plus-- let me do that in a different color-- plus x to the fourth over 4. That's this term right over here. And now we just have to evaluate at 1 and 0. And 0, luckily, all of these terms end up being a 0. So that's nice and cancels out. And so we are just left with-- we're just-- [INAUDIBLE] cancel out. It just evaluates to 0. So this is just 2 pi times when you evaluate all this business at 1. So that's going to be 4/3 minus 2/3 minus 2/5 plus 1/4. And the least common multiple right over here looks like 60, so we're going to want to put all this over a denominator of 60. So it's going to be 2 pi times all of this business over a denominator of 60. And 4/3 is same thing as 80/60. Negative 2/3 is the same thing as negative 40/60. Negative 2/5 is the same thing as negative 24/60. And then 1/4 is the same thing as 15/60. So this is equal to-- and actually this will cancel over here, and you'll just get a 30 in your denominator. So in your denominator, you get a 30. And up here 80 minus 40 is 40. 40 minus 24 gets us to 16. 16 plus 15 is 31. So we get 31 times pi over 30 for the volume of the figure right over there.\",\n",
              "       \" -  In previous videos we talk about GDP as the market value of final goods and services produced in a country in a given time period, let's say in a given year and we gave the example of producing jeans where maybe the farmer helps produce the cotton and then the thread maker takes that cotton and makes thread and then the fabric maker takes the thread and makes fabric and then the jean maker takes the fabric and produces jeans and then the market value of those jeans was $50 and so, assuming all of this happened in one year, in the time period that we're measuring GDP for, then we would just count the $50, if we're looking at the final market value or the market value of final goods and services. You would say the GDP for at least for this component of the GDP from these jeans is $50 but I do wanna clarify that there are multiple ways that you can measure GDP and you could even think about it from a value added approach but the key idea is no matter how you measure it you should get to the same value, so let's think about the various actors here and what their value add was. So, first, let's think about the framer right over here. So, this is the farmer, my not so elegantly drawn rectangle around what he's doing. So, the farmer's value add is what? Well, before you just had some dirt and things and so, maybe you could say that the market value was zero and then he's able to produce something or she's able to produce something that now has a market value of $10, so their value add is $10. Now, from there, the cotton goes to the thread maker. The threader maker they take that $10 cotton, so this is thread maker, they take the $10 cotton and are able to produce $20 worth of thread? What is their value add? Value add here? Well, they took something worth $10 and they were able to do something to it to make it worth $20, so their value add is now another $10 and then, this is the thread maker, and then from there it goes to the fabric maker and I think you see where this is going, the fabric maker is this part of our process, fabric maker and their value add is what? Pause this video and think about it. Well, they take something worth $20 and they're to turn it into something that has a market value of $30, so their value add is also $10 and then last but not least, you have the jean company, so the jean manufacturer, I'll call them the jean producer, the jean producer, they take something that has a market value of $30 and they're able to sell it for $50, so their value add here, if you take something for 30 and you make it worth 50, then you've added $20 of value and so, the value added approach to GDP will just sum up these value adds, so this is going to be this $10 from the farmer plus the value add of the thread maker, plus $10 from the thread maker plus $10 from the fabric maker plus $20 from the jean maker and what will that all add up to? Well, that's all going to add up to 10 plus 10 is 20 plus 10 is 30 plus 20 is $50 and lucky for us that is added up to the same amount as we had before where we just looked at the market value of the final goods and services. Now, one benefit of the value added approach is that real supply chains are quite complex and things might be going from one country to another, they might as we've talked about in another video, the year might end right over here and so, when something is made in China and there's value add in China but then it's shipped to the US and some value add is placed on it and then it's shipped back to China or Mexico, you have to be careful to only count the value add in the country for which you are measuring the GDP. So, that's one useful way or one useful reason, or one way in which the value added approach might be useful. The key idea though is that you're getting to the same value. You should get to the same value as the market value of the final goods and services produced in a given time period.\",\n",
              "       ...,\n",
              "       \" Let's see if we can get a little bit more practice and intuition of what cross products are all about. So in the last example, we took a cross b. Let's see what happens when we take b cross a. So let me erase some of this. I don't want to erase all of it because it might be useful to give us some intuition to compare. I'm going to keep that. Actually, I can erase this, I think. So the things I have drawn here, this was a cross b. Let me cordon it off so you don't get confused. So that was me using the right hand rule when I tried to do a cross b, and then we saw that the magnitude of this was 25, and n, the direction, pointed downwards. Or when I drew it here, it would point into the page. So let's see what happens with b cross a, so I'm just switching the order. b cross a. Well, the magnitude is going to be the same thing, right? Because I'm still going to take the magnitude of b times the magnitude of a times the sine of the angle between them, which was pi over 6 radians and then times some unit vector n. But this is going to be the same. When I multiply scalar quantities, it doesn't matter what order I multiply them in, right? So this is still going to be 25, whatever my units might have been, times some vector n. And we still know that that vector n has to be perpendicular to both a and b, and now we have to figure out, well, is it, in being perpendicular, it can either kind of point into the page here or it could pop out of the page, or point out of the page. So which one is it? And then we take our right hand out, and we try it again. So what we do is we take our right hand. I'm actually using my right hand right now, although you can't see it, just to make sure I draw the right thing. So in this example, if I take my right hand, I take the index finger in the direction of b. I take my middle finger in the direction of a, so my middle figure is going to look something like that, right? And then I have two leftover fingers there. Then the thumb goes in the direction of the cross product, right? Because your thumb has a right angle right there. That's the right angle of your thumb. So in this example, that's the direction of a, this is the direction of b, and we're doing b cross a. That's why b gets your index finger. The index finger gets the first term, your middle finger gets the second term, and the thumb gets the direction of the cross product. So in this example, the direction of the cross product is upwards. Or when we're drawing it in two dimensions right here, the cross product would actually pop out of the page for b cross a. So I'll draw it over. It would be the circle with the dot. Or if I were to draw it analogous to this, so this right here, that was a cross b. And then b cross a is the exact same magnitude, but it goes in the other direction. That's b cross a. It just flips in the opposite direction. And that's why you have to use your right hand, because you might know that, oh, something's going to pop in or out of the page, et cetera, et cetera, but you need to know your right hand to know whether it goes in or out of the page. Anyway, let's see if we can get a little bit more intuition of what this is all about because this is all about intuition. And frankly, I'll tell you, the cross product comes into use in a lot of concepts that frankly we don't have a lot of real-life intuition, with electrons flying through a magnetic field or magnetic fields through a coil. A lot of things in our everyday life experience, maybe if we were metal filings living in a magnetic field-- well, we do live in a magnetic field. In a strong magnetic field, maybe we would get an intuition, but it's hard to have as deep of an intuition as we do for, say, falling objects, or friction, or forces, or fluid dynamics even, because we've all played with water. But anyway, let's get a little bit more intuition. And let's think about why is there that sine of theta? Why not just multiply the magnitudes times each other and use the right hand rule and figure out a direction? What is that sine of theta all about? I think I need to clear this up a little bit just so this could be useful. So why is that sine of theta there? Let me redraw some vectors. I'll draw them a little fatter. So let's say that's a, that's a, this is b. b doesn't always have to be longer than a. So this is a and this is b. Now, we can think of it a little bit. We could say, well, this is the same thing as a sine theta times b, or we could say this is b sine theta times a. I hope I'm not confusing-- all I'm saying is you could interpret this as-- because these are just magnitudes, right? So it doesn't matter what order you multiply them in. You could say this is a sine theta times the magnitude of b, all of that in the direction of the normal vector, or you could put the sine theta the other way. But let's think about what this would mean. a sine theta, if this is theta. What is a sine theta? Sine is opposite over hypotenuse, right? So opposite over hypotenuse. So this would be the magnitude of a. Let me draw something. Let me draw a line here and make it a real line. Let me draw a line there, so I have a right angle. So what's a sine theta? This is the opposite side. So a sine theta is a, and sine of theta is opposite over hypotenuse. The hypotenuse is the magnitude of a, right? So sine of theta is equal to this side, which I call o for opposite, over the magnitude of a. So it's opposite over the magnitude of a. So this term a sine theta is actually just the magnitude of this line right here. Another way you could-- let me redraw it. It doesn't matter where the vectors start from. All you care about is this magnitude and direction, so you could shift vectors around. So this vector right here, and you could call it this opposite vector, that's the same thing as this vector. That's the same thing as this. I just shifted it away. And so another way to think about it is, it is the component of vector a, right? We're used to taking a vector and splitting it up into x- and y-components, but now we're taking a vector a, and we're splitting it up into-- you can think of it as a component that's parallel to vector b and a component that is perpendicular to vector b. So a sine theta is the magnitude of the component of vector a that is perpendicular to b. So when you're taking the cross product of two numbers, you're saying, well, I don't care about the entire magnitude of vector a in this example, I care about the magnitude of vector a that is perpendicular to vector b, and those are the two numbers that I want to multiply and then give it that direction as specified by the right hand rule. And I'll show you some applications. This is especially important-- well, we'll use it in torque and we'll also use it in magnetic fields, but it's important in both of those applications to figure out the components of the vector that are perpendicular to either a force or a radius in question. So that's why this cross product has the sine theta because we're taking-- so in this, if you view it as magnitude of a sine theta times b, this is kind of saying this is the magnitude of the component of a perpendicular to b, or you could interpret it the other way. You could interpret it as a times b sine theta, right? Put a parentheses here. And then you could view it the other way. You could say, well, b sine theta is the component of b that is perpendicular to a. Let me draw that, just to hit the point home. So that's my a, that's my b. This is a, this is b. So b has some component of it that is perpendicular to a, and that is going to look something like-- well, I've run out of space. Let me draw it here. If that's a, that's b, the component of b that is perpendicular to a is going to look like this. It's going to be perpendicular to a, and it's going to go that far, right? And then you could go back to SOH CAH TOA and you could prove to yourself that the magnitude of this vector is b sine theta. So that is where the sine theta comes from. 1It makes sure that we're not just multiplying the vectors. 1It makes sure we're multiplying the components of 1the vectors that are perpendicular to each other to 1get a third vector that is perpendicular to both of them. 1And then the people who invented the cross product 1said, well, it's still ambiguous because it doesn't 1tell us-- there's always two vectors that are perpendicular 1to these two. 1One goes in, one goes out. 1They're in opposite directions. 1And that's where the right hand rule comes in. 1They'll say, OK, well, we're just going to say a convention 1that you use your right hand, point it like a gun, make all 1your fingers perpendicular, and then you know what 1direction that vector points in. 1Anyway, hopefully, you're not confused. 1Now I want you to watch the next video. 1This is actually going to be some physics on electricity, 1magnetism and torque, and that's essentially the 1applications of the cross product, and it'll give you a 1little bit more intuition of how to use it. 1See you soon.\",\n",
              "       \" Find the probability of rolling even numbers three times using a six-sided die numbered from 1 to 6. So let's just figure out the probability of rolling it each of the times. So the probability of rolling even numbers. So even roll on six-sided die. So let's think about that probability. Well, how many total outcomes are there? How many possible rolls could we get? Well, you get one, two, three, four, five, six. And how many of them satisfy these conditions, that it's an even number? Well, it could be a 2, it could be a 4, or it could be a 6. So the probability is the events that match what you need, your condition for right here, so three of the possible events are an even roll. And it's out of a total of six possible events. So there is a-- 3 over 6 is the same thing as 1/2 probability of rolling even on each roll. Now they're going to roll-- they want to roll even three times. And these are all going to be independent events. Every time you roll, it's not going to affect what happens in the next roll, despite what some gamblers might think. It has no impact on what happens on the next roll. So the probability of rolling even three times is equal to the probability of an even roll one time, or even roll on six-sided die-- this thing over here is equal to that thing times that thing again. All right, that's our first roll-- we copy and we paste it-- times that thing and then times that thing again. Right? That's our first roll, which is that. That's our second roll. That's our third roll. They're independent events. So this is going to be equal to 1/2-- that's the same 1/2 right there-- times 1/2 times 1/2, which is equal to 1 over 8. There's a 1 in 8 possibility that you roll even numbers on all three rolls. On this roll, this roll, and that roll.\",\n",
              "       ' -  In the last video we began to see some pretty good evidence that DNA was the molecular basis for inheritance and we saw that from the work of Avery, McCarthy and McLead where they tried to identify whether it was DNA or proteins that acted as a transformation principle in Griffith\\'s experiments and I encourage you to watch that video if all of this sounds unfamiliar. But even their work in 1944 was not viewed as conclusive evidence. It was viewed as strong evidence, but not conclusive evidence because, remember how they did it, they took the heat killed smooth strain, the smooth strain you might remember from Griffith\\'s experiment was the virulent one. The heat killed it. When you heat kill it in injected amounts, it didn\\'t do anything to the mouse, but if you took the heat killed smooth strain and put it with the rough strain, it somehow transformed the rough strain in to the smooth strain, in to the virulent strain and so they took the heat killed smooth strain and they took out its different components and they eventually were able to isolate one that was able to transform the rough strain in to the smooth strain by itself and then they applied all sorts of chemical tests to it and said, \"Hey, there\\'s pretty good evidence \"that this is DNA,\" but it wasn\\'t conclusive because, well, maybe they didn\\'t purify it properly or maybe there was still a little bit of protein. Maybe it was mostly DNA, but maybe it was a little bit of protein that was still left there that actually did the transformation. So the scientific community, they weren\\'t just saying, \"Hey, that looks pretty good, \"Let\\'s move on, let\\'s just assume.\" They wanted to continue to test it and especially test it in different ways. And, the conclusive evidence didn\\'t come until a few years later, until 1952 when Alfred Hershey and Martha Chase decided to study T2 bacterio phage. Let me write this down. T2 bacterio phage, this is phage that infects bacteria. Bacterio phage. When you hear the word phage, we\\'re referring to viruses. Now they knew that T2 bacterio phage was composed of proteins and DNA and they didn\\'t, well, we now know, that it\\'s a protein shell and there\\'s DNA inside, but they, from their point of view, they said, \"Okay, it\\'s made up and if we try to \"look at the stuff that this virus is made of, \"it\\'s protein and DNA.\" So protein plus DNA, and they knew that this virus, when it infects bacteria, it injects something into that bacteria. So it injects something and that something is what hijacks that bacteria\\'s genetic information to start producing more of the T2 bacterio phage. So they could identify the something that gets injected. If they could figure out if that something was either a protein or a virus, then they would have conclusively proven so sorry, if they could show that something was not protein or virus, if it was protein or DNA, if they could show that it was either protein or DNA, then they could show conclusively that it\\'s either the protein or the DNA that forms the molecular basis and so they\\'re actually quite sceptical of Avery, McCarthy and MacLead\\'s experiments. They actually, Hershey and Chase, actually thought that they were gonna show that it was the protein, and remember, this whole time people were like, \"Protein, we know it\\'s these complex molecules \"that have these different shapes \"and all these different amino acids. \"It seems like that\\'s much more likely to encode \"the complexity of genetic information than DNA.\" They didn\\'t have an appreciation for the structure of DNA at this time. So they devised an experiment to figure out what is that something that the T2 bacterio phage is infecting. Is that something protein? Is it protein or DNA? So this is the question. So what they do is they take two batches or they developed two batches of T2 bacterio phage. One batch of the T2 bacterio phage they do it in the presence of radioactive phosphorus, phosphorus 32. In the other batch, I should say they grow that T2 bacterio phage in the process of another radioactive isotope, but this time it is of sulfur. This is sulfur 35. So why are they doing that? Well phosphorus is found in DNA. So in this first batch, the radioactive marker, you could say, is going to incorporate itself into the DNA. In the second batch, sulfur is found in the protein and not in the DNA and so this would actually tag the protein parts. And if you\\'re wondering, well, how do you develop these radioactive batches, well, you let the viruses hijack cells in a medium that has either the radioactive sulfur or the radioactive phosphorus and as they reproduce, they are going to incorporate that radioactive material into either the protein or the DNA of the new viruses that get produced. So anyway, they were able to produce some of the T2 bacterio phage in the presence of the radioactive phosphorus and they knew that way that the DNA would get that radioactive material in it and then with the radioactive sulfur they said the protein would have that radioactive sulfur. And then for each of those batches, they then infected bacterio phage with them and they said, okay, they\\'re going to inject something in to the bacterio phage, and to figure out what that something is that was injected in to the bacterio phage they take the products in either of the two scenarios, they first blend \\'em up so that all of the stuff that\\'s left outside gets taken off of the surface of the bacteria cells and then they stick it in to a centrifuge and the centrifuge is, you can imagine, it\\'s kind of just a big spinning machine. If you were to take a test tube and take it sideways, one way to think about it, put it sideways like this. Put maybe a stopper in it so nothing leaks and then you spin it around really, really, really, really fast, what you\\'re going to find, you can actually generate significant g forces and so the heavier stuff is going to gravitate to the bottom of the test tube or to the right when it is on its side, and the lighter stuff is going to gravitate to the left. And, it turns out that the bacteria, the actual bacterial cells, those are heavier so the bacterial cells are going to go towards the bottom of the test tube and they\\'re gonna form a material that we call the pellet and then all of the other stuff, all of the fluid and the leftover phage parts those are going to go up to the top of the test tube and we call that the supernatent. I always have trouble pronouncing that. Supernatent. And so they said, \"Look, if we look at the pellet,\" which they knew had the bacterial cells int here or you could even say the remts of the bacterial cells, \"if the pellet here \"contains phosphorus, that means that the DNA, \"our radioactive DNA or our tagged DNA \"made it in to the bacteria, \"but if it contains sulfur, that means that the protein \"made it in to the bacteria.\" And, what they found is they found that the radioactive phosphorus was in the pellet which allowed them to conclude that, hey, it\\'s the DNA from the virus that made it inside of the bacteria and not the protein and then they said, \"Well, it must be! \"Wow, the Avery, McCarthy and Maclead were correct. \"It\\'s actually the DNA that is this transformation principle \"that can go in and hijack the genetic, \"the machinery of the bacteria \"to produce more of the actual virus, \"so this is a really, really, really big deal.\" Once again, we started with Mendel saying, \"Hey, we have these inheritable factors \"and they seem to segregate and sort in certain ways. \"They seem to be discrete.\" Bover and Sutton said, \"Hey, \"chromosomes seem to kind of, \"their behavior during meiosis \"when cells split \"seem to kind of match up to that.\" Morgan starts to provide some evidence. We have Griffith\\'s experiments with the mice and the bacteria and saying, \"Hey, look, \"there\\'s some transformation principle.\" Avery, McCarthy and MacLeod say, \"Hey, \"looks like when we try to really purify \"this transformation principle, \"it seems like DNA is what really matters.\" And then Hershey and Chase validate that even more conclusively.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Vw226t2Qd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc595939-edc4-4513-f30d-28e577ca117a"
      },
      "source": [
        "train_labels = train_data[\"hierarchy\"].values\n",
        "len(train_labels)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhyEYzhWkcWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfd952d-3ad9-461e-abb4-ce698931665b"
      },
      "source": [
        "len(input_ids)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVWNF3VxH_R5"
      },
      "source": [
        "# train_embeddings = []\n",
        "# with torch.no_grad():\n",
        "#   outputs = model(input_ids.to(device),attention_masks.to(device))\n",
        "# train_embeddings = torch.mean(outputs[1][0].squeeze(),dim=1)\n",
        "# train_embeddings.shape"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL8yro8NxB44"
      },
      "source": [
        "import numpy as np\n",
        "class_emb = {cls:[] for cls in list(set(train_data[\"hierarchy\"].values))}\n",
        "for index,label in enumerate(list(set(train_data[\"hierarchy\"].values))):\n",
        "  sample_indices_for_label = np.where(train_labels == label)[0][:2]\n",
        "  input_ids_for_class = input_ids[sample_indices_for_label]\n",
        "  attention_masks_class = attention_masks[sample_indices_for_label]\n",
        "  input_ids_for_class = input_ids_for_class.to(device)\n",
        "  attention_masks_class = attention_masks_class.to(device)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids_for_class,attention_masks_class)\n",
        "  class_emb[label] = torch.cat((outputs[1][-3][0][0],outputs[1][-5][0][0],outputs[1][-6][0][0]),dim=-1)\n"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUknSb-5DKYD"
      },
      "source": [
        "class_keys = [item[0] for item in class_emb.items()]"
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU0HyfwmDZzk"
      },
      "source": [
        "class_values = [item[1] for item in class_emb.items()]"
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OErmDZLtDT8v"
      },
      "source": [
        "class_keys = np.array(class_keys)"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QesKf_9P-HL6"
      },
      "source": [
        "class_prototype_embeddings = torch.stack(class_values,dim=0)"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRrv46hP_H9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c13fa8b-9c42-41aa-89a3-f6588c82c738"
      },
      "source": [
        "class_prototype_embeddings.shape"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([569, 2304])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U367BCGeD3bt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc591f65-4b0d-47d9-dfd7-c85c6a621338"
      },
      "source": [
        "class_keys[0]"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math>>old-integral-calculus>>riemann-sums-ic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_xls_7GpM-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67628f3b-4c14-4b13-d0d5-ec20e8a3157e"
      },
      "source": [
        "\n",
        "len(input_ids)"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZpmBJuIC2nM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d00e90-3944-4b98-98f5-f04a38a61e34"
      },
      "source": [
        "test_features"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' -  What I hope to do in this video is get familiar with the notion of an interval, and also think about ways that we can show an interval, or interval notation. Right over here I have a number line. Let\\'s say I wanted to talk about the interval on the number line that goes from negative three to two. So I care about this-- Let me use a different color. Let\\'s say I care about this interval right over here. I care about all the numbers from negative three to two. So in order to be more precise, I have to be clear. Am I including negative three and two, or am I not including negative three and two, or maybe I\\'m just including one of them. So if I\\'m including negative three and two, then I would fill them in. So this right over here, I\\'m filling negative three and two in, which means that negative three and two are part of this interval. And when you include the endpoints, this is called a closed interval. Closed interval. And I just showed you how I can depict it on a number line, by actually filling in the endpoints and there\\'s multiple ways to talk about this interval mathematically. I could say that this is all of the... Let\\'s say this number line is showing different values for x. I could say these are all of the x\\'s that are between negative three and two. And notice, I have negative three is less than or equal to x so that\\'s telling us that x could be equal to, that x could be equal to negative three. And then we have x is less than or equal to positive two, so that means that x could be equal to positive two, so it is a closed interval. Another way that we could depict this closed interval is we could say, okay, we\\'re talking about the interval between, and we can use brackets because it\\'s a closed interval, negative three and two, and once again I\\'m using brackets here, these brackets tell us that we include, this bracket on the left says that we include negative three, and this bracket on the right says that we include positive two in our interval. Sometimes you might see things written a little bit more math-y. You might see x is a member of the real numbers such that... And I could put these curly brackets around like this. These curly brackets say that we\\'re talking about a set of values, and we\\'re saying that the set of all x\\'s that are a member of the real number, so this is just fancy math notation, it\\'s a member of the real numbers. I\\'m using the Greek letter epsilon right over here. It\\'s a member of the real numbers such that. This vertical line here means \"such that,\" negative three is less x is less than-- negative three is less than or equal to x, is less than or equal to two. I could also write it this way. I could write x is a member of the real numbers such that x is a member, such that x is a member of this closed set, I\\'m including the endpoints here. So these are all different ways of denoting or depicting the same interval. Let\\'s do some more examples here. So let\\'s-- Let me draw a number line again. So, a number line. And now let me do-- Let me just do an open interval. An open interval just so that we clearly can see the difference. Let\\'s say that I want to talk about the values between negative one and four. Let me use a different color. So the values between negative one and four, but I don\\'t want to include negative one and four. So this is going to be an open interval. So I\\'m not going to include four, and I\\'m not going to include negative one. Notice I have open circles here. Over here had closed circles, the closed circles told me that I included negative three and two. Now I have open circles here, so that says that I\\'m not, it\\'s all the values in between negative one and four. Negative .999999 is going to be included, but negative one is not going to be included. And 3.9999999 is going to be included, but four is not going to be included. So how would we-- What would be the notation for this? Well, here we could say x is going to be a member of the real numbers such that negative one-- I\\'m not going to say less than or equal to because x can\\'t be equal to negative one, so negative one is strictly less than x, is strictly less than four. Notice not less than or equal, because I can\\'t be equal to four, four is not included. So that\\'s one way to say it. Another way I could write it like this. x is a member of the real numbers such that x is a member of... Now the interval is from negative one to four but I\\'m not gonna use these brackets. These brackets say, \"Hey, let me include the endpoint,\" but I\\'m not going to include them, so I\\'m going to put the parentheses right over here. Parentheses. So this tells us that we\\'re dealing with an open interval. This right over here, let me make it clear, this is an open interval. Now you\\'re probably wondering, okay, in this case both endpoints were included, it\\'s a closed interval. In this case both endpoints were excluded, it\\'s an open interval. Can you have things that have one endpoint included and one point excluded, and the answer is absolutely. Let\\'s see an example of that. I\\'ll get another number line here. Another number line. And let\\'s say that we want to-- Actually, let me do it the other way around. Let me write it first, and then I\\'ll graph it. So let\\'s say we\\'re thinking about all of the x\\'s that are a member of the real numbers such that let\\'s say negative four is not included, is less than x, is less than or equal to negative one. So now negative one is included. So we\\'re not going to include negative four. Negative four is strictly less than, not less than or equal to, so x can\\'t be equal to negative four, open circle there. But x could be equal to negative one. It has to be less than or equal to negative one. It could be equal to negative one so I\\'m going to fill that in right over there. And it\\'s everything in between. If I want to write it with this notation I could write x is a member of the real numbers such that x is a member of the interval, so it\\'s going to go between negative four and negative one, but we\\'re not including negative four. We have an open circle here so I\\'m gonna put a parentheses on that side, but we are including negative one. We are including negative one. So we put a bracket on that side. That right over there would be the notation. Now there\\'s other things that you could do with interval notation. You could say, well hey, everything except for some values. Let me give another example. Let\\'s get another example here. Let\\'s say that we wanna talk about all the real numbers except for one. We want to include all of the real numbers. All of the real numbers except for one. Except for one, so we\\'re gonna exclude one right over here, open circle, but it can be any other real number. So how would we denote this? Well, we could write x is a member of the real numbers such that x does not equal one. So here I\\'m saying x can be a member of the real numbers but x cannot be equal to one. It can be anything else, but it cannot be equal to one. And there\\'s other ways of denoting this exact same interval. You could say x is a member of the real numbers such that x is less than one, or x is greater than one. So you could write it just like that. Or you could do something interesting. This is the one that I would use, this is the shortest and it makes it very clear. You say hey, everything except for one. But you could even do something fancy, like you could say x is a member of the real numbers such that x is a member of the set going from negative infinity to one, not including one, or x is a member of the set going from-- or a member of the interval going from one, not including one, all the way to positive, all the way to positive infinity. And when we\\'re talking about negative infinity or positive infinity, you always put a parentheses. And the view there is you could never include everything all the way up to infinity. It needs to be at least open at that endpoint because infinity just keeps going on and on. So you always want to put a parentheses if you\\'re talking about infinity or negative infinity. It\\'s not really an endpoint, it keeps going on and on forever. So you use the notation for open interval, at least at that end, and notice we\\'re not including, we\\'re not including one either, so if x is a member of this interval or that interval, it essentially could be anything other than one. But this would have been the simplest notation to describe that.',\n",
              "       \" In the last video we were able to set up this definite integral using the shell or the hollow cylinder method in order to figure out the volume of this solid of revolution. And so now let's just evaluate this thing. And really the main thing we have to do here is just to multiply what we have here out. So multiply this expression out. So this is going to be equal to-- I'll take the 2 pi out of the integral. 2 pi times the integral from 0 to 1. Let's see, 2 times the square root of x is 2-- I'll write it as 2 square roots of x. But I'll write it as 2x to the 1/2. It'll make it a little bit easier to take the antiderivative conceptually, or at least in our brain. So two times the square root of x is 2x to the 1/2. 2 times negative x squared is negative 2 x squared. And then we have negative x times the square root of x. Well, that's x to the first times x to the 1/2. That's going to be negative x to the 3/2 power. And then we have negative x times negative x squared that's going to be positive x to the third power. And all of that dx. And so now we're ready to take the antiderivative. So this is going to be equal to 2 pi times the antiderivative of all of this business evaluated at 1 and at 0. So the antiderivative of 2 times x to the 1/2 is going to be 2-- it's going to be-- let's see. We're going to take x to the 3/2 times 2/3. So it's going to be 4/3 x to the 3/2. And then for this term right over here it's going to be negative 2/3 x to the third. And you could take the derivative here to verify that you actually do get this. And then right over here, let's see, if we incremented this, you get x to the 5/2. And so we're going to want to multiply by 2/5. So minus-- let me do this in another color. Let's see, so this one right over here, it's going to be minus 2/5 x to the 5/2 power. Yep, that works out. And then finally you're going to have x to the fourth over 4 plus-- let me do that in a different color-- plus x to the fourth over 4. That's this term right over here. And now we just have to evaluate at 1 and 0. And 0, luckily, all of these terms end up being a 0. So that's nice and cancels out. And so we are just left with-- we're just-- [INAUDIBLE] cancel out. It just evaluates to 0. So this is just 2 pi times when you evaluate all this business at 1. So that's going to be 4/3 minus 2/3 minus 2/5 plus 1/4. And the least common multiple right over here looks like 60, so we're going to want to put all this over a denominator of 60. So it's going to be 2 pi times all of this business over a denominator of 60. And 4/3 is same thing as 80/60. Negative 2/3 is the same thing as negative 40/60. Negative 2/5 is the same thing as negative 24/60. And then 1/4 is the same thing as 15/60. So this is equal to-- and actually this will cancel over here, and you'll just get a 30 in your denominator. So in your denominator, you get a 30. And up here 80 minus 40 is 40. 40 minus 24 gets us to 16. 16 plus 15 is 31. So we get 31 times pi over 30 for the volume of the figure right over there.\",\n",
              "       \" -  In previous videos we talk about GDP as the market value of final goods and services produced in a country in a given time period, let's say in a given year and we gave the example of producing jeans where maybe the farmer helps produce the cotton and then the thread maker takes that cotton and makes thread and then the fabric maker takes the thread and makes fabric and then the jean maker takes the fabric and produces jeans and then the market value of those jeans was $50 and so, assuming all of this happened in one year, in the time period that we're measuring GDP for, then we would just count the $50, if we're looking at the final market value or the market value of final goods and services. You would say the GDP for at least for this component of the GDP from these jeans is $50 but I do wanna clarify that there are multiple ways that you can measure GDP and you could even think about it from a value added approach but the key idea is no matter how you measure it you should get to the same value, so let's think about the various actors here and what their value add was. So, first, let's think about the framer right over here. So, this is the farmer, my not so elegantly drawn rectangle around what he's doing. So, the farmer's value add is what? Well, before you just had some dirt and things and so, maybe you could say that the market value was zero and then he's able to produce something or she's able to produce something that now has a market value of $10, so their value add is $10. Now, from there, the cotton goes to the thread maker. The threader maker they take that $10 cotton, so this is thread maker, they take the $10 cotton and are able to produce $20 worth of thread? What is their value add? Value add here? Well, they took something worth $10 and they were able to do something to it to make it worth $20, so their value add is now another $10 and then, this is the thread maker, and then from there it goes to the fabric maker and I think you see where this is going, the fabric maker is this part of our process, fabric maker and their value add is what? Pause this video and think about it. Well, they take something worth $20 and they're to turn it into something that has a market value of $30, so their value add is also $10 and then last but not least, you have the jean company, so the jean manufacturer, I'll call them the jean producer, the jean producer, they take something that has a market value of $30 and they're able to sell it for $50, so their value add here, if you take something for 30 and you make it worth 50, then you've added $20 of value and so, the value added approach to GDP will just sum up these value adds, so this is going to be this $10 from the farmer plus the value add of the thread maker, plus $10 from the thread maker plus $10 from the fabric maker plus $20 from the jean maker and what will that all add up to? Well, that's all going to add up to 10 plus 10 is 20 plus 10 is 30 plus 20 is $50 and lucky for us that is added up to the same amount as we had before where we just looked at the market value of the final goods and services. Now, one benefit of the value added approach is that real supply chains are quite complex and things might be going from one country to another, they might as we've talked about in another video, the year might end right over here and so, when something is made in China and there's value add in China but then it's shipped to the US and some value add is placed on it and then it's shipped back to China or Mexico, you have to be careful to only count the value add in the country for which you are measuring the GDP. So, that's one useful way or one useful reason, or one way in which the value added approach might be useful. The key idea though is that you're getting to the same value. You should get to the same value as the market value of the final goods and services produced in a given time period.\",\n",
              "       ...,\n",
              "       \" Let's see if we can get a little bit more practice and intuition of what cross products are all about. So in the last example, we took a cross b. Let's see what happens when we take b cross a. So let me erase some of this. I don't want to erase all of it because it might be useful to give us some intuition to compare. I'm going to keep that. Actually, I can erase this, I think. So the things I have drawn here, this was a cross b. Let me cordon it off so you don't get confused. So that was me using the right hand rule when I tried to do a cross b, and then we saw that the magnitude of this was 25, and n, the direction, pointed downwards. Or when I drew it here, it would point into the page. So let's see what happens with b cross a, so I'm just switching the order. b cross a. Well, the magnitude is going to be the same thing, right? Because I'm still going to take the magnitude of b times the magnitude of a times the sine of the angle between them, which was pi over 6 radians and then times some unit vector n. But this is going to be the same. When I multiply scalar quantities, it doesn't matter what order I multiply them in, right? So this is still going to be 25, whatever my units might have been, times some vector n. And we still know that that vector n has to be perpendicular to both a and b, and now we have to figure out, well, is it, in being perpendicular, it can either kind of point into the page here or it could pop out of the page, or point out of the page. So which one is it? And then we take our right hand out, and we try it again. So what we do is we take our right hand. I'm actually using my right hand right now, although you can't see it, just to make sure I draw the right thing. So in this example, if I take my right hand, I take the index finger in the direction of b. I take my middle finger in the direction of a, so my middle figure is going to look something like that, right? And then I have two leftover fingers there. Then the thumb goes in the direction of the cross product, right? Because your thumb has a right angle right there. That's the right angle of your thumb. So in this example, that's the direction of a, this is the direction of b, and we're doing b cross a. That's why b gets your index finger. The index finger gets the first term, your middle finger gets the second term, and the thumb gets the direction of the cross product. So in this example, the direction of the cross product is upwards. Or when we're drawing it in two dimensions right here, the cross product would actually pop out of the page for b cross a. So I'll draw it over. It would be the circle with the dot. Or if I were to draw it analogous to this, so this right here, that was a cross b. And then b cross a is the exact same magnitude, but it goes in the other direction. That's b cross a. It just flips in the opposite direction. And that's why you have to use your right hand, because you might know that, oh, something's going to pop in or out of the page, et cetera, et cetera, but you need to know your right hand to know whether it goes in or out of the page. Anyway, let's see if we can get a little bit more intuition of what this is all about because this is all about intuition. And frankly, I'll tell you, the cross product comes into use in a lot of concepts that frankly we don't have a lot of real-life intuition, with electrons flying through a magnetic field or magnetic fields through a coil. A lot of things in our everyday life experience, maybe if we were metal filings living in a magnetic field-- well, we do live in a magnetic field. In a strong magnetic field, maybe we would get an intuition, but it's hard to have as deep of an intuition as we do for, say, falling objects, or friction, or forces, or fluid dynamics even, because we've all played with water. But anyway, let's get a little bit more intuition. And let's think about why is there that sine of theta? Why not just multiply the magnitudes times each other and use the right hand rule and figure out a direction? What is that sine of theta all about? I think I need to clear this up a little bit just so this could be useful. So why is that sine of theta there? Let me redraw some vectors. I'll draw them a little fatter. So let's say that's a, that's a, this is b. b doesn't always have to be longer than a. So this is a and this is b. Now, we can think of it a little bit. We could say, well, this is the same thing as a sine theta times b, or we could say this is b sine theta times a. I hope I'm not confusing-- all I'm saying is you could interpret this as-- because these are just magnitudes, right? So it doesn't matter what order you multiply them in. You could say this is a sine theta times the magnitude of b, all of that in the direction of the normal vector, or you could put the sine theta the other way. But let's think about what this would mean. a sine theta, if this is theta. What is a sine theta? Sine is opposite over hypotenuse, right? So opposite over hypotenuse. So this would be the magnitude of a. Let me draw something. Let me draw a line here and make it a real line. Let me draw a line there, so I have a right angle. So what's a sine theta? This is the opposite side. So a sine theta is a, and sine of theta is opposite over hypotenuse. The hypotenuse is the magnitude of a, right? So sine of theta is equal to this side, which I call o for opposite, over the magnitude of a. So it's opposite over the magnitude of a. So this term a sine theta is actually just the magnitude of this line right here. Another way you could-- let me redraw it. It doesn't matter where the vectors start from. All you care about is this magnitude and direction, so you could shift vectors around. So this vector right here, and you could call it this opposite vector, that's the same thing as this vector. That's the same thing as this. I just shifted it away. And so another way to think about it is, it is the component of vector a, right? We're used to taking a vector and splitting it up into x- and y-components, but now we're taking a vector a, and we're splitting it up into-- you can think of it as a component that's parallel to vector b and a component that is perpendicular to vector b. So a sine theta is the magnitude of the component of vector a that is perpendicular to b. So when you're taking the cross product of two numbers, you're saying, well, I don't care about the entire magnitude of vector a in this example, I care about the magnitude of vector a that is perpendicular to vector b, and those are the two numbers that I want to multiply and then give it that direction as specified by the right hand rule. And I'll show you some applications. This is especially important-- well, we'll use it in torque and we'll also use it in magnetic fields, but it's important in both of those applications to figure out the components of the vector that are perpendicular to either a force or a radius in question. So that's why this cross product has the sine theta because we're taking-- so in this, if you view it as magnitude of a sine theta times b, this is kind of saying this is the magnitude of the component of a perpendicular to b, or you could interpret it the other way. You could interpret it as a times b sine theta, right? Put a parentheses here. And then you could view it the other way. You could say, well, b sine theta is the component of b that is perpendicular to a. Let me draw that, just to hit the point home. So that's my a, that's my b. This is a, this is b. So b has some component of it that is perpendicular to a, and that is going to look something like-- well, I've run out of space. Let me draw it here. If that's a, that's b, the component of b that is perpendicular to a is going to look like this. It's going to be perpendicular to a, and it's going to go that far, right? And then you could go back to SOH CAH TOA and you could prove to yourself that the magnitude of this vector is b sine theta. So that is where the sine theta comes from. 1It makes sure that we're not just multiplying the vectors. 1It makes sure we're multiplying the components of 1the vectors that are perpendicular to each other to 1get a third vector that is perpendicular to both of them. 1And then the people who invented the cross product 1said, well, it's still ambiguous because it doesn't 1tell us-- there's always two vectors that are perpendicular 1to these two. 1One goes in, one goes out. 1They're in opposite directions. 1And that's where the right hand rule comes in. 1They'll say, OK, well, we're just going to say a convention 1that you use your right hand, point it like a gun, make all 1your fingers perpendicular, and then you know what 1direction that vector points in. 1Anyway, hopefully, you're not confused. 1Now I want you to watch the next video. 1This is actually going to be some physics on electricity, 1magnetism and torque, and that's essentially the 1applications of the cross product, and it'll give you a 1little bit more intuition of how to use it. 1See you soon.\",\n",
              "       \" Find the probability of rolling even numbers three times using a six-sided die numbered from 1 to 6. So let's just figure out the probability of rolling it each of the times. So the probability of rolling even numbers. So even roll on six-sided die. So let's think about that probability. Well, how many total outcomes are there? How many possible rolls could we get? Well, you get one, two, three, four, five, six. And how many of them satisfy these conditions, that it's an even number? Well, it could be a 2, it could be a 4, or it could be a 6. So the probability is the events that match what you need, your condition for right here, so three of the possible events are an even roll. And it's out of a total of six possible events. So there is a-- 3 over 6 is the same thing as 1/2 probability of rolling even on each roll. Now they're going to roll-- they want to roll even three times. And these are all going to be independent events. Every time you roll, it's not going to affect what happens in the next roll, despite what some gamblers might think. It has no impact on what happens on the next roll. So the probability of rolling even three times is equal to the probability of an even roll one time, or even roll on six-sided die-- this thing over here is equal to that thing times that thing again. All right, that's our first roll-- we copy and we paste it-- times that thing and then times that thing again. Right? That's our first roll, which is that. That's our second roll. That's our third roll. They're independent events. So this is going to be equal to 1/2-- that's the same 1/2 right there-- times 1/2 times 1/2, which is equal to 1 over 8. There's a 1 in 8 possibility that you roll even numbers on all three rolls. On this roll, this roll, and that roll.\",\n",
              "       ' -  In the last video we began to see some pretty good evidence that DNA was the molecular basis for inheritance and we saw that from the work of Avery, McCarthy and McLead where they tried to identify whether it was DNA or proteins that acted as a transformation principle in Griffith\\'s experiments and I encourage you to watch that video if all of this sounds unfamiliar. But even their work in 1944 was not viewed as conclusive evidence. It was viewed as strong evidence, but not conclusive evidence because, remember how they did it, they took the heat killed smooth strain, the smooth strain you might remember from Griffith\\'s experiment was the virulent one. The heat killed it. When you heat kill it in injected amounts, it didn\\'t do anything to the mouse, but if you took the heat killed smooth strain and put it with the rough strain, it somehow transformed the rough strain in to the smooth strain, in to the virulent strain and so they took the heat killed smooth strain and they took out its different components and they eventually were able to isolate one that was able to transform the rough strain in to the smooth strain by itself and then they applied all sorts of chemical tests to it and said, \"Hey, there\\'s pretty good evidence \"that this is DNA,\" but it wasn\\'t conclusive because, well, maybe they didn\\'t purify it properly or maybe there was still a little bit of protein. Maybe it was mostly DNA, but maybe it was a little bit of protein that was still left there that actually did the transformation. So the scientific community, they weren\\'t just saying, \"Hey, that looks pretty good, \"Let\\'s move on, let\\'s just assume.\" They wanted to continue to test it and especially test it in different ways. And, the conclusive evidence didn\\'t come until a few years later, until 1952 when Alfred Hershey and Martha Chase decided to study T2 bacterio phage. Let me write this down. T2 bacterio phage, this is phage that infects bacteria. Bacterio phage. When you hear the word phage, we\\'re referring to viruses. Now they knew that T2 bacterio phage was composed of proteins and DNA and they didn\\'t, well, we now know, that it\\'s a protein shell and there\\'s DNA inside, but they, from their point of view, they said, \"Okay, it\\'s made up and if we try to \"look at the stuff that this virus is made of, \"it\\'s protein and DNA.\" So protein plus DNA, and they knew that this virus, when it infects bacteria, it injects something into that bacteria. So it injects something and that something is what hijacks that bacteria\\'s genetic information to start producing more of the T2 bacterio phage. So they could identify the something that gets injected. If they could figure out if that something was either a protein or a virus, then they would have conclusively proven so sorry, if they could show that something was not protein or virus, if it was protein or DNA, if they could show that it was either protein or DNA, then they could show conclusively that it\\'s either the protein or the DNA that forms the molecular basis and so they\\'re actually quite sceptical of Avery, McCarthy and MacLead\\'s experiments. They actually, Hershey and Chase, actually thought that they were gonna show that it was the protein, and remember, this whole time people were like, \"Protein, we know it\\'s these complex molecules \"that have these different shapes \"and all these different amino acids. \"It seems like that\\'s much more likely to encode \"the complexity of genetic information than DNA.\" They didn\\'t have an appreciation for the structure of DNA at this time. So they devised an experiment to figure out what is that something that the T2 bacterio phage is infecting. Is that something protein? Is it protein or DNA? So this is the question. So what they do is they take two batches or they developed two batches of T2 bacterio phage. One batch of the T2 bacterio phage they do it in the presence of radioactive phosphorus, phosphorus 32. In the other batch, I should say they grow that T2 bacterio phage in the process of another radioactive isotope, but this time it is of sulfur. This is sulfur 35. So why are they doing that? Well phosphorus is found in DNA. So in this first batch, the radioactive marker, you could say, is going to incorporate itself into the DNA. In the second batch, sulfur is found in the protein and not in the DNA and so this would actually tag the protein parts. And if you\\'re wondering, well, how do you develop these radioactive batches, well, you let the viruses hijack cells in a medium that has either the radioactive sulfur or the radioactive phosphorus and as they reproduce, they are going to incorporate that radioactive material into either the protein or the DNA of the new viruses that get produced. So anyway, they were able to produce some of the T2 bacterio phage in the presence of the radioactive phosphorus and they knew that way that the DNA would get that radioactive material in it and then with the radioactive sulfur they said the protein would have that radioactive sulfur. And then for each of those batches, they then infected bacterio phage with them and they said, okay, they\\'re going to inject something in to the bacterio phage, and to figure out what that something is that was injected in to the bacterio phage they take the products in either of the two scenarios, they first blend \\'em up so that all of the stuff that\\'s left outside gets taken off of the surface of the bacteria cells and then they stick it in to a centrifuge and the centrifuge is, you can imagine, it\\'s kind of just a big spinning machine. If you were to take a test tube and take it sideways, one way to think about it, put it sideways like this. Put maybe a stopper in it so nothing leaks and then you spin it around really, really, really, really fast, what you\\'re going to find, you can actually generate significant g forces and so the heavier stuff is going to gravitate to the bottom of the test tube or to the right when it is on its side, and the lighter stuff is going to gravitate to the left. And, it turns out that the bacteria, the actual bacterial cells, those are heavier so the bacterial cells are going to go towards the bottom of the test tube and they\\'re gonna form a material that we call the pellet and then all of the other stuff, all of the fluid and the leftover phage parts those are going to go up to the top of the test tube and we call that the supernatent. I always have trouble pronouncing that. Supernatent. And so they said, \"Look, if we look at the pellet,\" which they knew had the bacterial cells int here or you could even say the remts of the bacterial cells, \"if the pellet here \"contains phosphorus, that means that the DNA, \"our radioactive DNA or our tagged DNA \"made it in to the bacteria, \"but if it contains sulfur, that means that the protein \"made it in to the bacteria.\" And, what they found is they found that the radioactive phosphorus was in the pellet which allowed them to conclude that, hey, it\\'s the DNA from the virus that made it inside of the bacteria and not the protein and then they said, \"Well, it must be! \"Wow, the Avery, McCarthy and Maclead were correct. \"It\\'s actually the DNA that is this transformation principle \"that can go in and hijack the genetic, \"the machinery of the bacteria \"to produce more of the actual virus, \"so this is a really, really, really big deal.\" Once again, we started with Mendel saying, \"Hey, we have these inheritable factors \"and they seem to segregate and sort in certain ways. \"They seem to be discrete.\" Bover and Sutton said, \"Hey, \"chromosomes seem to kind of, \"their behavior during meiosis \"when cells split \"seem to kind of match up to that.\" Morgan starts to provide some evidence. We have Griffith\\'s experiments with the mice and the bacteria and saying, \"Hey, look, \"there\\'s some transformation principle.\" Avery, McCarthy and MacLeod say, \"Hey, \"looks like when we try to really purify \"this transformation principle, \"it seems like DNA is what really matters.\" And then Hershey and Chase validate that even more conclusively.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe4qYkV2C4fX"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for sent in test_features:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "# test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(test_input_ids, test_attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QOm4B7G2yBO"
      },
      "source": [
        "test_poincare_tensor = class_prototype_embeddings# torch.tensor(taxonomy_vectors,dtype=torch.float)\n"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xvPpkWn3YzN"
      },
      "source": [
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ojh13-E25ml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0a2458c-3ebb-4743-e190-4ce261eb0468"
      },
      "source": [
        "test_labels[0]"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'math>>old-integral-calculus>>riemann-sums-ic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2GQjLi0qyXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c577af1a-fcc2-4b4d-ee1a-a52c04904a71"
      },
      "source": [
        "\n",
        "\n",
        "len(input_ids)"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4188"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8nYmLgcr-ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a15bd25-e1e9-40bd-e1c1-116f1f77e59a"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "\n",
        "test_input_ids = test_input_ids.to('cuda')\n",
        "test_attention_masks = test_attention_masks.to('cuda')\n",
        "class_prototype_embeddings = class_prototype_embeddings.to('cuda')\n",
        "# Tracking variables1\n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(test_input_ids, test_attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "  # print(torch.mean(outputs[1][0].squeeze(),dim=0).shape)\n",
        "  distances = cos(torch.cat((outputs[1][-3][0][0], outputs[1][-5][0][0],outputs[1][-6][0][0])),class_prototype_embeddings)\n",
        "  distances,indices = torch.topk(distances,10,largest=True)\n",
        "  predictions.append(class_keys[indices.cpu().numpy()])\n",
        "print(len(predictions))"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,047 test sentences...\n",
            "1047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5wj2gXYFkrQ"
      },
      "source": [
        "labels=test_data['label'].values"
      ],
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9-I0Bb2AKKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f86f04-f61f-472c-d231-c1a89d58fa3d"
      },
      "source": [
        "labels"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([335, 357,   3, ..., 537, 395, 450])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adb4gTNgGKUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7967077a-cf53-4e17-c3b3-2fe98d6a1fbe"
      },
      "source": [
        "labels"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([335, 357,   3, ..., 537, 395, 450])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 317
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4MzeESPfqZs"
      },
      "source": [
        "final_predictions = []\n",
        "for prediction in predictions:\n",
        "  final_predictions.append(LE.transform(prediction))\n"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byaR6XIRGKeo",
        "outputId": "78a34385-98b0-4f2f-9f89-da0e06091ac9"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 5\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 5)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 5)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 5)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 5) (1047,)\n",
            "precision 0.06208213944603629\n",
            "update_recall:  0.3104106972301815\n",
            "recall 0.3104106972301815\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 325.0, 722.0, 325.0, 4910.0]\n",
            "TMP_RANK:  TopKV2(values=array([[245, 162, 135, 107,  78],\n",
            "       [357, 313, 298, 158, 127],\n",
            "       [397, 346,   7,   3,   1],\n",
            "       ...,\n",
            "       [413, 385, 299, 256,  39],\n",
            "       [406, 385, 278, 126,  39],\n",
            "       [508, 506, 473, 457, 450]]), indices=array([[1, 0, 3, 2, 4],\n",
            "       [3, 1, 2, 4, 0],\n",
            "       [1, 2, 3, 0, 4],\n",
            "       ...,\n",
            "       [1, 0, 4, 2, 3],\n",
            "       [0, 4, 1, 2, 3],\n",
            "       [3, 1, 2, 0, 4]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIngC0dPvRsm"
      },
      "source": [
        "Following four cells show metrics Recall@5, R@10, R@15, R@20 for prototype inspired baseline code above\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KNc4qZQ5upJ",
        "outputId": "da3a2f4b-18f2-4bf5-d1f0-5d6af2288603"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 5)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 5)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 5)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 5) (1047,)\n",
            "precision 0.05291308500477555\n",
            "update_recall:  0.26456542502387775\n",
            "recall 0.26456542502387775\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 277.0, 770.0, 277.0, 4958.0]\n",
            "TMP_RANK:  TopKV2(values=array([[411, 192, 162, 115, 107],\n",
            "       [347, 337, 298, 170, 158],\n",
            "       [ 17,  14,  11,   3,   1],\n",
            "       ...,\n",
            "       [413, 298, 265, 170, 158],\n",
            "       [284, 273, 158,  88,  32],\n",
            "       [506, 465, 457, 451, 448]]), indices=array([[3, 0, 2, 4, 1],\n",
            "       [2, 3, 0, 1, 4],\n",
            "       [2, 4, 1, 0, 3],\n",
            "       ...,\n",
            "       [4, 1, 3, 0, 2],\n",
            "       [3, 1, 2, 4, 0],\n",
            "       [1, 2, 0, 3, 4]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5g2_nFJRPfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f610e3f2-dd1e-46b0-84bb-49d381ac94c0"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 10)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 10)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 10)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 10) (1047,)\n",
            "precision 0.03925501432664757\n",
            "update_recall:  0.39255014326647564\n",
            "recall 0.39255014326647564\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 411.0, 636.0, 411.0, 10059.0]\n",
            "TMP_RANK:  TopKV2(values=array([[356, 245, 210, ...,  78,  63,  30],\n",
            "       [381, 380, 358, ..., 158, 127,  66],\n",
            "       [397, 346,  19, ...,   5,   3,   1],\n",
            "       ...,\n",
            "       [413, 385, 357, ..., 158, 127,  39],\n",
            "       [413, 406, 385, ..., 126,  95,  39],\n",
            "       [508, 506, 473, ..., 442, 422, 397]]), indices=array([[8, 1, 7, ..., 4, 6, 5],\n",
            "       [8, 9, 5, ..., 4, 0, 7],\n",
            "       [1, 2, 6, ..., 7, 0, 4],\n",
            "       ...,\n",
            "       [1, 0, 9, ..., 5, 6, 3],\n",
            "       [6, 0, 4, ..., 2, 9, 3],\n",
            "       [3, 1, 2, ..., 8, 5, 6]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHh2eh_sRkCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c437b3eb-454e-42b6-f6b8-36a34009d05f"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 15)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 15)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 15)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 15) (1047,)\n",
            "precision 0.029290035020694046\n",
            "update_recall:  0.4393505253104107\n",
            "recall 0.4393505253104107\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 460.0, 587.0, 460.0, 15245.0]\n",
            "TMP_RANK:  TopKV2(values=array([[411, 356, 263, ...,  78,  63,  30],\n",
            "       [381, 380, 364, ..., 127,  71,  66],\n",
            "       [540, 506, 397, ...,   5,   3,   1],\n",
            "       ...,\n",
            "       [413, 385, 357, ..., 127, 100,  39],\n",
            "       [413, 406, 385, ..., 126,  95,  39],\n",
            "       [508, 506, 473, ..., 416, 397,  19]]), indices=array([[10,  8, 14, ...,  4,  6,  5],\n",
            "       [ 8,  9, 12, ...,  0, 13,  7],\n",
            "       [12, 14,  1, ...,  7,  0,  4],\n",
            "       ...,\n",
            "       [ 1,  0,  9, ...,  6, 12,  3],\n",
            "       [ 6,  0,  4, ...,  2,  9,  3],\n",
            "       [ 3,  1,  2, ..., 10,  6, 12]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJe9pbtqTuZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd440183-f158-43a0-c769-7eb4c1072db7"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 20)\n",
        "\n",
        "tmp_rank = tf.math.top_k(y_pred, 20,sorted=False)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 20) (1047,)\n",
            "precision 0.020678127984718242\n",
            "update_recall:  0.41356255969436484\n",
            "recall 0.41356255969436484\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 433.0, 614.0, 433.0, 20507.0]\n",
            "TMP_RANK:  [[192 107 162 ... 383 114 356]\n",
            " [298 170 347 ... 225 211 358]\n",
            " [  3  11  17 ... 383 397 506]\n",
            " ...\n",
            " [170 298 158 ... 284 364 249]\n",
            " [ 32 273 158 ... 170 413 358]\n",
            " [457 506 465 ... 167 479   9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBJ6etu_LFH_",
        "outputId": "87379a2a-bef8-4862-d3b9-f84082d1135a"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 20)\n",
        "\n",
        "tmp_rank = tf.math.top_k(y_pred, 20,sorted=False)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(y_pred))"
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 20) (1047,)\n",
            "precision 0.023686723973256926\n",
            "update_recall:  0.47373447946513847\n",
            "recall 0.47373447946513847\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 496.0, 551.0, 496.0, 20444.0]\n",
            "TMP_RANK:  [[162 245 107 ... 342 255 334]\n",
            " [127 313 298 ... 297 222  87]\n",
            " [  3 397 346 ... 405 368 167]\n",
            " ...\n",
            " [385 413 256 ...  40 384 287]\n",
            " [406 278 126 ... 185 287  92]\n",
            " [457 506 473 ... 437 274 148]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q99M2WbYo2jy"
      },
      "source": [
        "def get_cleaned_taxonomy(taxonomy):\n",
        "  cleaned_taxonomy = []\n",
        "  for value in taxonomy:\n",
        "      value = ' '.join(value.lower().split(\">>\"))\n",
        "      # taxonomy_words = [inflection.singularize(val)  for token in value for val in token.split(\" \") if val.isalpha()]\n",
        "      cleaned_taxonomy.append( value )\n",
        "  return cleaned_taxonomy\n",
        "test_labels = list(set(test_data[\"hierarchy\"].values))\n",
        "test_emb_data = get_cleaned_taxonomy(test_labels)"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crA_aDX70_ip"
      },
      "source": [
        "label_input_ids = []\n",
        "label_attention_masks = []\n",
        "for sent in test_emb_data:\n",
        "\n",
        "    label_encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 256,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    label_input_ids.append(label_encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    label_attention_masks.append(label_encoded_dict['attention_mask'])"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQr610Oc2Ehg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c706840-e5af-401a-c2de-c3522d6a96bd"
      },
      "source": [
        "import numpy as np\n",
        "taxonomy_vectors = []\n",
        "for label_input_id,label_att_mask in zip(label_input_ids,label_attention_masks):\n",
        "    label_input_id = label_input_id.to(device)\n",
        "    label_att_mask = label_att_mask.to(device)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(label_input_id.reshape(1,-1),label_att_mask.reshape(1,-1))\n",
        "    taxonomy_vectors.append(torch.cat((outputs[1][-2][0][0], outputs[1][-3][0][0],outputs[1][-4][0][0])).cpu().numpy())\n",
        "taxonomy_vectors = np.vstack(taxonomy_vectors)\n",
        "taxonomy_vectors.shape\n"
      ],
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(416, 2304)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 323
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apu9efiNLh4C",
        "outputId": "1193ce18-32c4-4753-ac7a-9714b3c89a20"
      },
      "source": [
        "labels"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([335, 357,   3, ..., 537, 395, 450])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx8BGDyCOwzU"
      },
      "source": [
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvg3EcmagJQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d96d7d-ba7b-476d-ec3b-b60e0e669f65"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
        "\n",
        "test_input_ids = test_input_ids.to('cuda')\n",
        "test_attention_masks = test_attention_masks.to('cuda')\n",
        "taxonomy_vectors = torch.tensor(taxonomy_vectors,dtype=torch.float).to('cuda')\n",
        "# Tracking variables1\n",
        "predictions , true_labels = [], []\n",
        "for input_id,attention_mask in zip(test_input_ids, test_attention_masks):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
        "  # print(torch.mean(outputs[1][0].squeeze(),dim=0).shape)\n",
        "  distances = cos(torch.cat((outputs[1][-1][0][0], outputs[1][-2][0][0],outputs[1][-3][0][0])),taxonomy_vectors)\n",
        "  distances,indices = torch.topk(distances,20,largest=True)\n",
        "  predictions.append(test_labels[indices.cpu().numpy()])\n",
        "print(len(predictions))"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,047 test sentences...\n",
            "1047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF4ZHzHjLrFB"
      },
      "source": [
        "final_predictions = []\n",
        "for prediction in predictions:\n",
        "  final_predictions.append(LE.transform(prediction))\n"
      ],
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkAmSRaNC1Bz",
        "outputId": "9aeeaa91-3d6c-4c96-e8d3-673ff29efb06"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 5)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 5)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 5)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 5) (1047,)\n",
            "precision 0.01585482330468004\n",
            "update_recall:  0.07927411652340019\n",
            "recall 0.07927411652340019\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 83.0, 964.0, 83.0, 5152.0]\n",
            "TMP_RANK:  TopKV2(values=array([[297, 294, 276, 202, 182],\n",
            "       [344, 335, 331, 124, 115],\n",
            "       [ 21,  16,  15,  14,   8],\n",
            "       ...,\n",
            "       [344, 339, 297, 294, 202],\n",
            "       [294, 211, 210, 202, 182],\n",
            "       [513, 459, 457, 450, 330]]), indices=array([[0, 2, 3, 1, 4],\n",
            "       [0, 3, 1, 2, 4],\n",
            "       [2, 1, 0, 3, 4],\n",
            "       ...,\n",
            "       [1, 3, 4, 0, 2],\n",
            "       [1, 3, 2, 4, 0],\n",
            "       [2, 3, 1, 0, 4]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rewyp8PDLmb9",
        "outputId": "d60c0f69-4853-425e-de58-2f5860c19b81"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 10)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 10)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 10)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 10) (1047,)\n",
            "precision 0.013467048710601719\n",
            "update_recall:  0.1346704871060172\n",
            "recall 0.1346704871060172\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 141.0, 906.0, 141.0, 10329.0]\n",
            "TMP_RANK:  TopKV2(values=array([[297, 294, 289, ..., 210, 202, 182],\n",
            "       [348, 346, 344, ..., 117, 116, 115],\n",
            "       [ 23,  21,  16, ...,   6,   5,   2],\n",
            "       ...,\n",
            "       [344, 342, 339, ..., 290, 202,  74],\n",
            "       [410, 407, 400, ..., 210, 202, 182],\n",
            "       [513, 459, 458, ..., 336, 330, 284]]), indices=array([[0, 2, 5, ..., 9, 1, 4],\n",
            "       [5, 7, 0, ..., 8, 6, 4],\n",
            "       [8, 2, 1, ..., 6, 9, 5],\n",
            "       ...,\n",
            "       [1, 5, 3, ..., 8, 2, 9],\n",
            "       [5, 9, 7, ..., 2, 4, 0],\n",
            "       [2, 3, 7, ..., 8, 4, 9]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0hEAx92_2MG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91a92f8b-1c1a-4798-a284-99cda8dddf32"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 15)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 15)\n",
        "\n",
        "tmp_rank = tf.nn.top_k(y_pred, 15)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 15) (1047,)\n",
            "precision 0.011397644062400509\n",
            "update_recall:  0.17096466093600765\n",
            "recall 0.17096466093600765\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 179.0, 868.0, 179.0, 15526.0]\n",
            "TMP_RANK:  TopKV2(values=array([[297, 294, 289, ..., 185, 182,  74],\n",
            "       [348, 346, 345, ..., 117, 116, 115],\n",
            "       [ 23,  21,  18, ...,   3,   2,   0],\n",
            "       ...,\n",
            "       [344, 342, 339, ..., 261, 202,  74],\n",
            "       [410, 407, 400, ..., 202, 195, 182],\n",
            "       [513, 461, 459, ..., 284, 281, 276]]), indices=array([[ 0,  2,  5, ..., 13,  4, 11],\n",
            "       [ 5,  7, 12, ...,  8,  6,  4],\n",
            "       [ 8,  2, 14, ..., 12,  5, 11],\n",
            "       ...,\n",
            "       [ 1,  5,  3, ..., 14,  2,  9],\n",
            "       [ 5,  9,  7, ...,  4, 14,  0],\n",
            "       [ 2, 10,  3, ...,  9, 12, 14]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGwRCZL_Lv-d",
        "outputId": "1e9855ca-e599-48d9-977b-1b39ec5f8e7e"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_true = np.array(labels)\n",
        "y_true = tf.identity(y_true)\n",
        "y_pred = np.array(final_predictions)\n",
        "y_pred = tf.identity(y_pred)\n",
        "print(y_pred.shape,y_true.shape)\n",
        "k = 8\n",
        "recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, 20)\n",
        "precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, 20)\n",
        "\n",
        "tmp_rank = tf.math.top_k(y_pred, 20,sorted=False)\n",
        "stream_vars = [i for i in tf.local_variables()]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"precision\",sess.run(update_precision))\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    print(\"update_recall: \",sess.run(update_recall ))\n",
        "    print(\"recall\",sess.run(recall))\n",
        "\n",
        "    print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 20) (1047,)\n",
            "precision 0.010076408787010506\n",
            "update_recall:  0.20152817574021012\n",
            "recall 0.20152817574021012\n",
            "STREAM_VARS:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 211.0, 836.0, 211.0, 20729.0]\n",
            "TMP_RANK:  TopKV2(values=array([[297, 294, 290, ..., 182,  74,   8],\n",
            "       [348, 346, 345, ..., 117, 116, 115],\n",
            "       [410,  23,  21, ...,   3,   2,   0],\n",
            "       ...,\n",
            "       [344, 342, 339, ..., 202, 182,  74],\n",
            "       [410, 407, 400, ..., 194, 182,  74],\n",
            "       [513, 461, 459, ..., 284, 281, 276]]), indices=array([[ 0,  2, 19, ...,  4, 11, 15],\n",
            "       [ 5,  7, 12, ...,  8,  6,  4],\n",
            "       [19,  8,  2, ..., 12,  5, 11],\n",
            "       ...,\n",
            "       [ 1,  5,  3, ...,  2, 19,  9],\n",
            "       [ 5,  9,  7, ..., 16,  0, 15],\n",
            "       [ 2, 10,  3, ...,  9, 12, 14]], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhzZIz_7zA76"
      },
      "source": [
        "def mrr_metric(labels, predictions, weights=None,\n",
        "              metrics_collections=None,\n",
        "              updates_collections=None,\n",
        "              name=None):\n",
        "    \n",
        "    with tf.name_scope(name, 'mrr_metric', [predictions, labels, weights]) as scope:\n",
        "\n",
        "    \n",
        "        k = 20 #predictions.get_shape().as_list()[-1]\n",
        "        print(predictions.get_shape())\n",
        "\n",
        "        get_ranked_indicies = tf.expand_dims(tf.where(tf.equal(tf.cast(predictions,tf.int64),labels[:,None]))[:,1],1)\n",
        "        rr = 1/(get_ranked_indicies+1)\n",
        "        m_rr =  tf.reduce_sum(rr)/tf.cast(labels.get_shape().as_list()[0],dtype=tf.float64)\n",
        "\n",
        "        if metrics_collections:\n",
        "            tf.add_to_collection(metrics_collections, m_rr)\n",
        "\n",
        "        if updates_collections:\n",
        "            tf.add_to_collections(updates_collections, update_mrr_op)\n",
        "\n",
        "        return m_rr,m_rr,rr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SR4jlnNzGsE",
        "outputId": "c780c702-1ab5-4d4e-dadb-68e04cc84f78"
      },
      "source": [
        "mrr, update_mrr,rr = mrr_metric(y_true,y_pred)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    print(\"update_mrr\",sess.run(update_mrr),sess.run(rr).shape)\n",
        "    # print(\"precision\",sess.run(precision))\n",
        "\n",
        "    # print(\"update_recall: \",sess.run(update_recall ))\n",
        "    # print(\"recall\",sess.run(recall))\n",
        "\n",
        "    # print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
        "    # print(\"TMP_RANK: \",sess.run(tmp_rank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 20)\n",
            "update_mrr 0.05334960867320016 (211, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXAqaCWDNxK6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}